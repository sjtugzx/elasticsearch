An Adaptive Algorithm Selection Framework∗

Hao Yu

IBM T. J. Watson Research Ctr
Yorktown Heights, NY 10598

yuh@us.ibm.com

Dongmin Zhang

Parasol Lab, Dept of CS
Texas A&M University
dzhang@cs.tamu.edu

Lawrence Rauchwerger
Parasol Lab, Dept of CS
Texas A&M University
rwerger@cs.tamu.edu

Abstract

Irregular and dynamic memory reference patterns
can cause performance variations for low level algo-
rithms in general and for parallel algorithms in partic-
ular. We present an adaptive algorithm selection frame-
work which can collect and interpret the inputs of a par-
ticular instance of a parallel algorithm and select the
best performing one from a an existing library. In this
paper present the dynamic selection of parallel reduc-
tion algorithms. First we introduce a set of high-level pa-
rameters that can characterize different parallel reduc-
tion algorithms. Then we describe an off-line, system-
atic process to generate predictive models which can be
used for run-time algorithm selection. Our experiments
show that our framework: (a) selects the most appro-
priate algorithms in 85% of the cases studied, (b) over-
all delievers 98% of the optimal performance, (c) adap-
tively selects the best algorithms for dynamic phases of
a running program (resulting in performance improve-
ments otherwise not possible), and (d) adapts to the un-
derlying machine architecture (tested on IBM Regatta
and HP V-Class systems).

1. Introduction

Improving performance on current parallel proces-
sors is a very complex task which, if done “by hand” by
programmers, becomes increasingly difﬁcult and error
prone. Programmers have obtained increasingly more
help from parallelizing (restructuring) compilers. Such
compilers address the need of detecting and exploit-
ing parallelism in sequential programs written in con-
ventional languages as well as parallel languages (e.g.,
HPF). They also optimize data layout and perform other

∗ Research was performed at Texas A&M and supported in part by
NSF CAREER Award CCR-9734471, NSF Grant ACI-9872126,
NSF Grant EIA-0103742, NSF Grant ACI-0326350, NSF Grant
ACI-0113971, DOE ASCI ASAP Level 2 Grant B347886

transformations to reduce and hide memory latency, the
other crucial optimization in modern large scale parallel
systems. The success in the “conventional” use of com-
pilers to automatically optimize code is limited to the
cases when performance is independent of the input data
of the applications. When codes are irregular (memory
references are irregular) and/or dynamic (change dur-
ing the execution of the same program instance) it is
very likely that important performance affecting pro-
gram characteristics are input and environment depen-
dent. Many important (frequently used and time con-
suming) algorithms used in such programs are indeed in-
put dependent. We have previously shown [25] that, for
example, parallel reduction algorithms are quite sensi-
tive to their input memory reference pattern and system
architecture. In [1] we have shown that parallel sorting
algorithms are sensitive to architecture, data type, size,
etc. One of the most powerful optimizations compilers
can employ is to substitute entire algorithms instead of
trying to perform low level optimizations on sequences
of code. In [1] and [25] we have shown that performance
improves signiﬁcantly if we dynamically select the best
algorithm for each program instance.

In this paper, we present a general framework to auto-
matically and adaptively select, at run-time, the best per-
forming, functionally equivalent algorithm for each of
their instantiations. The adaptive framework can select,
at run-time, the best parallel algorithm from an exist-
ing library. The selection process is based on an off-line
automatically generated prediction model and algorithm
input data collected and analyzed dynamically. For this
paper we have concentrated our attention on the auto-
matic selection of reduction algorithms. For brevity we
will not show in this paper the dynamic selection of par-
allel sorting algorithms.

Reductions (aka updates) are important because they
are at the core of a very large number of algorithms and
applications – both scientiﬁc and otherwise – and there
is a large body of literature dealing with their paralleliza-
tion. More formally, a reduction variable is a variable

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

whose value is used in one associative and possibly com-
mutative operation of the form x = x ⊗ exp, where
⊗ is the operator and x does not occur in exp or any-
where else in the loop. With the exception of some sim-
ple methods using unordered critical sections (locks),
reduction parallelization is performed through a simple
form of algorithm substitution. For example, a sequen-
tial summation is a reduction which can be replaced by a
parallel preﬁx, or recursive doubling, computation [12].
In the case of irregular, sparse programs, we deﬁne irreg-
ular reductions as reductions performed on data refer-
enced in an irregular pattern, usually through an index
array. For irregular reductions, the access pattern tra-
versed by the sequential reduction in a loop is often sen-
sitive to the input data or computation. Therefore, as we
have shown in [25], not all parallel reduction algorithms
or implementations are equally suited as substitutes for
the original sequential code. Each access pattern has its
own characteristics and will best be parallelized with an
appropriately tailored algorithm.

In [25] we have presented a small library of paral-
lel reduction algorithms and shown that the best perfor-
mance can be obtained only if we dynamically select the
most appropriate one for the instantiated input set (ref-
erence pattern). Also in [25] we have presented a taxon-
omy of reduction reference patterns and sketched a de-
cision tree (derived manually) based scheme.

We continue this work and introduce a systematic
and automatic process to generate predictive models that
match the parallel reduction algorithms to execution in-
stances of reduction loops. After manually establishing
a small set of parameters that can characterize irregu-
lar memory references and setting up a library of paral-
lel reduction algorithms [25], we measure their relative
performance for a number of memory reference param-
eters in a factorial experiment. This is achieved by run-
ning a synthetic loop which can generate reduction ref-
erences with the memory reference patterns selected by
our factorial experiment. The end result of this off-line
process is a mapping between various points in the mem-
ory reference pattern space and the best available reduc-
tion algorithm. At run-time, the memory reference char-
acteristics of the actual reduction loop are extracted and
matched through a regression to the corresponding best
algorithm (using our previously extracted map).

This paper’s main contribution is a framework for a
systematic process through which input sensitive predic-
tive models can be built off-line and used dynamically to
select from a particular list of functionally equivalent al-
gorithms, parallel reductions being just on important ex-
ample. The same approach could also be used for vari-
ous other compiler transformations that cannot be easily
analytically modeled. Our experiments on an IBM Re-

gatta and HP V-Class show that the framework: (a) se-
lects the best performing algorithms in 85% of the cases
studied, (b) overall delievers 98% of the optimal perfor-
mance, (c) achieves better performance by adaptively se-
lecting the best algorithm for the dynamic phase of a
running program, (d) adapts to machine architecture.

2. Framework Overview

In this section we give an overview of our general
framework for adaptive and automatic low level algo-
rithm selection, the details of which are presented in the
remainder of this paper as applied to the optimization of
parallel reduction algorithm selection.

Fig. 1 gives an overview of our adaptive framework.
We distinguish two phases: (a) a setup phase and (b) a
dynamic selection (optimization) phase.

The setup phase occurs once for each computer sys-
tem and thus, implicitly tailors our process to a particu-
lar architecture. We then establish the input domain and
the output domain (possible optimizations) of the algo-
rithm selection code.

In the particular case presented in this paper, i.e., par-
allel reductions, the input domain is the universe of all
possible and realistic memory reference patterns — be-
cause, because they crucially impact the obtained per-
formance [25]. Architecture type is also important, but
is used implicitly. Since it would be impractical to study
the entire universe of memory access patterns, we de-
ﬁne a small set of parameters that can sufﬁciently char-
acterize them. The domain of possible optimizations is
composed, in our case, of the different parallel reduc-
tion algorithms collected in a library. Here, we need
to note that the characteristic parameters for selecting
algorithms for different optimizations can be different.
To avoid extensive setup process, high level parameters
(usually need to be identiﬁed manually) are preferred.
For instance, the characteristic parameters for selecting
parallel sorting algorithms [1] are different from what
we choose for reduction parallelization.

We then explore our input domain and ﬁnd a map-
ping to the output domain. In our case we establish a
mapping between different points in the input parame-
ter space (memory reference patterns) and relative per-
formance rankings of the available algorithms. This task
is accomplished off-line by running a factorial exper-
iment. We generate a number of parameter sets that
have the potential cover our input domain. For each of
these data points, we measure the relative performance
of our algorithms on the particular architecture, and rank
them accordingly. We should mention here that we have
also tried other methods of exploring the data space.

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

Setup Phase

Optimization Phase

Application

Synthetic
Experiments

Model
Derivation

Optimizing Compiler

Adaptive Executable

Yes

Characteristics
Changed ?

Algo. Selection Code

Select Algo.

Selected Algo.

Figure 1. Adaptive Algorithm Selection

In [24, 1], we have used a machine learning algorithmto
explore the input space that deﬁnes performance.

The dynamic selection phase occurs during actual
program execution. Through instrumentation (or other-
wise) we extract the set of relevant parameters that char-
acterizes the actual input. Then we used the informa-
tion obtained during the setup phase to ﬁnd its corre-
sponding output. Speciﬁcally, we use statistical regres-
sion to eventually select the appropriate best perform-
ing parallel reduction algorithm. In [1] and [24], in turn,
we used statically generated decision trees which are dy-
namically traversed to select the best algorithms.

In this paper we specialize our adaptive framework
to parallel reduction algorithm selection. In the follow-
ing, we ﬁrst present our library of reduction algorithms
and a set of parameters that can characterize their ir-
regular memory reference patterns. Then we describe
the factorial experiment that explores our input space.
For each parameter set, we execute a synthetic parame-
terized loop that generates a memory reference pattern
on which we evaluate and rank the different algorithms
in our library. A regression method is used to compute
prediction models for each parallel reduction algorithm
based on the data from the factorial experiment. At run-
time, we compute values for the characterization param-
eters of the reduction operation in question and use them
in our pre-computed models to select the best algorith-
mic option. We then evaluate our framework experimen-
tally for static and dynamic reference patterns.

3. Reduction Algorithm Library

Reductions are associative recurrences and they can
be parallelized in several ways. Our library currently
contains two types of methods: direct update methods,
which update shared reduction variables during loop ex-
ecution, and private accumulation and global update
methods, which accumulate in private storage during
loop execution and update shared variables with each
processor’s contribution afterwards.

Direct update methods include the classical recursive
doubling [12], unordered critical sections [26] and local
write [7]; our library only includes local write because
the others are not as competitive. Local Write (LO-
CALWR) is originally described in [7], LOCALWR ﬁrst
collects the reference pattern in an inspector loop [19]
and partitions the iteration space based on the “owner–
computes” rule. Memory locations referenced on multi-
ple processors have their iterations replicated on those
processors as well. To execute the reduction(s), each
processor goes through its local copies of the iterations
containing the reduction(s) and operating on the proces-
sor’s local data.

We have implemented three private accumula-
tion methods in our library. Replicated Buffer (REP-
BUF) [12, 14] simply executes the reduction loop as a
doall, accumulating partial results in private reduc-
tion variables, and later accumulates results across pro-
cessors. Replicated Buffer With Links (REPLINK)
[25] avoids traversing the unused (but allocated) el-
ements during REPBUF’s cross-processor
reduction
phase by keeping a linked list to record the proces-
sors that access each shared element. Selective Privati-
zation (SELPRIV) [25] only privatizes array elements
that have cross-processor contention. By excluding un-
used private elements, SELPRIV maintains a dense pri-
vate space where most elements are used. Since the
private space does not align to the shared data ar-
ray, the reduction’s index array is modiﬁed to redirect
accesses to the selectively privatized elements.

Issues

REPBUF

REPLINK

SELPRIV

LOCALWR

Good when
contention is

Locality

Need schedule

reuse [19]
Extra Work
Extra Space

High
Poor

No
No
NxP

Low
Poor

Yes
No
NxP

Low
Good

Yes
No

NxP+M

Low
Good

Yes
Yes
MxP

M: # iterations; N: data array size; P: # processors.
Table 1. Comparison of reduction algo.

Our parallel reduction algorithm library contains sev-
eral methods suitable for a range of reference patterns.
For brevity, we provide here only a high level descrip-
tion of the methods. See [25] for additional details.

In Table 1, we present a qualitative comparison of
the algorithms described above. Here, the contention of
a reduction is the average number of iterations (# pro-

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

NBF: N=25600,CON=200,P=8

25600

19200

12800

6400

e
g
a
s
U
 
r
e
f
f
u
B

1

2

MOLDYN: N=16384,CON=95.8,P=8

16384

12000

8000

4000

e
g
a
s
U
 
r
e
f
f
u
B

IRREG: N=100000,CON=100,P=8

100000

e
g
a
s
U
 
r
e
f
f
u
B

80000

60000

40000

20000

4

3
6
Processors

5

7

8

1

2

4

3
6
Processors

5

7

8

1

2

4

3
6
Processors

5

7

8

Figure 2. Memory access patterns of REPBUF

cessors when running in parallel) referencing the same
element. When contention is low, many unused repli-
cated elements in REPBUF are accumulated across pro-
cessors, while other algorithms only pass useful data.
SELPRIV works on a compacted data space and there-
fore potentially has good spatial locality. In LOCALWR,
each processor works on a speciﬁc portion of the data ar-
ray and has thus potentially good locality. With respect
to overhead, REPLINK, SELPRIV and LOCALWR all
have auxiliary data structures that depend on the access
pattern, and which must be updated when it changes.
Their overhead can be reduced by employing the sched-
ule reuse [19] technique.

4. High-Level Parameters

In this section, we describe the parameters we
have chosen to characterize reduction operations. Ide-
ally, they should require little overhead to measure and
they should enable us to select the best parallel re-
duction algorithm from our library for each reduction
instance in the program. After deﬁning the identiﬁed pa-
rameters below, we present a summary of the decoupled
effects of the parameters on the performance of the par-
allel reduction algorithms to illustrate the effectiveness
of the chosen parameters. Here, we enumerate the pa-
rameters in no speciﬁc order.

N is the number data elements involved in the reduc-
tion (often the size of the reduction array). It strongly in-
ﬂuences the loop’s working set size, and thus potentially
performance. In some applications, several reduction ar-
rays have exactly same access pattern; here N includes
the data elements for all arrays.

CON, the Connectivity of a loop, is the ratio between
the number of iterations of the loop and N. This param-
eter is equivalent to the parameter deﬁned by Han and
Tseng in [7]; there, the underlying data structures (cor-
responding to the irregular reductions) represent graphs
G = (V, E) and the authors deﬁned Connectivity
as |V |/|E|. Generally, the higher the connectivity, the
higher the ratio of computation to communication, i.e.,

if the connectivity is high, a small number of elements
will be referenced by many iterations.

MOB, the Mobility per iteration of a loop, is the num-
ber of distinct subscripts of reductions in an iteration.
For the LOCALWR algorithm, the effect of high itera-
tion Mobility (actually lack of mobility) is a high degree
of iteration replication. MOB is a parameter that can be
measured at compile time.

OTH, represents the Other (non-reduction) work in
an iteration. If OTH is high, a penalty will be paid
for replicating iterations. To measure OTH, we instru-
mented a parallel loop transformed for the REPBUF al-
gorithm using light-weight timers (∼ 100 clock cycles).
SP, the Sparsity, is the ratio of the total number of
referenced private elements and total allocated space on
all processors using the REPBUF algorithm (pN). Intu-
itively, SP indicates if REPBUF is efﬁcient.

CLUS, the Number of Clusters, reﬂects spatial local-
ity and measures if the used private elements in the REP-
BUF are scattered or clustered on each processor. Fig. 2
shows three memory access patterns which can be clas-
siﬁed as clustered, partially-clustered, and scattered, re-
spectively. Currently, SP and CLUS are measured by in-
strumenting parallel reduction loops using the REPBUF
algorithm and the overhead is proportional to the num-
ber of used private elements. CLUS measures the aver-
age number of clusters of the used private elements on
each processor.

We have investigated the decoupled effects of the pa-
rameters on the performance of the parallel reduction
algorithms. Although the decoupling is not realistic, it
is useful for discovering qualitative trends. The trends
summarized in Table 2 are based on a comprehensive
set of experiments on multiple architectures that are re-
ported in [24]. The trends for REPLINK are similar to
REPBUF and are not listed here.

The effect of N is straight–forward, comparing to
the sequential reduction loop, SELPRIV and LOCALWR
have much smaller data sets on each processor and there-
fore their speedups increase with N. CON is inversely
correlated with inter-processor communication. Hence,
larger CON values will indicate better scaling REPBUF

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

parameters

N

CON
MOB
OTH
SP

CLUS

REPBUF

(cid:2)
↑
↑
↑
(cid:2)
(cid:2)

SELPRIV

↑
(cid:2)
(cid:4)
↑
↓
↑

LOCALWR

↑
–
↓
(cid:2)
(cid:4)
–

↑: positive effect; ↓: negative effect; (cid:6): little positive
effect; (cid:7): little negative effect; –: no effect.
Table 2. Decoupled effects

and SELPRIV, which have two reduction loops, one ac-
cumulating in private space and one accumulating cross-
processor shared data. Large MOB values (many refer-
ences to index arrays) may imply poor performance
for SELPRIV, because accesses to the reduction array
must access both the original and the modiﬁed index
arrays, and for LOCALWR, because large MOB val-
ues often result in high iteration replication. Large val-
ues of OTH often indicate good performance for REP-
BUF and SELPRIV because the ﬁrst private accumula-
tion loop has a larger iteration body; since LOCALWR
replicates “other work” it will not beneﬁt as much. Low
SP is good for SELPRIV and also for LOCALWR, since
it often correlates with low contention and hence low
iteration replication. For large CLUS, since SELPRIV
compacts the sparsely used data space, it achieves better
speedups than LOCALWR and REPBUF, which work on
original (non-compacted) data or in private spaces con-
formable to the original data.

5. Adaptive Reduction Selection

In this section we will elaborate on the setup and on
the dynamic selection phases of our adaptive scheme.
The setup is executed only once, during machine instal-
lation, and generates a map between points in the uni-
verse of all inputs (memory reference patterns charac-
terized with the previously deﬁned parameters) and their
corresponding best suited reduction algorithms. The dy-
namic selection phase is executed every time a targeted
reduction is encountered. It uses the map built during
the setup phase, a parameter collection mechanism to
characterize the memory references and an interpolation
function (the actual algorithm selector) to ﬁnd the best
suited algorithm in the library.

5.1. Setup Phase

We now outline the design of the initial map between
a set of synthetically generated parameter values and the
corresponding performance ranking of the various paral-

Experimental

Parameter

Values

Parameterized

Synthetic

Reduction Loop

Experimental

Speedups

A Compact Map

MODELS:
Speedups[1..S] = F( Parameters )

Run−Time

Run−Time Algorithm Selection

Figure 3. Setup phase

lelization algorithms available in our library. The overall
setup phase, is illustrated in Fig. 3.

The domain of values that can be taken by the input
parameters is explored by setting up a factorial experi-
ment [10]. Speciﬁcally, we choose several values (typi-
cal of realistic reduction loops) for each parameter and
generate a set of experiments from all combinations of
the chosen values of the different parameters. The cho-
sen parameter values for our reduction experiment are
shown in Table 3.

To measure the performance of different reduction
patterns, we have created a synthetic reduction loop.
The structure of the loop is shown in Fig 4, with C-like
pseudo-code. The non–reduction work and reductions
have been grouped in two loop nests. The dynamic
pattern depends strictly on the index array, which is
generated automatically to correspond to the parameters
SP and CLUS. In addition, OTH is dynamically mea-
sured. The contents of the index[*] array are gen-
erated via a randomized process which satisﬁes the re-
quirement speciﬁed by the parameters.

The performance ranking of the parallel reduction al-
gorithms in our library is accomplished by simply ex-
ecuting them all for each parameter combination, i.e.,
synthetically generated access pattern, and measuring
their actual speedups, as shown in Fig. 3. The end re-
sult is a compact map from parameter values to perfor-
mance (speedups) of candidate parallel algorithms.

From this map we can now create the prediction code,
the model that can then be used by an application at run-

FOR j = 1 to N*CON

FOR i = 1 to OTH /* non-reductions */

memory read & scalar computation;

FOR i = 1 to MOB

/* reductions */

data[ index[j][i] ] += expr;

Figure 4. Synthetic reduction loop

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

time. We applied two methods, general linear regression
and decision tree learning. However, due to space con-
straints, in this paper we will describe only the modeling
process using general linear regression.

Values
256K
16

Parameters
N (array size)
Connectivity
Mobility
Other Work
Sparsity
# clusters
Table 3. Param. for factorial experiment

16K
0.2
2
1
0.02
1

64K
2
8
4
0.2
4

0.45
20

1M
128

0.75

0.99

4M

The generated models are polynomial functions from
the parameter values to the speedups of the parallel al-
gorithms. We follow a standard term selection process
that automatically selects polynomial terms from a spec-
iﬁed pool [16]. We have speciﬁed a maximal model as
F = (lg N + lg C + M OB + OT H + lg S + lg L + 1)3.
and a minimal model as F = lg N + lg C + M OB +
OT H + lg S + lg L + 1. Here, C is CON, S is SP, and L
is CLUS. Then from the minimal model, terms are ran-
domly selected from the 84 terms of the maximal model.
The samples are ﬁrst separated into training data and
testing data. When adding a term into the model, the
training data are used to ﬁt (least square ﬁt) the coefﬁ-
cients and the ﬁtted model is evaluated using the testing
data. If the test error is higher than the test error of the
model before including the new term, the term will be
dropped. Though this sequential term selection process
will not give us the optimal model, it is fast and its auto-
matically generated models have produced good results
when used to predict relative performance of the paral-
lel algorithms on real reduction loops. Here, the order of
the model, 3, is chosen mainly due to practical reasons
such as generating less experimental samples with less
synthetic experimentation time and avoiding term explo-
sion. For parameter MOB, we have only chosen 2 values
for the experiment, we have excluded the terms contain-
ing a non-linear MOB factor from the maximal model.
For OTH and CLUS, though we speciﬁed few values,
the values used in the map are measured and computed
from the generated index array.

The ﬁnal polynomial models contain about 30 terms.
The corresponding C library routines are generated au-
tomatically to evaluate the polynomial F () for each al-
gorithm at run-time.

5.2. Dynamic Selection Phase

During the dynamic selection phase, to avoid exe-
cuting the parameter collection and algorithm selection

Begin

Pattern Changed?

Yes

SCH_reuse

SCH_adapt

Compute Parameters: P
SCH = select_algorithm(P)

End

Figure 5. Reduction selection at run-time

steps for every time a reduction loop is invoked, we use
a form of memoization, decision reuse, which detects if
the inputs to our selector function have changed. When
a new instance of a reduction is encountered and the in-
put parameters have not changed from the previous exe-
cution instance, we directly reuse the previously selected
algorithm, thus saving run-time selection overhead. This
is accomplished with standard compiler technology, i.e.,
the compiler instruments two version loops for each par-
allel algorithm (illustrated in Fig. 5).

In this phase, the pre-generated model evaluation rou-
tines are called to estimate speedups of all the algo-
rithms, rank them, and select the best one. The evalu-
ation of the polynomial models is fast because each of
the ﬁnal models contains only about 30 terms.

5.3. Selection Reuse for Dynamic Programs

In the previous section we presented a systematic pro-
cess to generate predictive models that could recom-
mend the best irregular reduction parallelization algo-
rithm by collecting a set of static and dynamic param-
eters. We mentioned that if the memory characteristics
parameters do not change we can reuse our decision and
thus reduce run-time overhead. This may be of value if
the compiler can automatically prove statically that the
reference pattern does not change. If, however this is not
possible, which is often the case for irregular dynamic
codes, we have to perform a selection for each reduction
loop instance. In this section we will show how to re-
duce this overhead by evaluating a trade-off between the
run-time overhead of selection and the beneﬁt of ﬁnd-
ing a better algorithm.

To better describe the run–time behavior of a dy-
namic program, we introduce the notion of dynamic
phase. For a loop containing irregular reductions, a
phase is composed of all the contiguous execution in-
stances for which the pattern does not change. For in-
stance, assume the access pattern of the irregular reduc-

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

Program
Irreg – FE based CFD kernel
Nbf – MD kernel
Moldyn – synthetic MD program
Charmm – MD kernel
Spark98 – FE simulation
Spice – circuit simulation
Fma3d – FE method for solids

Lang.
F77
F77
C
F77
C
F77
F90

Lines
223
116
688
277
1513
18912
60122

Source
[7]
[7, 21]
[7, 4, 15]
[8]
SPEC’2000
SPEC’92
SPEC’2000

Loop
apply elements’ forces (do 100)
non-bounded force calc. (do 50)
non-bounded force calc.
non-bounded force calc.
smvp loop – Symmetric MV Product
bjt loop – traverse BJT devices & update mesh
Scatter Element Nodal Forces Platq loop

Coverage
∼ 90%
∼ 90%
∼ 70%
∼ 80%
∼ 70%
11–45%
∼ 30%

# inp

4
4
4
3
2
4
1

Table 4. Scientiﬁc Applications and Reduction Loops

, the loop instances in [t, t

tions changes at instance t and the next change of the
(cid:1) − 1]
(cid:1)
pattern is at instance t
form a dynamic phase. We can therefore group the ex-
ecution instances of irregular reduction loops into dy-
namic phases. We deﬁne the re-usability of a phase as
the number of dynamic instances of the reduction loop
in that phase. It intuitively gives the number of times a
loop can be considered invariant and thus does not need
a new adaptive algorithm selection. That is, the overhead
of selecting a better algorithm at run-time can be amor-
tized over re-usability instantiations.

Tseq

We then include re-usability into our previously
developed models, predict
the phase-wise perfor-
mance and thus obtain a better overall performance.
This model will be employed at
run-time to de-
cide when it is worth changing the algorithm. The
phase–wise speedup of a parallel algorithm can be for-
mulated as (R−1)Tpar+(1+O)Tpar
. The best algorithm
is the algorithm that has the smallest value of R+O
Speedup .
In above formulas, R is the re-usability, O is the ra-
tio of the set-up phase overhead (in time) and the
parallel execution time of one instance of the par-
allel
loop applying the considered algorithm, and
Speedup is the speedup (relative to sequential ex-
ecution) of the considered algorithm excluding the
set-up phase overhead. Tpar and Tseq are the paral-
lel and sequential execution times of the loop. Using
the same off-line experimental process described pre-
viously, we have generated models to compute O,
the setup overhead ratio. Together with the pre-
dicted speedup (excluding the setup overhead), we
R+O
Speedup at run–time and se-
were able to evaluate
lect the best algorithm for a dynamic execution phase.

The re-usability R can be either computed by the
compiler if it can statically detect the phase changes,
i.e., the number of iterations of the surrounding loop
for which the address pattern remains constant (a well
known technique named ’schedule reuse’ [19]) or esti-
mate on-the-ﬂy with a simple statistical method (e.g.,
running average value of R). In our experiments we have
used the schedule reuse technique and the simple statisti-
cal method. As a general rule, simulations do not change
phase very fast so re-usability is high.

6. Evaluation of Algorithm Selection

In this section we evaluate our automatically gener-
ated performance models. We compare the actual per-
formance of the algorithm selected by our automatically
generated predictive models with the other algorithms.

6.1. Experimental Setup

We selected seven programs from a variety of sci-
entiﬁc domains (see Table 4). All Fortran codes were
automatically instrumented to collect information about
the access pattern using the Polaris compiler [2]; the
C programs were done manually. For most programs,
we chose or speciﬁed multiple inputs to explore input-
dependent behavior. While specifying the inputs, we
have tried, where possible, to use data sets that exercise
the entire memory hierarchy of our parallel machine.

We have studied two parallel systems: an UMA HP
V-Class with 16 processorsand a NUMA IBM Regatta
system with 32 processors.The machine conﬁgurations
are brieﬂy described in Table 5.

HP V2200
PA-8200
200 MHz

IBM Regatta
POWER 4
1300 MHz

2 MB
4 GB

32KB/1.48MB/32MB

CPU Type
CPU Clock
Data Cache
Memory
# CPUs
Topology
OS
Compiler
Table 5. Experimental parallel systems

32 / 4 MCMs
buses / ring

HP-UX 11.0
HP f90, c89

xlf r, xlc r

crossbar

64 GB

AIX 5

16

We used an 8-processor subsystem of the HP and a
16-processor subsystem of the IBM, which was sufﬁ-
cient for us to exercise the architectural characteristics
of these two systems. We ran 22 application/input com-
binations on the HP and due to resource limitations only
21 application/input cases on the IBM system (no results
were obtained for FMA3D).

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

1

0.8

0.5

s
p
u
d
e
e
p
S
 
d
e
z

i
l

a
m
r
o
N

1

0.8

0.5

s
p
u
d
e
e
p
S
d
e
z

 

i
l

a
m
r
o
N

Recommended algorithms for the missed cases

0
0
1
=
C

0
5
=
C

5
=
C

1
=
C

2
=
C

5
=
C

0
5
=
C

0
0
2
=
C

4
1
2
=
C

0
7
=
C

1
2
=
C

6
=
C

8
1
=
C

9
=
C

5

.

4
=
C

k
1
2
=
N

k
0
9
=
N

k
0
8
1
=
N

k
9
9
=
N

k
0
9
=
N

k
4
3
=
N

5

.

0
=
C

RepBuf
RepLink
SelPriv
LocalWr

Irreg

Nbf

Moldyn

Charmm

Spark98

Spice

Fma3d

Applications and Input Cases

Recommended algorithms for the missed cases

0
0
1
=
C

0
5
=
C

5
=
C

1
=
C

2
=
C

5
=
C

0
5
=
C

0
0
2
=
C

4
1
2
=
C

0
7
=
C

1
2
=
C

6
=
C

8
1
=
C

9
=
C

5

.

4
=
C

k
1
2
=
N

k
0
9
=
N

k
0
8
1
=
N

k
9
9
=
N

k
0
9
=
N

k
4
3
=
N

RepBuf
RepLink
SelPriv
LocalWr

Irreg

Nbf

Charmm
Applications and Input Cases

Moldyn

Spark98

Spice

Figure 6. Relative speedups of parallel reduction algorithms on HP and IBM systems

6.2. Results

Fig. 6 presents the results obtained on both systems.
Each group of bars shows the relative performance (nor-
malized to the best speedup obtained for that group)
of the four parallel reduction algorithms for one pro-
gram/input case. In most cases, the algorithm rankings
resulting from the automatically generated regression
model were consistent with the actual rankings. Over-
all, our regression model correctly identiﬁed the best al-
gorithm in 18/22 cases on the HP and 19/21 cases on
the IBM. As the arrows in the graphs show, for all the
mis-predicted cases there was little performance differ-
ence between the best and recommended algorithms.

To give a quantitative measure of

the perfor-
mance improvement obtained using the algorithms
recommended by our models, we compute the rela-
tive speedup which we deﬁne as
the ratio between
the speedup of the algorithm chosen by an “alterna-
tive selection method” and the algorithm recommended
by “our model”. We have compared the effective-
ness of our predictive models against four “alternative
selection methods”. The Best is a “perfect predic-
tive model”, or an “oracle”, that always selects the
best algorithm for a given loop–input case. RepBuf al-
ways applies REPBUF, which is the simplest algorithm
and it is speciﬁed as default in OpenMP [17]. Ran-
dom randomly selects a parallel algorithm. The speedup

obtained is the average speedup of all
the candi-
date parallel algorithms for that case. Default always
applies the “default” parallel algorithm for a given plat-
form. Based on our experimental results, we chose SEL-
PRIV and LOCALWR as the default algorithms for the
HP and IBM systems, respectively.

Fig. 7 gives the average relative speedups (across
all the loop-input cases). In the graph, the smaller the
speedup ratio value, the better the relative effectiveness
of our predictive models. The graph shows that our au-
tomatically generated predictive models work almost as
well as the “perfect predictive models”, obtaining more
than 98% of the best possible performance. Comparing
to other “non-perfect” selection methods, our predictive
models improve the performance signiﬁcantly.

s
p
u
d
e
e
p
S
−
e
v
i
t

l

 

a
e
R
e
g
a
r
e
v
A

1
0.9
0.8
0.7
0.6
0.5

RepBuf
Random
Default
Best

HP V−Class, P=8

IBM Regatta, P=16

Machines and # Processors

Figure 7. Average relative-speedups

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

Phase-Wise Execution Time (N=186624)

Step-Wise Execution Time (N=186624)

 2.5

 2

 1.5

 1

 0.5

e
m
i
T
 
n
o
i
t
u
c
e
x
E

DynaSel
LocalWr
RepBuf
SelPriv

 0

 0

 20
Adaptation Phases ( steps per phase)

 15

 10

 5

e
m
i
T
 
n
o
i
t
u
c
e
x
E

 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0

DynaSel
LocalWr
RepBuf
SelPriv

 0

 10

 20

 30

 40

 50

 60

Time Steps

Figure 8. Effects of dynamically selecting algorithms of MOLDYN (input #4)

As mentioned previously, the overhead of measuring
OTH is negligible by using a light-weight timer in few
iterations; the overhead of computing SP and CLUS is
proportional to the size of the data. Across all cases, the
average overheads on the HP and IBM systems are, re-
spectively, 11.95% and 11.65%, which can be largely
amortized since the overhead only incurs in loop in-
stances where the reduction pattern change.

Finally, we note that knowledge of the dynamically
collected parameters (N, CON, OTH, SP and CLUS) is
very important for our models. From Fig. 6, especially
for the bars corresponding to the results for Irreg and
Charmm on the HP system, the best schemes for dif-
ferent inputs change and our technique predicts the best
schemes correctly by collecting and utilizing the dynam-
ically collected parameters.

To test the robustness and generality of our predic-
tive models, we have applied them to two regular re-
duction loops: loop loops do400 in su2cor from
the SPEC’92 suite, and loop actfor do500 in bdna
from the PERFECT suite. For regular reduction loops,
the size of the reduction data (usually a vector) is relative
small and every element of the vector is accessed in ev-
ery loop iteration. Either selectively replicating the vec-
tor data or partitioning the vector will not help perfor-
mance (reducing cross–processor communications). The
experimental results indicate that REPBUF is always the
best algorithm and that our predictive models have given
the correct recommendations. Hence, our adaptive algo-
rithm selection technique is generally applicable to all
reductions.

7. Experiments on Dynamic Programs

In this section, we present experimental results show-
ing that our adaptive algorithm selection technique can
select the best parallelization scheme dynamically and
improve the overall performance of adaptive programs.

Here, DYNASEL represents applying algorithm selec-
tion dynamically for every computation-phase.

7.1. Molecular Dynamics (MOLDYN)

MOLDYN is a synthetic benchmark, conduct-
ing non-bonded force calculations in a molecular dy-
namics simulation. It has been widely used for the
purpose of evaluating systematic optimization trans-
formations [7, 4, 15]. For the program used in our ex-
perimentation, we replaced the step in MOLDYN that
building a reusable neighbor list with a link-cell al-
gorithm [18]. With the modiﬁcation,
the execution
time of MOLDYN is dominated by the force computa-
tion, which is the irregular reduction we would like to
optimize.

ID

#molecules

cut-off
radius

#steps

#phases

1
2
3
4
5

4.0
2.5
2.0
1.5
1.2

23328
186624
100800
186624
186624
Table 6. Inputs of MOLDYN

60
60
60
60
60

23
23
21
21
21

avg.
CON
157
37.6
21.8
6.6
5.4

For MOLDYN, since we have not observed much
performance change across dynamic phases for paral-
lel algorithms, we artiﬁcially set the re-usability (the
number of steps) for each dynamic phase so that the
phase-wise best algorithms can change due to the differ-
ent setup overheads of the algorithms. This way, we can
examine both the effectiveness of the prediction models
and the efﬁciency of selecting and switching algorithms.
We experiment with 5 different inputs on the 8-processor
subsystem of our HP V-class machine. The input speci-
ﬁcations are given in Table 6. Note that different phases
may have different numbers of time steps.

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

Due to space limitations, we show the step-wise and
phase-wise execution time Fig. 8 for one input. We note
that since the number of steps of the phases may change,
therefore in the phase-wise plots, we plotted the accu-
mulated time of each phase instead of the average time.
Again, the results show that DYNASEL out-performs ap-
plying any one algorithm for most cases.

Relative Reduction Speedups (Moldyn)

1.1
1.05
1
0.9

0.75

0.5

p
u
d
e
e
p
S
e
v
i
t

 

l

a
e
R

RepBuf
SelPriv
LocalWr
DynaSel

23328

55296

108000

186624

Input Data Size

186624

Figure 9. Results for MOLDYN

Fig. 9 gives the relative speedups of the reduction
loop across all steps for different parallel reduction al-
gorithms. The speedups are normalized to the best re-
sults obtained by applying one algorithm to clarify the
improvement. The conclusions here are that using our
adaptive technique to select and apply the best parallel
reduction algorithm for every dynamic phase has little
overhead and that it out-performs any other single algo-
rithm and improves overall performance up to 8%;

7.2. PP2D in FEATFLOW

We have applied our adaptive technique to the PP2D
code from the FEATFLOW package [20]. FEATFLOW
is a general purpose, F77 library of subroutines for solv-
ing incompressible Navier-Stokes equations in 2D and
3D. More speciﬁcally, PP2D solves nonlinear coupled
equations using multi-grid solvers. It has about 17,000
lines of code, excluding supporting library routines.

After proﬁling for our test input set, we have se-
lected a loop with irregular reductions from subroutine
GUPWD (about 11% of whole program execution time).
The loop updates a sparse matrix with old velocity val-
ues associated with grid nodes. The memory access pat-
tern of the irregular reduction is fully determined by the
indirection data structures deﬁning the sparse matrix.
The program uses a multi-grid method to solve linear
systems in each time step. The multi-level grids are pre-
deﬁned (speciﬁed in the input data ﬁle) and the sparse
matrix structures associated with the different grids are
deﬁned in an initialization step, before the time-step
loop. For our input data the program used 4 grids, with

grid levels between 2 and 5 (in general, the code could
use as many as 9 grid sizes). The studied reduction loop
uses the 4 sparse-matrix data structures (the four grids)
in an interleaved manner. The matrices themselves, i.e.,
the reduction access pattern, do not change, only their
interleaved invocation does. We then apply our adap-
tive algorithm selection technique for every invocation
of the reduction loop. By using the well known sched-
ule reuse [19] technique we can record our selection for
each of the four reduction patterns corresponding to each
of the four grids used by the code. The selection decision
is then reused for subsequent invocations of the loop. In
this manner we reduce the overhead associated with dy-
namic selection to only 4 instances (the 4 grids do not
change during program).

Detailed information about the used input, which is
distributed together with the source code for benchmark-
ing purposes, is shown in Table 7.

2
3
4
5

Level

#unknowns
12930
52620
206280
824720

#instances
86
86
86
166
Table 7. Grid parameters of PP2D input

#elements
920
3680
14720
58880

#nodes
1890
7460
29640
118160

Fig. 10 gives the speedups of the GUPWD loop ob-
tained, for each grid level, using three reduction tech-
niques, as well as the dynamic selection scheme. All
bars in the graph are normalized to the optimal scheme.
The group ’Total’ shows the normalized performance
of the loop, across all invocations, for the case when the
schemes would be selected only at the beginning of the
program as well as for the dynamic selection case when
a decision is made for every loop instance but the deci-
sion is reused.

On the HP system, although our DYNASEL has se-
lected the best algorithm (SELPRIV for all the grid lev-
els), it did not improve the overall performance of the
loop because the SELPRIV scheme performs best in all
cases. Noteworthy is the fact that the overhead of the dy-
namic scheme has not hurt performance.

On the IBM system, the results show that the perfor-
mance of DYNASEL align to the actual best algorithm
for each grid level. More importantly though, DYNASEL
obtains the overall best performance because the choice
of the optimal algorithm varies across grid levels, i.e.,
across the instances of the reduction loop. If we would
select only once the best scheme then performance could
degrade when the grid level changed. The use of the de-
cision reuse technique, keeps the effect of dynamic se-
lection overhead to a minimum.

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

Relative Reductions Speedup, HP V−Class, P=8

Relative Reductions Speedup, IBM Regatta, P=16

1.1
1

0.8

0.5

p
u
d
e
e
p
S
 
e
v
i
t
a
e
R

l

RepBuf
SelPriv
LocalWr
DynaSel

1.1
1

0.8

0.5

p
u
d
e
e
p
S
 
e
v
i
t
a
e
R

l

RepBuf
SelPriv
LocalWr
DynaSel

Total

Lev=5

Lev=4

Lev=3

Lev=2

Total

Lev=5

Lev=4

Lev=3

Lev=2

Figure 10. Results for PP2D

This experiment shows that we can apply our dy-
namic algorithm selection method even to dynamic pro-
grams in an always proﬁtable manner. It shows that even
if performance improvement in our particular case was
modest (upto 8%), our framework can take advantage of
instance speciﬁc opportunity for optimization.

8. Related Work

There does not appear to be a great body of effort in
the area of adaptive selection of high level algorithms.
Brewer[3] is probably the most extensive previous work
aimed at making a framework for this decision process.
In this approach, performance models are consulted to
determine the expected running time of possible imple-
mentation, with the minimum running algorithm then
chosen for use. These models are linear systems given,
along with some code annotations, by the end user.

Li, Garzaran, and Padua[13] present an approach for
choosing between several sequential sorting algorithms
based on data size, data entropy, and an installation
benchmarking phase that correctly selects the best al-
gorithm for the given situation. However, no attempt
is made to generalize the technique into a general ap-
proach and no discussion of the more difﬁcult parallel
case is given. Many previous work aim at tuning spe-
ciﬁc algorithm parameters. Examples are Spiral[23] and
FFTW[6] for FFT signal processing and Atlas[22] for
matrix multiplication. These approaches, though quite
effective, are very narrow in scope and do not constitute
a general framework for generic algorithm selection.

Another somewhat relevant approach is that of dy-
namic feedback [5] which selects code variants based on
on-line proﬁling. There are many recent efforts to de-
velop dynamic and adaptive compilation systems, how-
ever we feel they are out of scope for this work.

Both data afﬁliated loop [14] method and Lo-
cal Write [7] follow “owner computes” rule. Disadvan-
tages of these techniques are their potential to heavily

replicate unnecessary computation and cause load im-
balance, unlike data replication based methods.

Zoppetti and Agrawal [27] proposed a parallel reduc-
tion algorithm for multi-threaded architectures that can
overlap computation and communication. They also im-
plemented an incremental inspector that overlaps com-
putation and communication and update the computa-
tion schedule efﬁciently. The potential drawback of this
technique is that it may introduce unnecessary commu-
nication, e.g., pipelining data sections to irrelevant com-
putation threads.

Adaptive Data Repository (ADR) infrastructure [11]
was developed to perform range queries with user-
deﬁned aggregation operations on multi-dimensional
datasets, which are generalized reductions. In the ADR
infrastructure,
three strategies are used: fully repli-
cated accumulation, sparsely replicated accumulation,
distributed accumulation, which are analogous to REP-
BUF, SELPRIV and LOCALWR discussed in this paper.
Their experiments have shown that none of the strate-
gies worked the best for various query patterns and a
predictive model was desired.

9. Conclusion

In this paper, we presented an Adaptive Algorithm
Selection Framework that can automatically adapt to
the input data, environment and machine and select the
best performing algorithm. We have applied our frame-
work to the adaptive selection of parallel reduction al-
gorithms. We have identiﬁed a few high-level, archi-
tecture independent parameters characterizing a pro-
gram’s static structure, dynamic data access patterns and
candidate transformations. We applied an off-line syn-
thetic experimental process to automatically generate
predictive models which are used to dynamically select
the most appropriate optimization transformation among
several functionally equivalent candidates.

Through experimental results, we have shown that

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

[13] X. Li, M. J. Garzaran, and D. A. Padua. A dynamically
In Proc. 2004 Int’l Symp. Code

tuned sorting library.
Generation and Optimization, pp. 111–122, 2004.

[14] Y. Lin and D. A. Padua. On the automatic paralleliza-
tion of sprase and irregular fortran programs.
In Proc.
Int’l Wkshp. Lang., Compilers, and Run-time Systems for
Scalable Computers, pp. 41–56, 1998.

[15] J. Mellor-Crummey, D. Whalley, and K. Kennedy. Im-
proving memory hierarchy performance for irregular ap-
plications using data and computation reorderings. Int’l
J. Parallel Prog., 29(3):217–247, 2001.

[16] A. Miller. Subset Selection in Regression (Second Edi-

tion). Chapman & Hall/CRC, Boca Raton, FL, 2002.

[17] OpenMP Architecture Review Board. OpenMP Fortran

Application Program Interface, Version 2.0, 2000.

[18] S. Plimpton. Fast parallel algorithms for short-range

molecular dynamics. J. Comp. Phys., 117:1–19, 1995.

[19] J. H. Saltz, R. Mirchandaney, and K. Crowley. Run-
time parallelization and scheduling of loops. IEEE Trans.
Computers, 40(5):603–612, May 1991.

[20] S. Turek and C. Becker. FEATFLOW: Finite Element
Software for The Incompressible Navier-Strokes Equa-
tions, User Manual, Release 1.1. University of Heidel-
berg, Institute for Applied Mathmatics, Germany, 1998.
[21] R. von Hanxleden. Handling irregular problems with
Fortran D – a preliminary report. In Proc. Wkshp. Com-
pilers for Parallel Computers, pp. 353–364, 1993.

[22] R. C. Whaley, A. Petitet, and J. Dongarra. Automated
empirical optimizations of software and the ATLAS
project. Parallel Computing, 27(1–2):3–25, 2001.

[23] J. Xiong, J. Johnson, R. Johnson, and D. A. Padua. SPL:
In Proc.
A language and compiler for dsp algorithms.
ACM Conf. Prog. Lang. Design and Implementation, pp.
298–308, 2001.

[24] H. Yu, F. Dang, and L. Rauchwerger. Parallel reduc-
tion: An application of adaptive algorithm selection. In
Proc. Wkshp. Lang. Compilers for Parallel Computing,
pp. 171–185, 2002.

[25] H. Yu and L. Rauchwerger. Adaptive reduction paral-
In Proc. ACM Int’l Conf. Super-

lelization techniques.
computing, pp. 66–77, 2000.

[26] H. P. Zima. Supercompilers for Parallel and Vector Com-

puters. ACM Press, New York, NY, 1991.

[27] G. M. Zoppetti, G. Agrawal, and R. Kumar. Compiler
and runtime support for irregular reductions on a mul-
tithreaded architecture. In CDROM Proc. Int’l Parallel
and Distributed Processing Symp., 2002.

our technique can select the most appropriate parallel re-
duction algorithms at run–time with very low overhead.
When this technique is applied to dynamic programs se-
lecting new algorithms for each of their dynamic phases
performance is improved even further.

The importance of this work is that the presented
adaptive optimization technique can model programs
with irregular and dynamic behavior and customize so-
lutions to each program instance. It is a general frame-
work that can adapt any number of optimizations to the
program’s needs.

References

[1] P. An and et al. STAPL: An adaptive, generic parallel
c++ library. In Proc. Wkshp. Lang. Compilers for Paral-
lel Computing, pp. 193–208, 2001.

[2] W. Blume and et al. Advanced program restructuring for
IEEE Com-

high-performance computers with Polaris.
puter, 29(12):78–82, 1996.

[3] E. A. Brewer. High-level optimization via automated sta-
In Proc. ACM Symp. Principles and

tistical modeling.
Practice of Parallel Prog., pp. 80–91, 1995.

[4] C. Ding and K. Kennedy. Improving cache performance
of dynamic applications with computation and data lay-
out transformations.
In Proc. ACM Conf. Prog. Lang.
Design and Implementation, pp. 229–241, 1999.

[5] P. C. Diniz and M. C. Rinard. Dynamic feedback: An ef-
fective technique for adaptive computing. In Proc. ACM
Conf. Prog. Lang. Design and Implementation, pp. 71–
84, 1997.

[6] M. Frigo. A fast fourier transform compiler.

In Proc.
ACM Conf. Prog. Lang. Design and Implementation, pp.
169–180, 1999.

[7] H. Han and C.-W. Tseng. Improving compiler and run-
time support for adaptive irregular codes. In Proc. IEEE
Int’l Conf. Parallel Architectures and Compilation Tech-
niques, pp. 393–400, 1998.

[8] Y.-S. Hwang and et al. Runtime and language support
for compiling adaptive irregular programs on distributed-
memory machines. Software - Practice and Experience,
25(6):597–621, 1995.

[9] R. Iyer, N. M. Amato, L. Rauchwerger, and L. Bhuyan.
Comparing the memory system performance of the HP
V-Class and SGI Origin 2000 multiprocessors using mi-
crobenchmarks and scientiﬁc applications. In Proc. ACM
Int’l Conf. Supercomputing, pp. 339–347, 1999.

[10] R. Jain. The Art of Computer Systems Performance Anal-

ysis. John Wiley & Sons, Inc., Hoboken, NJ, 1991.

[11] T. Kurc and et al. Querying very large multi-dimensional

datasets in adr. In Proc. Supercomputing ’99, 1999.

[12] F. T. Leighton. Introduction to Parallel Algorithms and
Architectures: Arrays, Trees, Hypercubes. Morgan Kauf-
mann, San Francisco, CA, 1992.

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

An Adaptive Algorithm Selection Framework∗

Hao Yu

IBM T. J. Watson Research Ctr
Yorktown Heights, NY 10598

yuh@us.ibm.com

Dongmin Zhang

Parasol Lab, Dept of CS
Texas A&M University
dzhang@cs.tamu.edu

Lawrence Rauchwerger
Parasol Lab, Dept of CS
Texas A&M University
rwerger@cs.tamu.edu

Abstract

Irregular and dynamic memory reference patterns
can cause performance variations for low level algo-
rithms in general and for parallel algorithms in partic-
ular. We present an adaptive algorithm selection frame-
work which can collect and interpret the inputs of a par-
ticular instance of a parallel algorithm and select the
best performing one from a an existing library. In this
paper present the dynamic selection of parallel reduc-
tion algorithms. First we introduce a set of high-level pa-
rameters that can characterize different parallel reduc-
tion algorithms. Then we describe an off-line, system-
atic process to generate predictive models which can be
used for run-time algorithm selection. Our experiments
show that our framework: (a) selects the most appro-
priate algorithms in 85% of the cases studied, (b) over-
all delievers 98% of the optimal performance, (c) adap-
tively selects the best algorithms for dynamic phases of
a running program (resulting in performance improve-
ments otherwise not possible), and (d) adapts to the un-
derlying machine architecture (tested on IBM Regatta
and HP V-Class systems).

1. Introduction

Improving performance on current parallel proces-
sors is a very complex task which, if done “by hand” by
programmers, becomes increasingly difﬁcult and error
prone. Programmers have obtained increasingly more
help from parallelizing (restructuring) compilers. Such
compilers address the need of detecting and exploit-
ing parallelism in sequential programs written in con-
ventional languages as well as parallel languages (e.g.,
HPF). They also optimize data layout and perform other

∗ Research was performed at Texas A&M and supported in part by
NSF CAREER Award CCR-9734471, NSF Grant ACI-9872126,
NSF Grant EIA-0103742, NSF Grant ACI-0326350, NSF Grant
ACI-0113971, DOE ASCI ASAP Level 2 Grant B347886

transformations to reduce and hide memory latency, the
other crucial optimization in modern large scale parallel
systems. The success in the “conventional” use of com-
pilers to automatically optimize code is limited to the
cases when performance is independent of the input data
of the applications. When codes are irregular (memory
references are irregular) and/or dynamic (change dur-
ing the execution of the same program instance) it is
very likely that important performance affecting pro-
gram characteristics are input and environment depen-
dent. Many important (frequently used and time con-
suming) algorithms used in such programs are indeed in-
put dependent. We have previously shown [25] that, for
example, parallel reduction algorithms are quite sensi-
tive to their input memory reference pattern and system
architecture. In [1] we have shown that parallel sorting
algorithms are sensitive to architecture, data type, size,
etc. One of the most powerful optimizations compilers
can employ is to substitute entire algorithms instead of
trying to perform low level optimizations on sequences
of code. In [1] and [25] we have shown that performance
improves signiﬁcantly if we dynamically select the best
algorithm for each program instance.

In this paper, we present a general framework to auto-
matically and adaptively select, at run-time, the best per-
forming, functionally equivalent algorithm for each of
their instantiations. The adaptive framework can select,
at run-time, the best parallel algorithm from an exist-
ing library. The selection process is based on an off-line
automatically generated prediction model and algorithm
input data collected and analyzed dynamically. For this
paper we have concentrated our attention on the auto-
matic selection of reduction algorithms. For brevity we
will not show in this paper the dynamic selection of par-
allel sorting algorithms.

Reductions (aka updates) are important because they
are at the core of a very large number of algorithms and
applications – both scientiﬁc and otherwise – and there
is a large body of literature dealing with their paralleliza-
tion. More formally, a reduction variable is a variable

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

whose value is used in one associative and possibly com-
mutative operation of the form x = x ⊗ exp, where
⊗ is the operator and x does not occur in exp or any-
where else in the loop. With the exception of some sim-
ple methods using unordered critical sections (locks),
reduction parallelization is performed through a simple
form of algorithm substitution. For example, a sequen-
tial summation is a reduction which can be replaced by a
parallel preﬁx, or recursive doubling, computation [12].
In the case of irregular, sparse programs, we deﬁne irreg-
ular reductions as reductions performed on data refer-
enced in an irregular pattern, usually through an index
array. For irregular reductions, the access pattern tra-
versed by the sequential reduction in a loop is often sen-
sitive to the input data or computation. Therefore, as we
have shown in [25], not all parallel reduction algorithms
or implementations are equally suited as substitutes for
the original sequential code. Each access pattern has its
own characteristics and will best be parallelized with an
appropriately tailored algorithm.

In [25] we have presented a small library of paral-
lel reduction algorithms and shown that the best perfor-
mance can be obtained only if we dynamically select the
most appropriate one for the instantiated input set (ref-
erence pattern). Also in [25] we have presented a taxon-
omy of reduction reference patterns and sketched a de-
cision tree (derived manually) based scheme.

We continue this work and introduce a systematic
and automatic process to generate predictive models that
match the parallel reduction algorithms to execution in-
stances of reduction loops. After manually establishing
a small set of parameters that can characterize irregu-
lar memory references and setting up a library of paral-
lel reduction algorithms [25], we measure their relative
performance for a number of memory reference param-
eters in a factorial experiment. This is achieved by run-
ning a synthetic loop which can generate reduction ref-
erences with the memory reference patterns selected by
our factorial experiment. The end result of this off-line
process is a mapping between various points in the mem-
ory reference pattern space and the best available reduc-
tion algorithm. At run-time, the memory reference char-
acteristics of the actual reduction loop are extracted and
matched through a regression to the corresponding best
algorithm (using our previously extracted map).

This paper’s main contribution is a framework for a
systematic process through which input sensitive predic-
tive models can be built off-line and used dynamically to
select from a particular list of functionally equivalent al-
gorithms, parallel reductions being just on important ex-
ample. The same approach could also be used for vari-
ous other compiler transformations that cannot be easily
analytically modeled. Our experiments on an IBM Re-

gatta and HP V-Class show that the framework: (a) se-
lects the best performing algorithms in 85% of the cases
studied, (b) overall delievers 98% of the optimal perfor-
mance, (c) achieves better performance by adaptively se-
lecting the best algorithm for the dynamic phase of a
running program, (d) adapts to machine architecture.

2. Framework Overview

In this section we give an overview of our general
framework for adaptive and automatic low level algo-
rithm selection, the details of which are presented in the
remainder of this paper as applied to the optimization of
parallel reduction algorithm selection.

Fig. 1 gives an overview of our adaptive framework.
We distinguish two phases: (a) a setup phase and (b) a
dynamic selection (optimization) phase.

The setup phase occurs once for each computer sys-
tem and thus, implicitly tailors our process to a particu-
lar architecture. We then establish the input domain and
the output domain (possible optimizations) of the algo-
rithm selection code.

In the particular case presented in this paper, i.e., par-
allel reductions, the input domain is the universe of all
possible and realistic memory reference patterns — be-
cause, because they crucially impact the obtained per-
formance [25]. Architecture type is also important, but
is used implicitly. Since it would be impractical to study
the entire universe of memory access patterns, we de-
ﬁne a small set of parameters that can sufﬁciently char-
acterize them. The domain of possible optimizations is
composed, in our case, of the different parallel reduc-
tion algorithms collected in a library. Here, we need
to note that the characteristic parameters for selecting
algorithms for different optimizations can be different.
To avoid extensive setup process, high level parameters
(usually need to be identiﬁed manually) are preferred.
For instance, the characteristic parameters for selecting
parallel sorting algorithms [1] are different from what
we choose for reduction parallelization.

We then explore our input domain and ﬁnd a map-
ping to the output domain. In our case we establish a
mapping between different points in the input parame-
ter space (memory reference patterns) and relative per-
formance rankings of the available algorithms. This task
is accomplished off-line by running a factorial exper-
iment. We generate a number of parameter sets that
have the potential cover our input domain. For each of
these data points, we measure the relative performance
of our algorithms on the particular architecture, and rank
them accordingly. We should mention here that we have
also tried other methods of exploring the data space.

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

Setup Phase

Optimization Phase

Application

Synthetic
Experiments

Model
Derivation

Optimizing Compiler

Adaptive Executable

Yes

Characteristics
Changed ?

Algo. Selection Code

Select Algo.

Selected Algo.

Figure 1. Adaptive Algorithm Selection

In [24, 1], we have used a machine learning algorithmto
explore the input space that deﬁnes performance.

The dynamic selection phase occurs during actual
program execution. Through instrumentation (or other-
wise) we extract the set of relevant parameters that char-
acterizes the actual input. Then we used the informa-
tion obtained during the setup phase to ﬁnd its corre-
sponding output. Speciﬁcally, we use statistical regres-
sion to eventually select the appropriate best perform-
ing parallel reduction algorithm. In [1] and [24], in turn,
we used statically generated decision trees which are dy-
namically traversed to select the best algorithms.

In this paper we specialize our adaptive framework
to parallel reduction algorithm selection. In the follow-
ing, we ﬁrst present our library of reduction algorithms
and a set of parameters that can characterize their ir-
regular memory reference patterns. Then we describe
the factorial experiment that explores our input space.
For each parameter set, we execute a synthetic parame-
terized loop that generates a memory reference pattern
on which we evaluate and rank the different algorithms
in our library. A regression method is used to compute
prediction models for each parallel reduction algorithm
based on the data from the factorial experiment. At run-
time, we compute values for the characterization param-
eters of the reduction operation in question and use them
in our pre-computed models to select the best algorith-
mic option. We then evaluate our framework experimen-
tally for static and dynamic reference patterns.

3. Reduction Algorithm Library

Reductions are associative recurrences and they can
be parallelized in several ways. Our library currently
contains two types of methods: direct update methods,
which update shared reduction variables during loop ex-
ecution, and private accumulation and global update
methods, which accumulate in private storage during
loop execution and update shared variables with each
processor’s contribution afterwards.

Direct update methods include the classical recursive
doubling [12], unordered critical sections [26] and local
write [7]; our library only includes local write because
the others are not as competitive. Local Write (LO-
CALWR) is originally described in [7], LOCALWR ﬁrst
collects the reference pattern in an inspector loop [19]
and partitions the iteration space based on the “owner–
computes” rule. Memory locations referenced on multi-
ple processors have their iterations replicated on those
processors as well. To execute the reduction(s), each
processor goes through its local copies of the iterations
containing the reduction(s) and operating on the proces-
sor’s local data.

We have implemented three private accumula-
tion methods in our library. Replicated Buffer (REP-
BUF) [12, 14] simply executes the reduction loop as a
doall, accumulating partial results in private reduc-
tion variables, and later accumulates results across pro-
cessors. Replicated Buffer With Links (REPLINK)
[25] avoids traversing the unused (but allocated) el-
ements during REPBUF’s cross-processor
reduction
phase by keeping a linked list to record the proces-
sors that access each shared element. Selective Privati-
zation (SELPRIV) [25] only privatizes array elements
that have cross-processor contention. By excluding un-
used private elements, SELPRIV maintains a dense pri-
vate space where most elements are used. Since the
private space does not align to the shared data ar-
ray, the reduction’s index array is modiﬁed to redirect
accesses to the selectively privatized elements.

Issues

REPBUF

REPLINK

SELPRIV

LOCALWR

Good when
contention is

Locality

Need schedule

reuse [19]
Extra Work
Extra Space

High
Poor

No
No
NxP

Low
Poor

Yes
No
NxP

Low
Good

Yes
No

NxP+M

Low
Good

Yes
Yes
MxP

M: # iterations; N: data array size; P: # processors.
Table 1. Comparison of reduction algo.

Our parallel reduction algorithm library contains sev-
eral methods suitable for a range of reference patterns.
For brevity, we provide here only a high level descrip-
tion of the methods. See [25] for additional details.

In Table 1, we present a qualitative comparison of
the algorithms described above. Here, the contention of
a reduction is the average number of iterations (# pro-

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

NBF: N=25600,CON=200,P=8

25600

19200

12800

6400

e
g
a
s
U
 
r
e
f
f
u
B

1

2

MOLDYN: N=16384,CON=95.8,P=8

16384

12000

8000

4000

e
g
a
s
U
 
r
e
f
f
u
B

IRREG: N=100000,CON=100,P=8

100000

e
g
a
s
U
 
r
e
f
f
u
B

80000

60000

40000

20000

4

3
6
Processors

5

7

8

1

2

4

3
6
Processors

5

7

8

1

2

4

3
6
Processors

5

7

8

Figure 2. Memory access patterns of REPBUF

cessors when running in parallel) referencing the same
element. When contention is low, many unused repli-
cated elements in REPBUF are accumulated across pro-
cessors, while other algorithms only pass useful data.
SELPRIV works on a compacted data space and there-
fore potentially has good spatial locality. In LOCALWR,
each processor works on a speciﬁc portion of the data ar-
ray and has thus potentially good locality. With respect
to overhead, REPLINK, SELPRIV and LOCALWR all
have auxiliary data structures that depend on the access
pattern, and which must be updated when it changes.
Their overhead can be reduced by employing the sched-
ule reuse [19] technique.

4. High-Level Parameters

In this section, we describe the parameters we
have chosen to characterize reduction operations. Ide-
ally, they should require little overhead to measure and
they should enable us to select the best parallel re-
duction algorithm from our library for each reduction
instance in the program. After deﬁning the identiﬁed pa-
rameters below, we present a summary of the decoupled
effects of the parameters on the performance of the par-
allel reduction algorithms to illustrate the effectiveness
of the chosen parameters. Here, we enumerate the pa-
rameters in no speciﬁc order.

N is the number data elements involved in the reduc-
tion (often the size of the reduction array). It strongly in-
ﬂuences the loop’s working set size, and thus potentially
performance. In some applications, several reduction ar-
rays have exactly same access pattern; here N includes
the data elements for all arrays.

CON, the Connectivity of a loop, is the ratio between
the number of iterations of the loop and N. This param-
eter is equivalent to the parameter deﬁned by Han and
Tseng in [7]; there, the underlying data structures (cor-
responding to the irregular reductions) represent graphs
G = (V, E) and the authors deﬁned Connectivity
as |V |/|E|. Generally, the higher the connectivity, the
higher the ratio of computation to communication, i.e.,

if the connectivity is high, a small number of elements
will be referenced by many iterations.

MOB, the Mobility per iteration of a loop, is the num-
ber of distinct subscripts of reductions in an iteration.
For the LOCALWR algorithm, the effect of high itera-
tion Mobility (actually lack of mobility) is a high degree
of iteration replication. MOB is a parameter that can be
measured at compile time.

OTH, represents the Other (non-reduction) work in
an iteration. If OTH is high, a penalty will be paid
for replicating iterations. To measure OTH, we instru-
mented a parallel loop transformed for the REPBUF al-
gorithm using light-weight timers (∼ 100 clock cycles).
SP, the Sparsity, is the ratio of the total number of
referenced private elements and total allocated space on
all processors using the REPBUF algorithm (pN). Intu-
itively, SP indicates if REPBUF is efﬁcient.

CLUS, the Number of Clusters, reﬂects spatial local-
ity and measures if the used private elements in the REP-
BUF are scattered or clustered on each processor. Fig. 2
shows three memory access patterns which can be clas-
siﬁed as clustered, partially-clustered, and scattered, re-
spectively. Currently, SP and CLUS are measured by in-
strumenting parallel reduction loops using the REPBUF
algorithm and the overhead is proportional to the num-
ber of used private elements. CLUS measures the aver-
age number of clusters of the used private elements on
each processor.

We have investigated the decoupled effects of the pa-
rameters on the performance of the parallel reduction
algorithms. Although the decoupling is not realistic, it
is useful for discovering qualitative trends. The trends
summarized in Table 2 are based on a comprehensive
set of experiments on multiple architectures that are re-
ported in [24]. The trends for REPLINK are similar to
REPBUF and are not listed here.

The effect of N is straight–forward, comparing to
the sequential reduction loop, SELPRIV and LOCALWR
have much smaller data sets on each processor and there-
fore their speedups increase with N. CON is inversely
correlated with inter-processor communication. Hence,
larger CON values will indicate better scaling REPBUF

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

parameters

N

CON
MOB
OTH
SP

CLUS

REPBUF

(cid:2)
↑
↑
↑
(cid:2)
(cid:2)

SELPRIV

↑
(cid:2)
(cid:4)
↑
↓
↑

LOCALWR

↑
–
↓
(cid:2)
(cid:4)
–

↑: positive effect; ↓: negative effect; (cid:6): little positive
effect; (cid:7): little negative effect; –: no effect.
Table 2. Decoupled effects

and SELPRIV, which have two reduction loops, one ac-
cumulating in private space and one accumulating cross-
processor shared data. Large MOB values (many refer-
ences to index arrays) may imply poor performance
for SELPRIV, because accesses to the reduction array
must access both the original and the modiﬁed index
arrays, and for LOCALWR, because large MOB val-
ues often result in high iteration replication. Large val-
ues of OTH often indicate good performance for REP-
BUF and SELPRIV because the ﬁrst private accumula-
tion loop has a larger iteration body; since LOCALWR
replicates “other work” it will not beneﬁt as much. Low
SP is good for SELPRIV and also for LOCALWR, since
it often correlates with low contention and hence low
iteration replication. For large CLUS, since SELPRIV
compacts the sparsely used data space, it achieves better
speedups than LOCALWR and REPBUF, which work on
original (non-compacted) data or in private spaces con-
formable to the original data.

5. Adaptive Reduction Selection

In this section we will elaborate on the setup and on
the dynamic selection phases of our adaptive scheme.
The setup is executed only once, during machine instal-
lation, and generates a map between points in the uni-
verse of all inputs (memory reference patterns charac-
terized with the previously deﬁned parameters) and their
corresponding best suited reduction algorithms. The dy-
namic selection phase is executed every time a targeted
reduction is encountered. It uses the map built during
the setup phase, a parameter collection mechanism to
characterize the memory references and an interpolation
function (the actual algorithm selector) to ﬁnd the best
suited algorithm in the library.

5.1. Setup Phase

We now outline the design of the initial map between
a set of synthetically generated parameter values and the
corresponding performance ranking of the various paral-

Experimental

Parameter

Values

Parameterized

Synthetic

Reduction Loop

Experimental

Speedups

A Compact Map

MODELS:
Speedups[1..S] = F( Parameters )

Run−Time

Run−Time Algorithm Selection

Figure 3. Setup phase

lelization algorithms available in our library. The overall
setup phase, is illustrated in Fig. 3.

The domain of values that can be taken by the input
parameters is explored by setting up a factorial experi-
ment [10]. Speciﬁcally, we choose several values (typi-
cal of realistic reduction loops) for each parameter and
generate a set of experiments from all combinations of
the chosen values of the different parameters. The cho-
sen parameter values for our reduction experiment are
shown in Table 3.

To measure the performance of different reduction
patterns, we have created a synthetic reduction loop.
The structure of the loop is shown in Fig 4, with C-like
pseudo-code. The non–reduction work and reductions
have been grouped in two loop nests. The dynamic
pattern depends strictly on the index array, which is
generated automatically to correspond to the parameters
SP and CLUS. In addition, OTH is dynamically mea-
sured. The contents of the index[*] array are gen-
erated via a randomized process which satisﬁes the re-
quirement speciﬁed by the parameters.

The performance ranking of the parallel reduction al-
gorithms in our library is accomplished by simply ex-
ecuting them all for each parameter combination, i.e.,
synthetically generated access pattern, and measuring
their actual speedups, as shown in Fig. 3. The end re-
sult is a compact map from parameter values to perfor-
mance (speedups) of candidate parallel algorithms.

From this map we can now create the prediction code,
the model that can then be used by an application at run-

FOR j = 1 to N*CON

FOR i = 1 to OTH /* non-reductions */

memory read & scalar computation;

FOR i = 1 to MOB

/* reductions */

data[ index[j][i] ] += expr;

Figure 4. Synthetic reduction loop

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

time. We applied two methods, general linear regression
and decision tree learning. However, due to space con-
straints, in this paper we will describe only the modeling
process using general linear regression.

Values
256K
16

Parameters
N (array size)
Connectivity
Mobility
Other Work
Sparsity
# clusters
Table 3. Param. for factorial experiment

16K
0.2
2
1
0.02
1

64K
2
8
4
0.2
4

0.45
20

1M
128

0.75

0.99

4M

The generated models are polynomial functions from
the parameter values to the speedups of the parallel al-
gorithms. We follow a standard term selection process
that automatically selects polynomial terms from a spec-
iﬁed pool [16]. We have speciﬁed a maximal model as
F = (lg N + lg C + M OB + OT H + lg S + lg L + 1)3.
and a minimal model as F = lg N + lg C + M OB +
OT H + lg S + lg L + 1. Here, C is CON, S is SP, and L
is CLUS. Then from the minimal model, terms are ran-
domly selected from the 84 terms of the maximal model.
The samples are ﬁrst separated into training data and
testing data. When adding a term into the model, the
training data are used to ﬁt (least square ﬁt) the coefﬁ-
cients and the ﬁtted model is evaluated using the testing
data. If the test error is higher than the test error of the
model before including the new term, the term will be
dropped. Though this sequential term selection process
will not give us the optimal model, it is fast and its auto-
matically generated models have produced good results
when used to predict relative performance of the paral-
lel algorithms on real reduction loops. Here, the order of
the model, 3, is chosen mainly due to practical reasons
such as generating less experimental samples with less
synthetic experimentation time and avoiding term explo-
sion. For parameter MOB, we have only chosen 2 values
for the experiment, we have excluded the terms contain-
ing a non-linear MOB factor from the maximal model.
For OTH and CLUS, though we speciﬁed few values,
the values used in the map are measured and computed
from the generated index array.

The ﬁnal polynomial models contain about 30 terms.
The corresponding C library routines are generated au-
tomatically to evaluate the polynomial F () for each al-
gorithm at run-time.

5.2. Dynamic Selection Phase

During the dynamic selection phase, to avoid exe-
cuting the parameter collection and algorithm selection

Begin

Pattern Changed?

Yes

SCH_reuse

SCH_adapt

Compute Parameters: P
SCH = select_algorithm(P)

End

Figure 5. Reduction selection at run-time

steps for every time a reduction loop is invoked, we use
a form of memoization, decision reuse, which detects if
the inputs to our selector function have changed. When
a new instance of a reduction is encountered and the in-
put parameters have not changed from the previous exe-
cution instance, we directly reuse the previously selected
algorithm, thus saving run-time selection overhead. This
is accomplished with standard compiler technology, i.e.,
the compiler instruments two version loops for each par-
allel algorithm (illustrated in Fig. 5).

In this phase, the pre-generated model evaluation rou-
tines are called to estimate speedups of all the algo-
rithms, rank them, and select the best one. The evalu-
ation of the polynomial models is fast because each of
the ﬁnal models contains only about 30 terms.

5.3. Selection Reuse for Dynamic Programs

In the previous section we presented a systematic pro-
cess to generate predictive models that could recom-
mend the best irregular reduction parallelization algo-
rithm by collecting a set of static and dynamic param-
eters. We mentioned that if the memory characteristics
parameters do not change we can reuse our decision and
thus reduce run-time overhead. This may be of value if
the compiler can automatically prove statically that the
reference pattern does not change. If, however this is not
possible, which is often the case for irregular dynamic
codes, we have to perform a selection for each reduction
loop instance. In this section we will show how to re-
duce this overhead by evaluating a trade-off between the
run-time overhead of selection and the beneﬁt of ﬁnd-
ing a better algorithm.

To better describe the run–time behavior of a dy-
namic program, we introduce the notion of dynamic
phase. For a loop containing irregular reductions, a
phase is composed of all the contiguous execution in-
stances for which the pattern does not change. For in-
stance, assume the access pattern of the irregular reduc-

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

Program
Irreg – FE based CFD kernel
Nbf – MD kernel
Moldyn – synthetic MD program
Charmm – MD kernel
Spark98 – FE simulation
Spice – circuit simulation
Fma3d – FE method for solids

Lang.
F77
F77
C
F77
C
F77
F90

Lines
223
116
688
277
1513
18912
60122

Source
[7]
[7, 21]
[7, 4, 15]
[8]
SPEC’2000
SPEC’92
SPEC’2000

Loop
apply elements’ forces (do 100)
non-bounded force calc. (do 50)
non-bounded force calc.
non-bounded force calc.
smvp loop – Symmetric MV Product
bjt loop – traverse BJT devices & update mesh
Scatter Element Nodal Forces Platq loop

Coverage
∼ 90%
∼ 90%
∼ 70%
∼ 80%
∼ 70%
11–45%
∼ 30%

# inp

4
4
4
3
2
4
1

Table 4. Scientiﬁc Applications and Reduction Loops

, the loop instances in [t, t

tions changes at instance t and the next change of the
(cid:1) − 1]
(cid:1)
pattern is at instance t
form a dynamic phase. We can therefore group the ex-
ecution instances of irregular reduction loops into dy-
namic phases. We deﬁne the re-usability of a phase as
the number of dynamic instances of the reduction loop
in that phase. It intuitively gives the number of times a
loop can be considered invariant and thus does not need
a new adaptive algorithm selection. That is, the overhead
of selecting a better algorithm at run-time can be amor-
tized over re-usability instantiations.

Tseq

We then include re-usability into our previously
developed models, predict
the phase-wise perfor-
mance and thus obtain a better overall performance.
This model will be employed at
run-time to de-
cide when it is worth changing the algorithm. The
phase–wise speedup of a parallel algorithm can be for-
mulated as (R−1)Tpar+(1+O)Tpar
. The best algorithm
is the algorithm that has the smallest value of R+O
Speedup .
In above formulas, R is the re-usability, O is the ra-
tio of the set-up phase overhead (in time) and the
parallel execution time of one instance of the par-
allel
loop applying the considered algorithm, and
Speedup is the speedup (relative to sequential ex-
ecution) of the considered algorithm excluding the
set-up phase overhead. Tpar and Tseq are the paral-
lel and sequential execution times of the loop. Using
the same off-line experimental process described pre-
viously, we have generated models to compute O,
the setup overhead ratio. Together with the pre-
dicted speedup (excluding the setup overhead), we
R+O
Speedup at run–time and se-
were able to evaluate
lect the best algorithm for a dynamic execution phase.

The re-usability R can be either computed by the
compiler if it can statically detect the phase changes,
i.e., the number of iterations of the surrounding loop
for which the address pattern remains constant (a well
known technique named ’schedule reuse’ [19]) or esti-
mate on-the-ﬂy with a simple statistical method (e.g.,
running average value of R). In our experiments we have
used the schedule reuse technique and the simple statisti-
cal method. As a general rule, simulations do not change
phase very fast so re-usability is high.

6. Evaluation of Algorithm Selection

In this section we evaluate our automatically gener-
ated performance models. We compare the actual per-
formance of the algorithm selected by our automatically
generated predictive models with the other algorithms.

6.1. Experimental Setup

We selected seven programs from a variety of sci-
entiﬁc domains (see Table 4). All Fortran codes were
automatically instrumented to collect information about
the access pattern using the Polaris compiler [2]; the
C programs were done manually. For most programs,
we chose or speciﬁed multiple inputs to explore input-
dependent behavior. While specifying the inputs, we
have tried, where possible, to use data sets that exercise
the entire memory hierarchy of our parallel machine.

We have studied two parallel systems: an UMA HP
V-Class with 16 processorsand a NUMA IBM Regatta
system with 32 processors.The machine conﬁgurations
are brieﬂy described in Table 5.

HP V2200
PA-8200
200 MHz

IBM Regatta
POWER 4
1300 MHz

2 MB
4 GB

32KB/1.48MB/32MB

CPU Type
CPU Clock
Data Cache
Memory
# CPUs
Topology
OS
Compiler
Table 5. Experimental parallel systems

32 / 4 MCMs
buses / ring

HP-UX 11.0
HP f90, c89

xlf r, xlc r

crossbar

64 GB

AIX 5

16

We used an 8-processor subsystem of the HP and a
16-processor subsystem of the IBM, which was sufﬁ-
cient for us to exercise the architectural characteristics
of these two systems. We ran 22 application/input com-
binations on the HP and due to resource limitations only
21 application/input cases on the IBM system (no results
were obtained for FMA3D).

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

1

0.8

0.5

s
p
u
d
e
e
p
S
 
d
e
z

i
l

a
m
r
o
N

1

0.8

0.5

s
p
u
d
e
e
p
S
d
e
z

 

i
l

a
m
r
o
N

Recommended algorithms for the missed cases

0
0
1
=
C

0
5
=
C

5
=
C

1
=
C

2
=
C

5
=
C

0
5
=
C

0
0
2
=
C

4
1
2
=
C

0
7
=
C

1
2
=
C

6
=
C

8
1
=
C

9
=
C

5

.

4
=
C

k
1
2
=
N

k
0
9
=
N

k
0
8
1
=
N

k
9
9
=
N

k
0
9
=
N

k
4
3
=
N

5

.

0
=
C

RepBuf
RepLink
SelPriv
LocalWr

Irreg

Nbf

Moldyn

Charmm

Spark98

Spice

Fma3d

Applications and Input Cases

Recommended algorithms for the missed cases

0
0
1
=
C

0
5
=
C

5
=
C

1
=
C

2
=
C

5
=
C

0
5
=
C

0
0
2
=
C

4
1
2
=
C

0
7
=
C

1
2
=
C

6
=
C

8
1
=
C

9
=
C

5

.

4
=
C

k
1
2
=
N

k
0
9
=
N

k
0
8
1
=
N

k
9
9
=
N

k
0
9
=
N

k
4
3
=
N

RepBuf
RepLink
SelPriv
LocalWr

Irreg

Nbf

Charmm
Applications and Input Cases

Moldyn

Spark98

Spice

Figure 6. Relative speedups of parallel reduction algorithms on HP and IBM systems

6.2. Results

Fig. 6 presents the results obtained on both systems.
Each group of bars shows the relative performance (nor-
malized to the best speedup obtained for that group)
of the four parallel reduction algorithms for one pro-
gram/input case. In most cases, the algorithm rankings
resulting from the automatically generated regression
model were consistent with the actual rankings. Over-
all, our regression model correctly identiﬁed the best al-
gorithm in 18/22 cases on the HP and 19/21 cases on
the IBM. As the arrows in the graphs show, for all the
mis-predicted cases there was little performance differ-
ence between the best and recommended algorithms.

To give a quantitative measure of

the perfor-
mance improvement obtained using the algorithms
recommended by our models, we compute the rela-
tive speedup which we deﬁne as
the ratio between
the speedup of the algorithm chosen by an “alterna-
tive selection method” and the algorithm recommended
by “our model”. We have compared the effective-
ness of our predictive models against four “alternative
selection methods”. The Best is a “perfect predic-
tive model”, or an “oracle”, that always selects the
best algorithm for a given loop–input case. RepBuf al-
ways applies REPBUF, which is the simplest algorithm
and it is speciﬁed as default in OpenMP [17]. Ran-
dom randomly selects a parallel algorithm. The speedup

obtained is the average speedup of all
the candi-
date parallel algorithms for that case. Default always
applies the “default” parallel algorithm for a given plat-
form. Based on our experimental results, we chose SEL-
PRIV and LOCALWR as the default algorithms for the
HP and IBM systems, respectively.

Fig. 7 gives the average relative speedups (across
all the loop-input cases). In the graph, the smaller the
speedup ratio value, the better the relative effectiveness
of our predictive models. The graph shows that our au-
tomatically generated predictive models work almost as
well as the “perfect predictive models”, obtaining more
than 98% of the best possible performance. Comparing
to other “non-perfect” selection methods, our predictive
models improve the performance signiﬁcantly.

s
p
u
d
e
e
p
S
−
e
v
i
t

l

 

a
e
R
e
g
a
r
e
v
A

1
0.9
0.8
0.7
0.6
0.5

RepBuf
Random
Default
Best

HP V−Class, P=8

IBM Regatta, P=16

Machines and # Processors

Figure 7. Average relative-speedups

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

Phase-Wise Execution Time (N=186624)

Step-Wise Execution Time (N=186624)

 2.5

 2

 1.5

 1

 0.5

e
m
i
T
 
n
o
i
t
u
c
e
x
E

DynaSel
LocalWr
RepBuf
SelPriv

 0

 0

 20
Adaptation Phases ( steps per phase)

 15

 10

 5

e
m
i
T
 
n
o
i
t
u
c
e
x
E

 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0

DynaSel
LocalWr
RepBuf
SelPriv

 0

 10

 20

 30

 40

 50

 60

Time Steps

Figure 8. Effects of dynamically selecting algorithms of MOLDYN (input #4)

As mentioned previously, the overhead of measuring
OTH is negligible by using a light-weight timer in few
iterations; the overhead of computing SP and CLUS is
proportional to the size of the data. Across all cases, the
average overheads on the HP and IBM systems are, re-
spectively, 11.95% and 11.65%, which can be largely
amortized since the overhead only incurs in loop in-
stances where the reduction pattern change.

Finally, we note that knowledge of the dynamically
collected parameters (N, CON, OTH, SP and CLUS) is
very important for our models. From Fig. 6, especially
for the bars corresponding to the results for Irreg and
Charmm on the HP system, the best schemes for dif-
ferent inputs change and our technique predicts the best
schemes correctly by collecting and utilizing the dynam-
ically collected parameters.

To test the robustness and generality of our predic-
tive models, we have applied them to two regular re-
duction loops: loop loops do400 in su2cor from
the SPEC’92 suite, and loop actfor do500 in bdna
from the PERFECT suite. For regular reduction loops,
the size of the reduction data (usually a vector) is relative
small and every element of the vector is accessed in ev-
ery loop iteration. Either selectively replicating the vec-
tor data or partitioning the vector will not help perfor-
mance (reducing cross–processor communications). The
experimental results indicate that REPBUF is always the
best algorithm and that our predictive models have given
the correct recommendations. Hence, our adaptive algo-
rithm selection technique is generally applicable to all
reductions.

7. Experiments on Dynamic Programs

In this section, we present experimental results show-
ing that our adaptive algorithm selection technique can
select the best parallelization scheme dynamically and
improve the overall performance of adaptive programs.

Here, DYNASEL represents applying algorithm selec-
tion dynamically for every computation-phase.

7.1. Molecular Dynamics (MOLDYN)

MOLDYN is a synthetic benchmark, conduct-
ing non-bonded force calculations in a molecular dy-
namics simulation. It has been widely used for the
purpose of evaluating systematic optimization trans-
formations [7, 4, 15]. For the program used in our ex-
perimentation, we replaced the step in MOLDYN that
building a reusable neighbor list with a link-cell al-
gorithm [18]. With the modiﬁcation,
the execution
time of MOLDYN is dominated by the force computa-
tion, which is the irregular reduction we would like to
optimize.

ID

#molecules

cut-off
radius

#steps

#phases

1
2
3
4
5

4.0
2.5
2.0
1.5
1.2

23328
186624
100800
186624
186624
Table 6. Inputs of MOLDYN

60
60
60
60
60

23
23
21
21
21

avg.
CON
157
37.6
21.8
6.6
5.4

For MOLDYN, since we have not observed much
performance change across dynamic phases for paral-
lel algorithms, we artiﬁcially set the re-usability (the
number of steps) for each dynamic phase so that the
phase-wise best algorithms can change due to the differ-
ent setup overheads of the algorithms. This way, we can
examine both the effectiveness of the prediction models
and the efﬁciency of selecting and switching algorithms.
We experiment with 5 different inputs on the 8-processor
subsystem of our HP V-class machine. The input speci-
ﬁcations are given in Table 6. Note that different phases
may have different numbers of time steps.

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

Due to space limitations, we show the step-wise and
phase-wise execution time Fig. 8 for one input. We note
that since the number of steps of the phases may change,
therefore in the phase-wise plots, we plotted the accu-
mulated time of each phase instead of the average time.
Again, the results show that DYNASEL out-performs ap-
plying any one algorithm for most cases.

Relative Reduction Speedups (Moldyn)

1.1
1.05
1
0.9

0.75

0.5

p
u
d
e
e
p
S
e
v
i
t

 

l

a
e
R

RepBuf
SelPriv
LocalWr
DynaSel

23328

55296

108000

186624

Input Data Size

186624

Figure 9. Results for MOLDYN

Fig. 9 gives the relative speedups of the reduction
loop across all steps for different parallel reduction al-
gorithms. The speedups are normalized to the best re-
sults obtained by applying one algorithm to clarify the
improvement. The conclusions here are that using our
adaptive technique to select and apply the best parallel
reduction algorithm for every dynamic phase has little
overhead and that it out-performs any other single algo-
rithm and improves overall performance up to 8%;

7.2. PP2D in FEATFLOW

We have applied our adaptive technique to the PP2D
code from the FEATFLOW package [20]. FEATFLOW
is a general purpose, F77 library of subroutines for solv-
ing incompressible Navier-Stokes equations in 2D and
3D. More speciﬁcally, PP2D solves nonlinear coupled
equations using multi-grid solvers. It has about 17,000
lines of code, excluding supporting library routines.

After proﬁling for our test input set, we have se-
lected a loop with irregular reductions from subroutine
GUPWD (about 11% of whole program execution time).
The loop updates a sparse matrix with old velocity val-
ues associated with grid nodes. The memory access pat-
tern of the irregular reduction is fully determined by the
indirection data structures deﬁning the sparse matrix.
The program uses a multi-grid method to solve linear
systems in each time step. The multi-level grids are pre-
deﬁned (speciﬁed in the input data ﬁle) and the sparse
matrix structures associated with the different grids are
deﬁned in an initialization step, before the time-step
loop. For our input data the program used 4 grids, with

grid levels between 2 and 5 (in general, the code could
use as many as 9 grid sizes). The studied reduction loop
uses the 4 sparse-matrix data structures (the four grids)
in an interleaved manner. The matrices themselves, i.e.,
the reduction access pattern, do not change, only their
interleaved invocation does. We then apply our adap-
tive algorithm selection technique for every invocation
of the reduction loop. By using the well known sched-
ule reuse [19] technique we can record our selection for
each of the four reduction patterns corresponding to each
of the four grids used by the code. The selection decision
is then reused for subsequent invocations of the loop. In
this manner we reduce the overhead associated with dy-
namic selection to only 4 instances (the 4 grids do not
change during program).

Detailed information about the used input, which is
distributed together with the source code for benchmark-
ing purposes, is shown in Table 7.

2
3
4
5

Level

#unknowns
12930
52620
206280
824720

#instances
86
86
86
166
Table 7. Grid parameters of PP2D input

#elements
920
3680
14720
58880

#nodes
1890
7460
29640
118160

Fig. 10 gives the speedups of the GUPWD loop ob-
tained, for each grid level, using three reduction tech-
niques, as well as the dynamic selection scheme. All
bars in the graph are normalized to the optimal scheme.
The group ’Total’ shows the normalized performance
of the loop, across all invocations, for the case when the
schemes would be selected only at the beginning of the
program as well as for the dynamic selection case when
a decision is made for every loop instance but the deci-
sion is reused.

On the HP system, although our DYNASEL has se-
lected the best algorithm (SELPRIV for all the grid lev-
els), it did not improve the overall performance of the
loop because the SELPRIV scheme performs best in all
cases. Noteworthy is the fact that the overhead of the dy-
namic scheme has not hurt performance.

On the IBM system, the results show that the perfor-
mance of DYNASEL align to the actual best algorithm
for each grid level. More importantly though, DYNASEL
obtains the overall best performance because the choice
of the optimal algorithm varies across grid levels, i.e.,
across the instances of the reduction loop. If we would
select only once the best scheme then performance could
degrade when the grid level changed. The use of the de-
cision reuse technique, keeps the effect of dynamic se-
lection overhead to a minimum.

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

Relative Reductions Speedup, HP V−Class, P=8

Relative Reductions Speedup, IBM Regatta, P=16

1.1
1

0.8

0.5

p
u
d
e
e
p
S
 
e
v
i
t
a
e
R

l

RepBuf
SelPriv
LocalWr
DynaSel

1.1
1

0.8

0.5

p
u
d
e
e
p
S
 
e
v
i
t
a
e
R

l

RepBuf
SelPriv
LocalWr
DynaSel

Total

Lev=5

Lev=4

Lev=3

Lev=2

Total

Lev=5

Lev=4

Lev=3

Lev=2

Figure 10. Results for PP2D

This experiment shows that we can apply our dy-
namic algorithm selection method even to dynamic pro-
grams in an always proﬁtable manner. It shows that even
if performance improvement in our particular case was
modest (upto 8%), our framework can take advantage of
instance speciﬁc opportunity for optimization.

8. Related Work

There does not appear to be a great body of effort in
the area of adaptive selection of high level algorithms.
Brewer[3] is probably the most extensive previous work
aimed at making a framework for this decision process.
In this approach, performance models are consulted to
determine the expected running time of possible imple-
mentation, with the minimum running algorithm then
chosen for use. These models are linear systems given,
along with some code annotations, by the end user.

Li, Garzaran, and Padua[13] present an approach for
choosing between several sequential sorting algorithms
based on data size, data entropy, and an installation
benchmarking phase that correctly selects the best al-
gorithm for the given situation. However, no attempt
is made to generalize the technique into a general ap-
proach and no discussion of the more difﬁcult parallel
case is given. Many previous work aim at tuning spe-
ciﬁc algorithm parameters. Examples are Spiral[23] and
FFTW[6] for FFT signal processing and Atlas[22] for
matrix multiplication. These approaches, though quite
effective, are very narrow in scope and do not constitute
a general framework for generic algorithm selection.

Another somewhat relevant approach is that of dy-
namic feedback [5] which selects code variants based on
on-line proﬁling. There are many recent efforts to de-
velop dynamic and adaptive compilation systems, how-
ever we feel they are out of scope for this work.

Both data afﬁliated loop [14] method and Lo-
cal Write [7] follow “owner computes” rule. Disadvan-
tages of these techniques are their potential to heavily

replicate unnecessary computation and cause load im-
balance, unlike data replication based methods.

Zoppetti and Agrawal [27] proposed a parallel reduc-
tion algorithm for multi-threaded architectures that can
overlap computation and communication. They also im-
plemented an incremental inspector that overlaps com-
putation and communication and update the computa-
tion schedule efﬁciently. The potential drawback of this
technique is that it may introduce unnecessary commu-
nication, e.g., pipelining data sections to irrelevant com-
putation threads.

Adaptive Data Repository (ADR) infrastructure [11]
was developed to perform range queries with user-
deﬁned aggregation operations on multi-dimensional
datasets, which are generalized reductions. In the ADR
infrastructure,
three strategies are used: fully repli-
cated accumulation, sparsely replicated accumulation,
distributed accumulation, which are analogous to REP-
BUF, SELPRIV and LOCALWR discussed in this paper.
Their experiments have shown that none of the strate-
gies worked the best for various query patterns and a
predictive model was desired.

9. Conclusion

In this paper, we presented an Adaptive Algorithm
Selection Framework that can automatically adapt to
the input data, environment and machine and select the
best performing algorithm. We have applied our frame-
work to the adaptive selection of parallel reduction al-
gorithms. We have identiﬁed a few high-level, archi-
tecture independent parameters characterizing a pro-
gram’s static structure, dynamic data access patterns and
candidate transformations. We applied an off-line syn-
thetic experimental process to automatically generate
predictive models which are used to dynamically select
the most appropriate optimization transformation among
several functionally equivalent candidates.

Through experimental results, we have shown that

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

[13] X. Li, M. J. Garzaran, and D. A. Padua. A dynamically
In Proc. 2004 Int’l Symp. Code

tuned sorting library.
Generation and Optimization, pp. 111–122, 2004.

[14] Y. Lin and D. A. Padua. On the automatic paralleliza-
tion of sprase and irregular fortran programs.
In Proc.
Int’l Wkshp. Lang., Compilers, and Run-time Systems for
Scalable Computers, pp. 41–56, 1998.

[15] J. Mellor-Crummey, D. Whalley, and K. Kennedy. Im-
proving memory hierarchy performance for irregular ap-
plications using data and computation reorderings. Int’l
J. Parallel Prog., 29(3):217–247, 2001.

[16] A. Miller. Subset Selection in Regression (Second Edi-

tion). Chapman & Hall/CRC, Boca Raton, FL, 2002.

[17] OpenMP Architecture Review Board. OpenMP Fortran

Application Program Interface, Version 2.0, 2000.

[18] S. Plimpton. Fast parallel algorithms for short-range

molecular dynamics. J. Comp. Phys., 117:1–19, 1995.

[19] J. H. Saltz, R. Mirchandaney, and K. Crowley. Run-
time parallelization and scheduling of loops. IEEE Trans.
Computers, 40(5):603–612, May 1991.

[20] S. Turek and C. Becker. FEATFLOW: Finite Element
Software for The Incompressible Navier-Strokes Equa-
tions, User Manual, Release 1.1. University of Heidel-
berg, Institute for Applied Mathmatics, Germany, 1998.
[21] R. von Hanxleden. Handling irregular problems with
Fortran D – a preliminary report. In Proc. Wkshp. Com-
pilers for Parallel Computers, pp. 353–364, 1993.

[22] R. C. Whaley, A. Petitet, and J. Dongarra. Automated
empirical optimizations of software and the ATLAS
project. Parallel Computing, 27(1–2):3–25, 2001.

[23] J. Xiong, J. Johnson, R. Johnson, and D. A. Padua. SPL:
In Proc.
A language and compiler for dsp algorithms.
ACM Conf. Prog. Lang. Design and Implementation, pp.
298–308, 2001.

[24] H. Yu, F. Dang, and L. Rauchwerger. Parallel reduc-
tion: An application of adaptive algorithm selection. In
Proc. Wkshp. Lang. Compilers for Parallel Computing,
pp. 171–185, 2002.

[25] H. Yu and L. Rauchwerger. Adaptive reduction paral-
In Proc. ACM Int’l Conf. Super-

lelization techniques.
computing, pp. 66–77, 2000.

[26] H. P. Zima. Supercompilers for Parallel and Vector Com-

puters. ACM Press, New York, NY, 1991.

[27] G. M. Zoppetti, G. Agrawal, and R. Kumar. Compiler
and runtime support for irregular reductions on a mul-
tithreaded architecture. In CDROM Proc. Int’l Parallel
and Distributed Processing Symp., 2002.

our technique can select the most appropriate parallel re-
duction algorithms at run–time with very low overhead.
When this technique is applied to dynamic programs se-
lecting new algorithms for each of their dynamic phases
performance is improved even further.

The importance of this work is that the presented
adaptive optimization technique can model programs
with irregular and dynamic behavior and customize so-
lutions to each program instance. It is a general frame-
work that can adapt any number of optimizations to the
program’s needs.

References

[1] P. An and et al. STAPL: An adaptive, generic parallel
c++ library. In Proc. Wkshp. Lang. Compilers for Paral-
lel Computing, pp. 193–208, 2001.

[2] W. Blume and et al. Advanced program restructuring for
IEEE Com-

high-performance computers with Polaris.
puter, 29(12):78–82, 1996.

[3] E. A. Brewer. High-level optimization via automated sta-
In Proc. ACM Symp. Principles and

tistical modeling.
Practice of Parallel Prog., pp. 80–91, 1995.

[4] C. Ding and K. Kennedy. Improving cache performance
of dynamic applications with computation and data lay-
out transformations.
In Proc. ACM Conf. Prog. Lang.
Design and Implementation, pp. 229–241, 1999.

[5] P. C. Diniz and M. C. Rinard. Dynamic feedback: An ef-
fective technique for adaptive computing. In Proc. ACM
Conf. Prog. Lang. Design and Implementation, pp. 71–
84, 1997.

[6] M. Frigo. A fast fourier transform compiler.

In Proc.
ACM Conf. Prog. Lang. Design and Implementation, pp.
169–180, 1999.

[7] H. Han and C.-W. Tseng. Improving compiler and run-
time support for adaptive irregular codes. In Proc. IEEE
Int’l Conf. Parallel Architectures and Compilation Tech-
niques, pp. 393–400, 1998.

[8] Y.-S. Hwang and et al. Runtime and language support
for compiling adaptive irregular programs on distributed-
memory machines. Software - Practice and Experience,
25(6):597–621, 1995.

[9] R. Iyer, N. M. Amato, L. Rauchwerger, and L. Bhuyan.
Comparing the memory system performance of the HP
V-Class and SGI Origin 2000 multiprocessors using mi-
crobenchmarks and scientiﬁc applications. In Proc. ACM
Int’l Conf. Supercomputing, pp. 339–347, 1999.

[10] R. Jain. The Art of Computer Systems Performance Anal-

ysis. John Wiley & Sons, Inc., Hoboken, NJ, 1991.

[11] T. Kurc and et al. Querying very large multi-dimensional

datasets in adr. In Proc. Supercomputing ’99, 1999.

[12] F. T. Leighton. Introduction to Parallel Algorithms and
Architectures: Arrays, Trees, Hypercubes. Morgan Kauf-
mann, San Francisco, CA, 1992.

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

An Adaptive Algorithm Selection Framework∗

Hao Yu

IBM T. J. Watson Research Ctr
Yorktown Heights, NY 10598

yuh@us.ibm.com

Dongmin Zhang

Parasol Lab, Dept of CS
Texas A&M University
dzhang@cs.tamu.edu

Lawrence Rauchwerger
Parasol Lab, Dept of CS
Texas A&M University
rwerger@cs.tamu.edu

Abstract

Irregular and dynamic memory reference patterns
can cause performance variations for low level algo-
rithms in general and for parallel algorithms in partic-
ular. We present an adaptive algorithm selection frame-
work which can collect and interpret the inputs of a par-
ticular instance of a parallel algorithm and select the
best performing one from a an existing library. In this
paper present the dynamic selection of parallel reduc-
tion algorithms. First we introduce a set of high-level pa-
rameters that can characterize different parallel reduc-
tion algorithms. Then we describe an off-line, system-
atic process to generate predictive models which can be
used for run-time algorithm selection. Our experiments
show that our framework: (a) selects the most appro-
priate algorithms in 85% of the cases studied, (b) over-
all delievers 98% of the optimal performance, (c) adap-
tively selects the best algorithms for dynamic phases of
a running program (resulting in performance improve-
ments otherwise not possible), and (d) adapts to the un-
derlying machine architecture (tested on IBM Regatta
and HP V-Class systems).

1. Introduction

Improving performance on current parallel proces-
sors is a very complex task which, if done “by hand” by
programmers, becomes increasingly difﬁcult and error
prone. Programmers have obtained increasingly more
help from parallelizing (restructuring) compilers. Such
compilers address the need of detecting and exploit-
ing parallelism in sequential programs written in con-
ventional languages as well as parallel languages (e.g.,
HPF). They also optimize data layout and perform other

∗ Research was performed at Texas A&M and supported in part by
NSF CAREER Award CCR-9734471, NSF Grant ACI-9872126,
NSF Grant EIA-0103742, NSF Grant ACI-0326350, NSF Grant
ACI-0113971, DOE ASCI ASAP Level 2 Grant B347886

transformations to reduce and hide memory latency, the
other crucial optimization in modern large scale parallel
systems. The success in the “conventional” use of com-
pilers to automatically optimize code is limited to the
cases when performance is independent of the input data
of the applications. When codes are irregular (memory
references are irregular) and/or dynamic (change dur-
ing the execution of the same program instance) it is
very likely that important performance affecting pro-
gram characteristics are input and environment depen-
dent. Many important (frequently used and time con-
suming) algorithms used in such programs are indeed in-
put dependent. We have previously shown [25] that, for
example, parallel reduction algorithms are quite sensi-
tive to their input memory reference pattern and system
architecture. In [1] we have shown that parallel sorting
algorithms are sensitive to architecture, data type, size,
etc. One of the most powerful optimizations compilers
can employ is to substitute entire algorithms instead of
trying to perform low level optimizations on sequences
of code. In [1] and [25] we have shown that performance
improves signiﬁcantly if we dynamically select the best
algorithm for each program instance.

In this paper, we present a general framework to auto-
matically and adaptively select, at run-time, the best per-
forming, functionally equivalent algorithm for each of
their instantiations. The adaptive framework can select,
at run-time, the best parallel algorithm from an exist-
ing library. The selection process is based on an off-line
automatically generated prediction model and algorithm
input data collected and analyzed dynamically. For this
paper we have concentrated our attention on the auto-
matic selection of reduction algorithms. For brevity we
will not show in this paper the dynamic selection of par-
allel sorting algorithms.

Reductions (aka updates) are important because they
are at the core of a very large number of algorithms and
applications – both scientiﬁc and otherwise – and there
is a large body of literature dealing with their paralleliza-
tion. More formally, a reduction variable is a variable

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

whose value is used in one associative and possibly com-
mutative operation of the form x = x ⊗ exp, where
⊗ is the operator and x does not occur in exp or any-
where else in the loop. With the exception of some sim-
ple methods using unordered critical sections (locks),
reduction parallelization is performed through a simple
form of algorithm substitution. For example, a sequen-
tial summation is a reduction which can be replaced by a
parallel preﬁx, or recursive doubling, computation [12].
In the case of irregular, sparse programs, we deﬁne irreg-
ular reductions as reductions performed on data refer-
enced in an irregular pattern, usually through an index
array. For irregular reductions, the access pattern tra-
versed by the sequential reduction in a loop is often sen-
sitive to the input data or computation. Therefore, as we
have shown in [25], not all parallel reduction algorithms
or implementations are equally suited as substitutes for
the original sequential code. Each access pattern has its
own characteristics and will best be parallelized with an
appropriately tailored algorithm.

In [25] we have presented a small library of paral-
lel reduction algorithms and shown that the best perfor-
mance can be obtained only if we dynamically select the
most appropriate one for the instantiated input set (ref-
erence pattern). Also in [25] we have presented a taxon-
omy of reduction reference patterns and sketched a de-
cision tree (derived manually) based scheme.

We continue this work and introduce a systematic
and automatic process to generate predictive models that
match the parallel reduction algorithms to execution in-
stances of reduction loops. After manually establishing
a small set of parameters that can characterize irregu-
lar memory references and setting up a library of paral-
lel reduction algorithms [25], we measure their relative
performance for a number of memory reference param-
eters in a factorial experiment. This is achieved by run-
ning a synthetic loop which can generate reduction ref-
erences with the memory reference patterns selected by
our factorial experiment. The end result of this off-line
process is a mapping between various points in the mem-
ory reference pattern space and the best available reduc-
tion algorithm. At run-time, the memory reference char-
acteristics of the actual reduction loop are extracted and
matched through a regression to the corresponding best
algorithm (using our previously extracted map).

This paper’s main contribution is a framework for a
systematic process through which input sensitive predic-
tive models can be built off-line and used dynamically to
select from a particular list of functionally equivalent al-
gorithms, parallel reductions being just on important ex-
ample. The same approach could also be used for vari-
ous other compiler transformations that cannot be easily
analytically modeled. Our experiments on an IBM Re-

gatta and HP V-Class show that the framework: (a) se-
lects the best performing algorithms in 85% of the cases
studied, (b) overall delievers 98% of the optimal perfor-
mance, (c) achieves better performance by adaptively se-
lecting the best algorithm for the dynamic phase of a
running program, (d) adapts to machine architecture.

2. Framework Overview

In this section we give an overview of our general
framework for adaptive and automatic low level algo-
rithm selection, the details of which are presented in the
remainder of this paper as applied to the optimization of
parallel reduction algorithm selection.

Fig. 1 gives an overview of our adaptive framework.
We distinguish two phases: (a) a setup phase and (b) a
dynamic selection (optimization) phase.

The setup phase occurs once for each computer sys-
tem and thus, implicitly tailors our process to a particu-
lar architecture. We then establish the input domain and
the output domain (possible optimizations) of the algo-
rithm selection code.

In the particular case presented in this paper, i.e., par-
allel reductions, the input domain is the universe of all
possible and realistic memory reference patterns — be-
cause, because they crucially impact the obtained per-
formance [25]. Architecture type is also important, but
is used implicitly. Since it would be impractical to study
the entire universe of memory access patterns, we de-
ﬁne a small set of parameters that can sufﬁciently char-
acterize them. The domain of possible optimizations is
composed, in our case, of the different parallel reduc-
tion algorithms collected in a library. Here, we need
to note that the characteristic parameters for selecting
algorithms for different optimizations can be different.
To avoid extensive setup process, high level parameters
(usually need to be identiﬁed manually) are preferred.
For instance, the characteristic parameters for selecting
parallel sorting algorithms [1] are different from what
we choose for reduction parallelization.

We then explore our input domain and ﬁnd a map-
ping to the output domain. In our case we establish a
mapping between different points in the input parame-
ter space (memory reference patterns) and relative per-
formance rankings of the available algorithms. This task
is accomplished off-line by running a factorial exper-
iment. We generate a number of parameter sets that
have the potential cover our input domain. For each of
these data points, we measure the relative performance
of our algorithms on the particular architecture, and rank
them accordingly. We should mention here that we have
also tried other methods of exploring the data space.

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

Setup Phase

Optimization Phase

Application

Synthetic
Experiments

Model
Derivation

Optimizing Compiler

Adaptive Executable

Yes

Characteristics
Changed ?

Algo. Selection Code

Select Algo.

Selected Algo.

Figure 1. Adaptive Algorithm Selection

In [24, 1], we have used a machine learning algorithmto
explore the input space that deﬁnes performance.

The dynamic selection phase occurs during actual
program execution. Through instrumentation (or other-
wise) we extract the set of relevant parameters that char-
acterizes the actual input. Then we used the informa-
tion obtained during the setup phase to ﬁnd its corre-
sponding output. Speciﬁcally, we use statistical regres-
sion to eventually select the appropriate best perform-
ing parallel reduction algorithm. In [1] and [24], in turn,
we used statically generated decision trees which are dy-
namically traversed to select the best algorithms.

In this paper we specialize our adaptive framework
to parallel reduction algorithm selection. In the follow-
ing, we ﬁrst present our library of reduction algorithms
and a set of parameters that can characterize their ir-
regular memory reference patterns. Then we describe
the factorial experiment that explores our input space.
For each parameter set, we execute a synthetic parame-
terized loop that generates a memory reference pattern
on which we evaluate and rank the different algorithms
in our library. A regression method is used to compute
prediction models for each parallel reduction algorithm
based on the data from the factorial experiment. At run-
time, we compute values for the characterization param-
eters of the reduction operation in question and use them
in our pre-computed models to select the best algorith-
mic option. We then evaluate our framework experimen-
tally for static and dynamic reference patterns.

3. Reduction Algorithm Library

Reductions are associative recurrences and they can
be parallelized in several ways. Our library currently
contains two types of methods: direct update methods,
which update shared reduction variables during loop ex-
ecution, and private accumulation and global update
methods, which accumulate in private storage during
loop execution and update shared variables with each
processor’s contribution afterwards.

Direct update methods include the classical recursive
doubling [12], unordered critical sections [26] and local
write [7]; our library only includes local write because
the others are not as competitive. Local Write (LO-
CALWR) is originally described in [7], LOCALWR ﬁrst
collects the reference pattern in an inspector loop [19]
and partitions the iteration space based on the “owner–
computes” rule. Memory locations referenced on multi-
ple processors have their iterations replicated on those
processors as well. To execute the reduction(s), each
processor goes through its local copies of the iterations
containing the reduction(s) and operating on the proces-
sor’s local data.

We have implemented three private accumula-
tion methods in our library. Replicated Buffer (REP-
BUF) [12, 14] simply executes the reduction loop as a
doall, accumulating partial results in private reduc-
tion variables, and later accumulates results across pro-
cessors. Replicated Buffer With Links (REPLINK)
[25] avoids traversing the unused (but allocated) el-
ements during REPBUF’s cross-processor
reduction
phase by keeping a linked list to record the proces-
sors that access each shared element. Selective Privati-
zation (SELPRIV) [25] only privatizes array elements
that have cross-processor contention. By excluding un-
used private elements, SELPRIV maintains a dense pri-
vate space where most elements are used. Since the
private space does not align to the shared data ar-
ray, the reduction’s index array is modiﬁed to redirect
accesses to the selectively privatized elements.

Issues

REPBUF

REPLINK

SELPRIV

LOCALWR

Good when
contention is

Locality

Need schedule

reuse [19]
Extra Work
Extra Space

High
Poor

No
No
NxP

Low
Poor

Yes
No
NxP

Low
Good

Yes
No

NxP+M

Low
Good

Yes
Yes
MxP

M: # iterations; N: data array size; P: # processors.
Table 1. Comparison of reduction algo.

Our parallel reduction algorithm library contains sev-
eral methods suitable for a range of reference patterns.
For brevity, we provide here only a high level descrip-
tion of the methods. See [25] for additional details.

In Table 1, we present a qualitative comparison of
the algorithms described above. Here, the contention of
a reduction is the average number of iterations (# pro-

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

NBF: N=25600,CON=200,P=8

25600

19200

12800

6400

e
g
a
s
U
 
r
e
f
f
u
B

1

2

MOLDYN: N=16384,CON=95.8,P=8

16384

12000

8000

4000

e
g
a
s
U
 
r
e
f
f
u
B

IRREG: N=100000,CON=100,P=8

100000

e
g
a
s
U
 
r
e
f
f
u
B

80000

60000

40000

20000

4

3
6
Processors

5

7

8

1

2

4

3
6
Processors

5

7

8

1

2

4

3
6
Processors

5

7

8

Figure 2. Memory access patterns of REPBUF

cessors when running in parallel) referencing the same
element. When contention is low, many unused repli-
cated elements in REPBUF are accumulated across pro-
cessors, while other algorithms only pass useful data.
SELPRIV works on a compacted data space and there-
fore potentially has good spatial locality. In LOCALWR,
each processor works on a speciﬁc portion of the data ar-
ray and has thus potentially good locality. With respect
to overhead, REPLINK, SELPRIV and LOCALWR all
have auxiliary data structures that depend on the access
pattern, and which must be updated when it changes.
Their overhead can be reduced by employing the sched-
ule reuse [19] technique.

4. High-Level Parameters

In this section, we describe the parameters we
have chosen to characterize reduction operations. Ide-
ally, they should require little overhead to measure and
they should enable us to select the best parallel re-
duction algorithm from our library for each reduction
instance in the program. After deﬁning the identiﬁed pa-
rameters below, we present a summary of the decoupled
effects of the parameters on the performance of the par-
allel reduction algorithms to illustrate the effectiveness
of the chosen parameters. Here, we enumerate the pa-
rameters in no speciﬁc order.

N is the number data elements involved in the reduc-
tion (often the size of the reduction array). It strongly in-
ﬂuences the loop’s working set size, and thus potentially
performance. In some applications, several reduction ar-
rays have exactly same access pattern; here N includes
the data elements for all arrays.

CON, the Connectivity of a loop, is the ratio between
the number of iterations of the loop and N. This param-
eter is equivalent to the parameter deﬁned by Han and
Tseng in [7]; there, the underlying data structures (cor-
responding to the irregular reductions) represent graphs
G = (V, E) and the authors deﬁned Connectivity
as |V |/|E|. Generally, the higher the connectivity, the
higher the ratio of computation to communication, i.e.,

if the connectivity is high, a small number of elements
will be referenced by many iterations.

MOB, the Mobility per iteration of a loop, is the num-
ber of distinct subscripts of reductions in an iteration.
For the LOCALWR algorithm, the effect of high itera-
tion Mobility (actually lack of mobility) is a high degree
of iteration replication. MOB is a parameter that can be
measured at compile time.

OTH, represents the Other (non-reduction) work in
an iteration. If OTH is high, a penalty will be paid
for replicating iterations. To measure OTH, we instru-
mented a parallel loop transformed for the REPBUF al-
gorithm using light-weight timers (∼ 100 clock cycles).
SP, the Sparsity, is the ratio of the total number of
referenced private elements and total allocated space on
all processors using the REPBUF algorithm (pN). Intu-
itively, SP indicates if REPBUF is efﬁcient.

CLUS, the Number of Clusters, reﬂects spatial local-
ity and measures if the used private elements in the REP-
BUF are scattered or clustered on each processor. Fig. 2
shows three memory access patterns which can be clas-
siﬁed as clustered, partially-clustered, and scattered, re-
spectively. Currently, SP and CLUS are measured by in-
strumenting parallel reduction loops using the REPBUF
algorithm and the overhead is proportional to the num-
ber of used private elements. CLUS measures the aver-
age number of clusters of the used private elements on
each processor.

We have investigated the decoupled effects of the pa-
rameters on the performance of the parallel reduction
algorithms. Although the decoupling is not realistic, it
is useful for discovering qualitative trends. The trends
summarized in Table 2 are based on a comprehensive
set of experiments on multiple architectures that are re-
ported in [24]. The trends for REPLINK are similar to
REPBUF and are not listed here.

The effect of N is straight–forward, comparing to
the sequential reduction loop, SELPRIV and LOCALWR
have much smaller data sets on each processor and there-
fore their speedups increase with N. CON is inversely
correlated with inter-processor communication. Hence,
larger CON values will indicate better scaling REPBUF

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

parameters

N

CON
MOB
OTH
SP

CLUS

REPBUF

(cid:2)
↑
↑
↑
(cid:2)
(cid:2)

SELPRIV

↑
(cid:2)
(cid:4)
↑
↓
↑

LOCALWR

↑
–
↓
(cid:2)
(cid:4)
–

↑: positive effect; ↓: negative effect; (cid:6): little positive
effect; (cid:7): little negative effect; –: no effect.
Table 2. Decoupled effects

and SELPRIV, which have two reduction loops, one ac-
cumulating in private space and one accumulating cross-
processor shared data. Large MOB values (many refer-
ences to index arrays) may imply poor performance
for SELPRIV, because accesses to the reduction array
must access both the original and the modiﬁed index
arrays, and for LOCALWR, because large MOB val-
ues often result in high iteration replication. Large val-
ues of OTH often indicate good performance for REP-
BUF and SELPRIV because the ﬁrst private accumula-
tion loop has a larger iteration body; since LOCALWR
replicates “other work” it will not beneﬁt as much. Low
SP is good for SELPRIV and also for LOCALWR, since
it often correlates with low contention and hence low
iteration replication. For large CLUS, since SELPRIV
compacts the sparsely used data space, it achieves better
speedups than LOCALWR and REPBUF, which work on
original (non-compacted) data or in private spaces con-
formable to the original data.

5. Adaptive Reduction Selection

In this section we will elaborate on the setup and on
the dynamic selection phases of our adaptive scheme.
The setup is executed only once, during machine instal-
lation, and generates a map between points in the uni-
verse of all inputs (memory reference patterns charac-
terized with the previously deﬁned parameters) and their
corresponding best suited reduction algorithms. The dy-
namic selection phase is executed every time a targeted
reduction is encountered. It uses the map built during
the setup phase, a parameter collection mechanism to
characterize the memory references and an interpolation
function (the actual algorithm selector) to ﬁnd the best
suited algorithm in the library.

5.1. Setup Phase

We now outline the design of the initial map between
a set of synthetically generated parameter values and the
corresponding performance ranking of the various paral-

Experimental

Parameter

Values

Parameterized

Synthetic

Reduction Loop

Experimental

Speedups

A Compact Map

MODELS:
Speedups[1..S] = F( Parameters )

Run−Time

Run−Time Algorithm Selection

Figure 3. Setup phase

lelization algorithms available in our library. The overall
setup phase, is illustrated in Fig. 3.

The domain of values that can be taken by the input
parameters is explored by setting up a factorial experi-
ment [10]. Speciﬁcally, we choose several values (typi-
cal of realistic reduction loops) for each parameter and
generate a set of experiments from all combinations of
the chosen values of the different parameters. The cho-
sen parameter values for our reduction experiment are
shown in Table 3.

To measure the performance of different reduction
patterns, we have created a synthetic reduction loop.
The structure of the loop is shown in Fig 4, with C-like
pseudo-code. The non–reduction work and reductions
have been grouped in two loop nests. The dynamic
pattern depends strictly on the index array, which is
generated automatically to correspond to the parameters
SP and CLUS. In addition, OTH is dynamically mea-
sured. The contents of the index[*] array are gen-
erated via a randomized process which satisﬁes the re-
quirement speciﬁed by the parameters.

The performance ranking of the parallel reduction al-
gorithms in our library is accomplished by simply ex-
ecuting them all for each parameter combination, i.e.,
synthetically generated access pattern, and measuring
their actual speedups, as shown in Fig. 3. The end re-
sult is a compact map from parameter values to perfor-
mance (speedups) of candidate parallel algorithms.

From this map we can now create the prediction code,
the model that can then be used by an application at run-

FOR j = 1 to N*CON

FOR i = 1 to OTH /* non-reductions */

memory read & scalar computation;

FOR i = 1 to MOB

/* reductions */

data[ index[j][i] ] += expr;

Figure 4. Synthetic reduction loop

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

time. We applied two methods, general linear regression
and decision tree learning. However, due to space con-
straints, in this paper we will describe only the modeling
process using general linear regression.

Values
256K
16

Parameters
N (array size)
Connectivity
Mobility
Other Work
Sparsity
# clusters
Table 3. Param. for factorial experiment

16K
0.2
2
1
0.02
1

64K
2
8
4
0.2
4

0.45
20

1M
128

0.75

0.99

4M

The generated models are polynomial functions from
the parameter values to the speedups of the parallel al-
gorithms. We follow a standard term selection process
that automatically selects polynomial terms from a spec-
iﬁed pool [16]. We have speciﬁed a maximal model as
F = (lg N + lg C + M OB + OT H + lg S + lg L + 1)3.
and a minimal model as F = lg N + lg C + M OB +
OT H + lg S + lg L + 1. Here, C is CON, S is SP, and L
is CLUS. Then from the minimal model, terms are ran-
domly selected from the 84 terms of the maximal model.
The samples are ﬁrst separated into training data and
testing data. When adding a term into the model, the
training data are used to ﬁt (least square ﬁt) the coefﬁ-
cients and the ﬁtted model is evaluated using the testing
data. If the test error is higher than the test error of the
model before including the new term, the term will be
dropped. Though this sequential term selection process
will not give us the optimal model, it is fast and its auto-
matically generated models have produced good results
when used to predict relative performance of the paral-
lel algorithms on real reduction loops. Here, the order of
the model, 3, is chosen mainly due to practical reasons
such as generating less experimental samples with less
synthetic experimentation time and avoiding term explo-
sion. For parameter MOB, we have only chosen 2 values
for the experiment, we have excluded the terms contain-
ing a non-linear MOB factor from the maximal model.
For OTH and CLUS, though we speciﬁed few values,
the values used in the map are measured and computed
from the generated index array.

The ﬁnal polynomial models contain about 30 terms.
The corresponding C library routines are generated au-
tomatically to evaluate the polynomial F () for each al-
gorithm at run-time.

5.2. Dynamic Selection Phase

During the dynamic selection phase, to avoid exe-
cuting the parameter collection and algorithm selection

Begin

Pattern Changed?

Yes

SCH_reuse

SCH_adapt

Compute Parameters: P
SCH = select_algorithm(P)

End

Figure 5. Reduction selection at run-time

steps for every time a reduction loop is invoked, we use
a form of memoization, decision reuse, which detects if
the inputs to our selector function have changed. When
a new instance of a reduction is encountered and the in-
put parameters have not changed from the previous exe-
cution instance, we directly reuse the previously selected
algorithm, thus saving run-time selection overhead. This
is accomplished with standard compiler technology, i.e.,
the compiler instruments two version loops for each par-
allel algorithm (illustrated in Fig. 5).

In this phase, the pre-generated model evaluation rou-
tines are called to estimate speedups of all the algo-
rithms, rank them, and select the best one. The evalu-
ation of the polynomial models is fast because each of
the ﬁnal models contains only about 30 terms.

5.3. Selection Reuse for Dynamic Programs

In the previous section we presented a systematic pro-
cess to generate predictive models that could recom-
mend the best irregular reduction parallelization algo-
rithm by collecting a set of static and dynamic param-
eters. We mentioned that if the memory characteristics
parameters do not change we can reuse our decision and
thus reduce run-time overhead. This may be of value if
the compiler can automatically prove statically that the
reference pattern does not change. If, however this is not
possible, which is often the case for irregular dynamic
codes, we have to perform a selection for each reduction
loop instance. In this section we will show how to re-
duce this overhead by evaluating a trade-off between the
run-time overhead of selection and the beneﬁt of ﬁnd-
ing a better algorithm.

To better describe the run–time behavior of a dy-
namic program, we introduce the notion of dynamic
phase. For a loop containing irregular reductions, a
phase is composed of all the contiguous execution in-
stances for which the pattern does not change. For in-
stance, assume the access pattern of the irregular reduc-

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

Program
Irreg – FE based CFD kernel
Nbf – MD kernel
Moldyn – synthetic MD program
Charmm – MD kernel
Spark98 – FE simulation
Spice – circuit simulation
Fma3d – FE method for solids

Lang.
F77
F77
C
F77
C
F77
F90

Lines
223
116
688
277
1513
18912
60122

Source
[7]
[7, 21]
[7, 4, 15]
[8]
SPEC’2000
SPEC’92
SPEC’2000

Loop
apply elements’ forces (do 100)
non-bounded force calc. (do 50)
non-bounded force calc.
non-bounded force calc.
smvp loop – Symmetric MV Product
bjt loop – traverse BJT devices & update mesh
Scatter Element Nodal Forces Platq loop

Coverage
∼ 90%
∼ 90%
∼ 70%
∼ 80%
∼ 70%
11–45%
∼ 30%

# inp

4
4
4
3
2
4
1

Table 4. Scientiﬁc Applications and Reduction Loops

, the loop instances in [t, t

tions changes at instance t and the next change of the
(cid:1) − 1]
(cid:1)
pattern is at instance t
form a dynamic phase. We can therefore group the ex-
ecution instances of irregular reduction loops into dy-
namic phases. We deﬁne the re-usability of a phase as
the number of dynamic instances of the reduction loop
in that phase. It intuitively gives the number of times a
loop can be considered invariant and thus does not need
a new adaptive algorithm selection. That is, the overhead
of selecting a better algorithm at run-time can be amor-
tized over re-usability instantiations.

Tseq

We then include re-usability into our previously
developed models, predict
the phase-wise perfor-
mance and thus obtain a better overall performance.
This model will be employed at
run-time to de-
cide when it is worth changing the algorithm. The
phase–wise speedup of a parallel algorithm can be for-
mulated as (R−1)Tpar+(1+O)Tpar
. The best algorithm
is the algorithm that has the smallest value of R+O
Speedup .
In above formulas, R is the re-usability, O is the ra-
tio of the set-up phase overhead (in time) and the
parallel execution time of one instance of the par-
allel
loop applying the considered algorithm, and
Speedup is the speedup (relative to sequential ex-
ecution) of the considered algorithm excluding the
set-up phase overhead. Tpar and Tseq are the paral-
lel and sequential execution times of the loop. Using
the same off-line experimental process described pre-
viously, we have generated models to compute O,
the setup overhead ratio. Together with the pre-
dicted speedup (excluding the setup overhead), we
R+O
Speedup at run–time and se-
were able to evaluate
lect the best algorithm for a dynamic execution phase.

The re-usability R can be either computed by the
compiler if it can statically detect the phase changes,
i.e., the number of iterations of the surrounding loop
for which the address pattern remains constant (a well
known technique named ’schedule reuse’ [19]) or esti-
mate on-the-ﬂy with a simple statistical method (e.g.,
running average value of R). In our experiments we have
used the schedule reuse technique and the simple statisti-
cal method. As a general rule, simulations do not change
phase very fast so re-usability is high.

6. Evaluation of Algorithm Selection

In this section we evaluate our automatically gener-
ated performance models. We compare the actual per-
formance of the algorithm selected by our automatically
generated predictive models with the other algorithms.

6.1. Experimental Setup

We selected seven programs from a variety of sci-
entiﬁc domains (see Table 4). All Fortran codes were
automatically instrumented to collect information about
the access pattern using the Polaris compiler [2]; the
C programs were done manually. For most programs,
we chose or speciﬁed multiple inputs to explore input-
dependent behavior. While specifying the inputs, we
have tried, where possible, to use data sets that exercise
the entire memory hierarchy of our parallel machine.

We have studied two parallel systems: an UMA HP
V-Class with 16 processorsand a NUMA IBM Regatta
system with 32 processors.The machine conﬁgurations
are brieﬂy described in Table 5.

HP V2200
PA-8200
200 MHz

IBM Regatta
POWER 4
1300 MHz

2 MB
4 GB

32KB/1.48MB/32MB

CPU Type
CPU Clock
Data Cache
Memory
# CPUs
Topology
OS
Compiler
Table 5. Experimental parallel systems

32 / 4 MCMs
buses / ring

HP-UX 11.0
HP f90, c89

xlf r, xlc r

crossbar

64 GB

AIX 5

16

We used an 8-processor subsystem of the HP and a
16-processor subsystem of the IBM, which was sufﬁ-
cient for us to exercise the architectural characteristics
of these two systems. We ran 22 application/input com-
binations on the HP and due to resource limitations only
21 application/input cases on the IBM system (no results
were obtained for FMA3D).

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

1

0.8

0.5

s
p
u
d
e
e
p
S
 
d
e
z

i
l

a
m
r
o
N

1

0.8

0.5

s
p
u
d
e
e
p
S
d
e
z

 

i
l

a
m
r
o
N

Recommended algorithms for the missed cases

0
0
1
=
C

0
5
=
C

5
=
C

1
=
C

2
=
C

5
=
C

0
5
=
C

0
0
2
=
C

4
1
2
=
C

0
7
=
C

1
2
=
C

6
=
C

8
1
=
C

9
=
C

5

.

4
=
C

k
1
2
=
N

k
0
9
=
N

k
0
8
1
=
N

k
9
9
=
N

k
0
9
=
N

k
4
3
=
N

5

.

0
=
C

RepBuf
RepLink
SelPriv
LocalWr

Irreg

Nbf

Moldyn

Charmm

Spark98

Spice

Fma3d

Applications and Input Cases

Recommended algorithms for the missed cases

0
0
1
=
C

0
5
=
C

5
=
C

1
=
C

2
=
C

5
=
C

0
5
=
C

0
0
2
=
C

4
1
2
=
C

0
7
=
C

1
2
=
C

6
=
C

8
1
=
C

9
=
C

5

.

4
=
C

k
1
2
=
N

k
0
9
=
N

k
0
8
1
=
N

k
9
9
=
N

k
0
9
=
N

k
4
3
=
N

RepBuf
RepLink
SelPriv
LocalWr

Irreg

Nbf

Charmm
Applications and Input Cases

Moldyn

Spark98

Spice

Figure 6. Relative speedups of parallel reduction algorithms on HP and IBM systems

6.2. Results

Fig. 6 presents the results obtained on both systems.
Each group of bars shows the relative performance (nor-
malized to the best speedup obtained for that group)
of the four parallel reduction algorithms for one pro-
gram/input case. In most cases, the algorithm rankings
resulting from the automatically generated regression
model were consistent with the actual rankings. Over-
all, our regression model correctly identiﬁed the best al-
gorithm in 18/22 cases on the HP and 19/21 cases on
the IBM. As the arrows in the graphs show, for all the
mis-predicted cases there was little performance differ-
ence between the best and recommended algorithms.

To give a quantitative measure of

the perfor-
mance improvement obtained using the algorithms
recommended by our models, we compute the rela-
tive speedup which we deﬁne as
the ratio between
the speedup of the algorithm chosen by an “alterna-
tive selection method” and the algorithm recommended
by “our model”. We have compared the effective-
ness of our predictive models against four “alternative
selection methods”. The Best is a “perfect predic-
tive model”, or an “oracle”, that always selects the
best algorithm for a given loop–input case. RepBuf al-
ways applies REPBUF, which is the simplest algorithm
and it is speciﬁed as default in OpenMP [17]. Ran-
dom randomly selects a parallel algorithm. The speedup

obtained is the average speedup of all
the candi-
date parallel algorithms for that case. Default always
applies the “default” parallel algorithm for a given plat-
form. Based on our experimental results, we chose SEL-
PRIV and LOCALWR as the default algorithms for the
HP and IBM systems, respectively.

Fig. 7 gives the average relative speedups (across
all the loop-input cases). In the graph, the smaller the
speedup ratio value, the better the relative effectiveness
of our predictive models. The graph shows that our au-
tomatically generated predictive models work almost as
well as the “perfect predictive models”, obtaining more
than 98% of the best possible performance. Comparing
to other “non-perfect” selection methods, our predictive
models improve the performance signiﬁcantly.

s
p
u
d
e
e
p
S
−
e
v
i
t

l

 

a
e
R
e
g
a
r
e
v
A

1
0.9
0.8
0.7
0.6
0.5

RepBuf
Random
Default
Best

HP V−Class, P=8

IBM Regatta, P=16

Machines and # Processors

Figure 7. Average relative-speedups

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

Phase-Wise Execution Time (N=186624)

Step-Wise Execution Time (N=186624)

 2.5

 2

 1.5

 1

 0.5

e
m
i
T
 
n
o
i
t
u
c
e
x
E

DynaSel
LocalWr
RepBuf
SelPriv

 0

 0

 20
Adaptation Phases ( steps per phase)

 15

 10

 5

e
m
i
T
 
n
o
i
t
u
c
e
x
E

 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0

DynaSel
LocalWr
RepBuf
SelPriv

 0

 10

 20

 30

 40

 50

 60

Time Steps

Figure 8. Effects of dynamically selecting algorithms of MOLDYN (input #4)

As mentioned previously, the overhead of measuring
OTH is negligible by using a light-weight timer in few
iterations; the overhead of computing SP and CLUS is
proportional to the size of the data. Across all cases, the
average overheads on the HP and IBM systems are, re-
spectively, 11.95% and 11.65%, which can be largely
amortized since the overhead only incurs in loop in-
stances where the reduction pattern change.

Finally, we note that knowledge of the dynamically
collected parameters (N, CON, OTH, SP and CLUS) is
very important for our models. From Fig. 6, especially
for the bars corresponding to the results for Irreg and
Charmm on the HP system, the best schemes for dif-
ferent inputs change and our technique predicts the best
schemes correctly by collecting and utilizing the dynam-
ically collected parameters.

To test the robustness and generality of our predic-
tive models, we have applied them to two regular re-
duction loops: loop loops do400 in su2cor from
the SPEC’92 suite, and loop actfor do500 in bdna
from the PERFECT suite. For regular reduction loops,
the size of the reduction data (usually a vector) is relative
small and every element of the vector is accessed in ev-
ery loop iteration. Either selectively replicating the vec-
tor data or partitioning the vector will not help perfor-
mance (reducing cross–processor communications). The
experimental results indicate that REPBUF is always the
best algorithm and that our predictive models have given
the correct recommendations. Hence, our adaptive algo-
rithm selection technique is generally applicable to all
reductions.

7. Experiments on Dynamic Programs

In this section, we present experimental results show-
ing that our adaptive algorithm selection technique can
select the best parallelization scheme dynamically and
improve the overall performance of adaptive programs.

Here, DYNASEL represents applying algorithm selec-
tion dynamically for every computation-phase.

7.1. Molecular Dynamics (MOLDYN)

MOLDYN is a synthetic benchmark, conduct-
ing non-bonded force calculations in a molecular dy-
namics simulation. It has been widely used for the
purpose of evaluating systematic optimization trans-
formations [7, 4, 15]. For the program used in our ex-
perimentation, we replaced the step in MOLDYN that
building a reusable neighbor list with a link-cell al-
gorithm [18]. With the modiﬁcation,
the execution
time of MOLDYN is dominated by the force computa-
tion, which is the irregular reduction we would like to
optimize.

ID

#molecules

cut-off
radius

#steps

#phases

1
2
3
4
5

4.0
2.5
2.0
1.5
1.2

23328
186624
100800
186624
186624
Table 6. Inputs of MOLDYN

60
60
60
60
60

23
23
21
21
21

avg.
CON
157
37.6
21.8
6.6
5.4

For MOLDYN, since we have not observed much
performance change across dynamic phases for paral-
lel algorithms, we artiﬁcially set the re-usability (the
number of steps) for each dynamic phase so that the
phase-wise best algorithms can change due to the differ-
ent setup overheads of the algorithms. This way, we can
examine both the effectiveness of the prediction models
and the efﬁciency of selecting and switching algorithms.
We experiment with 5 different inputs on the 8-processor
subsystem of our HP V-class machine. The input speci-
ﬁcations are given in Table 6. Note that different phases
may have different numbers of time steps.

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

Due to space limitations, we show the step-wise and
phase-wise execution time Fig. 8 for one input. We note
that since the number of steps of the phases may change,
therefore in the phase-wise plots, we plotted the accu-
mulated time of each phase instead of the average time.
Again, the results show that DYNASEL out-performs ap-
plying any one algorithm for most cases.

Relative Reduction Speedups (Moldyn)

1.1
1.05
1
0.9

0.75

0.5

p
u
d
e
e
p
S
e
v
i
t

 

l

a
e
R

RepBuf
SelPriv
LocalWr
DynaSel

23328

55296

108000

186624

Input Data Size

186624

Figure 9. Results for MOLDYN

Fig. 9 gives the relative speedups of the reduction
loop across all steps for different parallel reduction al-
gorithms. The speedups are normalized to the best re-
sults obtained by applying one algorithm to clarify the
improvement. The conclusions here are that using our
adaptive technique to select and apply the best parallel
reduction algorithm for every dynamic phase has little
overhead and that it out-performs any other single algo-
rithm and improves overall performance up to 8%;

7.2. PP2D in FEATFLOW

We have applied our adaptive technique to the PP2D
code from the FEATFLOW package [20]. FEATFLOW
is a general purpose, F77 library of subroutines for solv-
ing incompressible Navier-Stokes equations in 2D and
3D. More speciﬁcally, PP2D solves nonlinear coupled
equations using multi-grid solvers. It has about 17,000
lines of code, excluding supporting library routines.

After proﬁling for our test input set, we have se-
lected a loop with irregular reductions from subroutine
GUPWD (about 11% of whole program execution time).
The loop updates a sparse matrix with old velocity val-
ues associated with grid nodes. The memory access pat-
tern of the irregular reduction is fully determined by the
indirection data structures deﬁning the sparse matrix.
The program uses a multi-grid method to solve linear
systems in each time step. The multi-level grids are pre-
deﬁned (speciﬁed in the input data ﬁle) and the sparse
matrix structures associated with the different grids are
deﬁned in an initialization step, before the time-step
loop. For our input data the program used 4 grids, with

grid levels between 2 and 5 (in general, the code could
use as many as 9 grid sizes). The studied reduction loop
uses the 4 sparse-matrix data structures (the four grids)
in an interleaved manner. The matrices themselves, i.e.,
the reduction access pattern, do not change, only their
interleaved invocation does. We then apply our adap-
tive algorithm selection technique for every invocation
of the reduction loop. By using the well known sched-
ule reuse [19] technique we can record our selection for
each of the four reduction patterns corresponding to each
of the four grids used by the code. The selection decision
is then reused for subsequent invocations of the loop. In
this manner we reduce the overhead associated with dy-
namic selection to only 4 instances (the 4 grids do not
change during program).

Detailed information about the used input, which is
distributed together with the source code for benchmark-
ing purposes, is shown in Table 7.

2
3
4
5

Level

#unknowns
12930
52620
206280
824720

#instances
86
86
86
166
Table 7. Grid parameters of PP2D input

#elements
920
3680
14720
58880

#nodes
1890
7460
29640
118160

Fig. 10 gives the speedups of the GUPWD loop ob-
tained, for each grid level, using three reduction tech-
niques, as well as the dynamic selection scheme. All
bars in the graph are normalized to the optimal scheme.
The group ’Total’ shows the normalized performance
of the loop, across all invocations, for the case when the
schemes would be selected only at the beginning of the
program as well as for the dynamic selection case when
a decision is made for every loop instance but the deci-
sion is reused.

On the HP system, although our DYNASEL has se-
lected the best algorithm (SELPRIV for all the grid lev-
els), it did not improve the overall performance of the
loop because the SELPRIV scheme performs best in all
cases. Noteworthy is the fact that the overhead of the dy-
namic scheme has not hurt performance.

On the IBM system, the results show that the perfor-
mance of DYNASEL align to the actual best algorithm
for each grid level. More importantly though, DYNASEL
obtains the overall best performance because the choice
of the optimal algorithm varies across grid levels, i.e.,
across the instances of the reduction loop. If we would
select only once the best scheme then performance could
degrade when the grid level changed. The use of the de-
cision reuse technique, keeps the effect of dynamic se-
lection overhead to a minimum.

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

Relative Reductions Speedup, HP V−Class, P=8

Relative Reductions Speedup, IBM Regatta, P=16

1.1
1

0.8

0.5

p
u
d
e
e
p
S
 
e
v
i
t
a
e
R

l

RepBuf
SelPriv
LocalWr
DynaSel

1.1
1

0.8

0.5

p
u
d
e
e
p
S
 
e
v
i
t
a
e
R

l

RepBuf
SelPriv
LocalWr
DynaSel

Total

Lev=5

Lev=4

Lev=3

Lev=2

Total

Lev=5

Lev=4

Lev=3

Lev=2

Figure 10. Results for PP2D

This experiment shows that we can apply our dy-
namic algorithm selection method even to dynamic pro-
grams in an always proﬁtable manner. It shows that even
if performance improvement in our particular case was
modest (upto 8%), our framework can take advantage of
instance speciﬁc opportunity for optimization.

8. Related Work

There does not appear to be a great body of effort in
the area of adaptive selection of high level algorithms.
Brewer[3] is probably the most extensive previous work
aimed at making a framework for this decision process.
In this approach, performance models are consulted to
determine the expected running time of possible imple-
mentation, with the minimum running algorithm then
chosen for use. These models are linear systems given,
along with some code annotations, by the end user.

Li, Garzaran, and Padua[13] present an approach for
choosing between several sequential sorting algorithms
based on data size, data entropy, and an installation
benchmarking phase that correctly selects the best al-
gorithm for the given situation. However, no attempt
is made to generalize the technique into a general ap-
proach and no discussion of the more difﬁcult parallel
case is given. Many previous work aim at tuning spe-
ciﬁc algorithm parameters. Examples are Spiral[23] and
FFTW[6] for FFT signal processing and Atlas[22] for
matrix multiplication. These approaches, though quite
effective, are very narrow in scope and do not constitute
a general framework for generic algorithm selection.

Another somewhat relevant approach is that of dy-
namic feedback [5] which selects code variants based on
on-line proﬁling. There are many recent efforts to de-
velop dynamic and adaptive compilation systems, how-
ever we feel they are out of scope for this work.

Both data afﬁliated loop [14] method and Lo-
cal Write [7] follow “owner computes” rule. Disadvan-
tages of these techniques are their potential to heavily

replicate unnecessary computation and cause load im-
balance, unlike data replication based methods.

Zoppetti and Agrawal [27] proposed a parallel reduc-
tion algorithm for multi-threaded architectures that can
overlap computation and communication. They also im-
plemented an incremental inspector that overlaps com-
putation and communication and update the computa-
tion schedule efﬁciently. The potential drawback of this
technique is that it may introduce unnecessary commu-
nication, e.g., pipelining data sections to irrelevant com-
putation threads.

Adaptive Data Repository (ADR) infrastructure [11]
was developed to perform range queries with user-
deﬁned aggregation operations on multi-dimensional
datasets, which are generalized reductions. In the ADR
infrastructure,
three strategies are used: fully repli-
cated accumulation, sparsely replicated accumulation,
distributed accumulation, which are analogous to REP-
BUF, SELPRIV and LOCALWR discussed in this paper.
Their experiments have shown that none of the strate-
gies worked the best for various query patterns and a
predictive model was desired.

9. Conclusion

In this paper, we presented an Adaptive Algorithm
Selection Framework that can automatically adapt to
the input data, environment and machine and select the
best performing algorithm. We have applied our frame-
work to the adaptive selection of parallel reduction al-
gorithms. We have identiﬁed a few high-level, archi-
tecture independent parameters characterizing a pro-
gram’s static structure, dynamic data access patterns and
candidate transformations. We applied an off-line syn-
thetic experimental process to automatically generate
predictive models which are used to dynamically select
the most appropriate optimization transformation among
several functionally equivalent candidates.

Through experimental results, we have shown that

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

[13] X. Li, M. J. Garzaran, and D. A. Padua. A dynamically
In Proc. 2004 Int’l Symp. Code

tuned sorting library.
Generation and Optimization, pp. 111–122, 2004.

[14] Y. Lin and D. A. Padua. On the automatic paralleliza-
tion of sprase and irregular fortran programs.
In Proc.
Int’l Wkshp. Lang., Compilers, and Run-time Systems for
Scalable Computers, pp. 41–56, 1998.

[15] J. Mellor-Crummey, D. Whalley, and K. Kennedy. Im-
proving memory hierarchy performance for irregular ap-
plications using data and computation reorderings. Int’l
J. Parallel Prog., 29(3):217–247, 2001.

[16] A. Miller. Subset Selection in Regression (Second Edi-

tion). Chapman & Hall/CRC, Boca Raton, FL, 2002.

[17] OpenMP Architecture Review Board. OpenMP Fortran

Application Program Interface, Version 2.0, 2000.

[18] S. Plimpton. Fast parallel algorithms for short-range

molecular dynamics. J. Comp. Phys., 117:1–19, 1995.

[19] J. H. Saltz, R. Mirchandaney, and K. Crowley. Run-
time parallelization and scheduling of loops. IEEE Trans.
Computers, 40(5):603–612, May 1991.

[20] S. Turek and C. Becker. FEATFLOW: Finite Element
Software for The Incompressible Navier-Strokes Equa-
tions, User Manual, Release 1.1. University of Heidel-
berg, Institute for Applied Mathmatics, Germany, 1998.
[21] R. von Hanxleden. Handling irregular problems with
Fortran D – a preliminary report. In Proc. Wkshp. Com-
pilers for Parallel Computers, pp. 353–364, 1993.

[22] R. C. Whaley, A. Petitet, and J. Dongarra. Automated
empirical optimizations of software and the ATLAS
project. Parallel Computing, 27(1–2):3–25, 2001.

[23] J. Xiong, J. Johnson, R. Johnson, and D. A. Padua. SPL:
In Proc.
A language and compiler for dsp algorithms.
ACM Conf. Prog. Lang. Design and Implementation, pp.
298–308, 2001.

[24] H. Yu, F. Dang, and L. Rauchwerger. Parallel reduc-
tion: An application of adaptive algorithm selection. In
Proc. Wkshp. Lang. Compilers for Parallel Computing,
pp. 171–185, 2002.

[25] H. Yu and L. Rauchwerger. Adaptive reduction paral-
In Proc. ACM Int’l Conf. Super-

lelization techniques.
computing, pp. 66–77, 2000.

[26] H. P. Zima. Supercompilers for Parallel and Vector Com-

puters. ACM Press, New York, NY, 1991.

[27] G. M. Zoppetti, G. Agrawal, and R. Kumar. Compiler
and runtime support for irregular reductions on a mul-
tithreaded architecture. In CDROM Proc. Int’l Parallel
and Distributed Processing Symp., 2002.

our technique can select the most appropriate parallel re-
duction algorithms at run–time with very low overhead.
When this technique is applied to dynamic programs se-
lecting new algorithms for each of their dynamic phases
performance is improved even further.

The importance of this work is that the presented
adaptive optimization technique can model programs
with irregular and dynamic behavior and customize so-
lutions to each program instance. It is a general frame-
work that can adapt any number of optimizations to the
program’s needs.

References

[1] P. An and et al. STAPL: An adaptive, generic parallel
c++ library. In Proc. Wkshp. Lang. Compilers for Paral-
lel Computing, pp. 193–208, 2001.

[2] W. Blume and et al. Advanced program restructuring for
IEEE Com-

high-performance computers with Polaris.
puter, 29(12):78–82, 1996.

[3] E. A. Brewer. High-level optimization via automated sta-
In Proc. ACM Symp. Principles and

tistical modeling.
Practice of Parallel Prog., pp. 80–91, 1995.

[4] C. Ding and K. Kennedy. Improving cache performance
of dynamic applications with computation and data lay-
out transformations.
In Proc. ACM Conf. Prog. Lang.
Design and Implementation, pp. 229–241, 1999.

[5] P. C. Diniz and M. C. Rinard. Dynamic feedback: An ef-
fective technique for adaptive computing. In Proc. ACM
Conf. Prog. Lang. Design and Implementation, pp. 71–
84, 1997.

[6] M. Frigo. A fast fourier transform compiler.

In Proc.
ACM Conf. Prog. Lang. Design and Implementation, pp.
169–180, 1999.

[7] H. Han and C.-W. Tseng. Improving compiler and run-
time support for adaptive irregular codes. In Proc. IEEE
Int’l Conf. Parallel Architectures and Compilation Tech-
niques, pp. 393–400, 1998.

[8] Y.-S. Hwang and et al. Runtime and language support
for compiling adaptive irregular programs on distributed-
memory machines. Software - Practice and Experience,
25(6):597–621, 1995.

[9] R. Iyer, N. M. Amato, L. Rauchwerger, and L. Bhuyan.
Comparing the memory system performance of the HP
V-Class and SGI Origin 2000 multiprocessors using mi-
crobenchmarks and scientiﬁc applications. In Proc. ACM
Int’l Conf. Supercomputing, pp. 339–347, 1999.

[10] R. Jain. The Art of Computer Systems Performance Anal-

ysis. John Wiley & Sons, Inc., Hoboken, NJ, 1991.

[11] T. Kurc and et al. Querying very large multi-dimensional

datasets in adr. In Proc. Supercomputing ’99, 1999.

[12] F. T. Leighton. Introduction to Parallel Algorithms and
Architectures: Arrays, Trees, Hypercubes. Morgan Kauf-
mann, San Francisco, CA, 1992.

Proceedings of the 13th International Conference on Parallel Architecture and Compilation Techniques (PACT’04) 
1089-795X/04 $ 20.00 IEEE 

