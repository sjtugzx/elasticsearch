Optimal Integration of Inter-Task and Intra-Task
Dynamic Voltage Scaling Techniques for Hard

Real-Time Applications

Jaewon Seo

KAIST, Daejeon, KOREA
jwseo@jupiter.kaist.ac.kr

Taewhan Kim

Nikil D. Dutt

Seoul National University, Seoul, KOREA

University of California, Irvine, CA

tkim@ssl.snu.ac.kr

dutt@ics.uci.edu

Abstract— It is generally accepted that the dynamic voltage
scaling (DVS) is one of the most effective techniques for energy
minimization. According to the granularity of units to which volt-
age scaling is applied, the DVS problem can be divided into two
subproblems: (i) inter-task DVS problem and (ii) intra-task DVS
problem. A lot of effective DVS techniques have addressed either
one of the two subproblems, but none of them have attempted
to solve both simultaneously, which is mainly due to an excessive
computation complexity to solve it optimally. This work addresses
this core issue, that is, Can the combined problem be solved
effectively and efﬁciently? More specﬁcally, our work shows, for
a set of inter-dependednt tasks, that the combined DVS problem
can be solved optimally in polynomial time. Experimental results
indicate that the proposed integrated DVS technique is able to
reduce energy consumption by 10.6% on average over the results
by [11]+ [7] (i.e., a straightforward combination of two optimal
inter- and intra-task DVS techniques.)

I. INTRODUCTION

Over the past decades there have been enormous efforts to
minimize the energy consumption of CMOS circuit systems.
Dynamic voltage scaling (DVS) – involving dynamic adjust-
ments of the supply voltage and the corresponding operating
clock frequency – has emerged as one of the most effective
energy minimization techniques. A one-to-one correspondence
between the supply voltage and the clock frequency in CMOS
circuits imposes an inherent constraint to DVS techniques
to ensure that voltage adjustments do not violate the target
system’s timing constraints.

Many previous works have focused on hard real-time sys-
tems with multiple tasks. Their primary concern is to assign
a proper operating voltage to each task while satisfying the
task’s timing constraint. In these techniques, determination
of the voltage is carried out on a task-by-task basis and the
voltage assigned to the task is unchanged during the whole ex-
ecution of the task (Inter-Task DVS). Yao et al. [10] proposed
an optimal inter-task voltage scaling algorithm for independent
tasks, which determines the execution speed of each task at any
given time so that the total energy consumption is minimized.
Although their formulation does not impose any constraint on
the number of operating voltages assigned to each task, by
convexity of the power function, every task is given only one
voltage and executed at a constant speed. This result comes
from the underlying assumption that the required number of
cycles to ﬁnish the task is constant, which is unacceptable

in practice. With the same assumption, many works in the
literature have tried to formulate their own inter-task DVS
problems considering other issues such as dependent task sets
[1], [3], [6], [9], [11], discretely variable voltage processors
[3], [4], multi-processor environments [1], [3], [6], [9], [11],
voltage transition overheads [1], etc.

In the past few years, several studies (e.g., [7], [8]) added a
new dimension to the voltage scaling problem, by considering
energy saving opportunities within the task boundary. In their
approach, the operating voltage of the task is dynamically
adjusted according to the execution behavior (Intra-Task
DVS). Shin et al. [8] proposed the remaining worst-case
path-based algorithm which achieves the best granularity by
executing each basic block with possibly different operating
voltage. To obtain tight operating points for minimum energy
consumption, the algorithm updates the remaining path length
as soon as the execution deviates from the previous remain-
ing worst-case path. More recently, a proﬁle-based optimal
intra-task voltage scaling technique was presented in [7]. It
shows the best energy reduction by incorporating the task’s
execution proﬁle into the calculation of the operating voltages.
The algorithm is proved to be optimal in the sense that it
achieves minimum average energy consumption when the task
is executed repeatedly.

To the best of our knowledge, no work has attempted to
solve the inter-task and intra-task DVS problems simultane-
ously so as not to miss the energy saving opportunities at
both granularities. In this paper, we present the ﬁrst work that
solves this combined inter- and intra-task DVS problem to
fully exploit the advantages of both techniques. We propose
an optimal integrated approach based on two energy-optimal
inter-task [10] and intra-task [7] DVS algorithms.

II. DEFINITIONS AND PROBLEM FORMULATION

We consider that an application consists of N communicating
tasks, T = {τ1, τ2, . . . , τN}, of which relations are represented
by a directed acyclic graph (DAG). Each node in the graph
represents task and each arc between two nodes indicates
control/data dependency for two tasks. Fig. 3(a) shows an
example of this graph. If two tasks τi and τj are connected
by arc (τi, τj), the execution of τi must be completed before
the execution of τj. A deadline di might be assigned to task

0-7803-9254-X/05/$20.00 ©2005 IEEE.

449

τi in order to ensure correct functionality (e.g., τ4 and τ6 in
Fig. 3(a)), which we call local deadline. In addition, to restrict
all tasks’ ending time we add a special node called sink to the
graph. Every task of which execution is not constrained by
any of the deadlines is connected to the sink. (e.g., τ5 and
τ7 in Fig. 3(a)) Those tasks should be executed before the
deadline of the sink, which we call global deadline. More
detailed explanation on this task model can be found in the
recent inter-task DVS works e.g., in [3], [6], [11].
Each task τi ∈ T is associated with the following parame-

ters:

• ri : the required number of CPU cycles to complete,
• si : the starting time,
• ei : the ending time (ei ≥ si), and
• ti : the execution time (ti = ei − si)

Note that ri is given for each task τi, while the values of si,
ei and ti are determined after the task is actually scheduled.
The amount of energy consumption during the execution of

the task τi is expressed as:

Ei = Cef f · Vi

2 · ri

(1)

where Cef f and Vi denote the effective charged capacitance
and the voltage supplied to the task, respectively. The rela-
tionship between clock frequency fclk and supply voltage Vdd
in CMOS circuits is [4]:

fclk ∝ 1/Td = µCox(W/L)(Vdd − Vth)2

≈ Vdd

(2)

CLVdd

where Td is the delay, CL is the total node capacitance,
µ is the mobility, Cox is the oxide capacitance, Vth is the
threshold voltage and W/L is the width to length ratio of
transistors. Based on the above one-to-one correspondence,
we use voltage and corresponding speed (i.e., clock frequency)
interchangeably throughout the paper and rewrite Eq.(1) as:

Ei = K · fi

2 · ri

(3)

where K is the system-dependent constant and fi
is the
operational speed of the task τi. Although this equation seems
somewhat oversimpliﬁed, our method is generally applicable
if the energy consumption is represented as a power function
of the speed i.e., Ei = af b
i . For the most of the commercial
DVS processors it gives a good approximation of the actual
energy consumption with an admissible error. (e.g., less than
3.4% error for Transmeta TM5900 1000MHz [12])

Since in most cases a task shows different behaviors depend-
ing on input data, i.e. ri is not given as a constant, we cannot
directly apply the simple energy equation, Eq.(3). One way to
deal with the different execution paths is to consider average
(i.e., expected) energy consumption, which can be expressed
as:

(cid:1)

(cid:2)

(cid:3)
P rob(π) · E(π)

Ei =

(4)

∀exec. path π of τi

where P rob(π) is the probability that the execution of task τi
follows path π and E(π) is the energy consumption on that
path. The fact that many tasks in real-time environments are

N(cid:1)

executed repeatedly provides the rationale for the use of this
model. The overall energy consumption for the entire task set
T is given as:

Etot =

Ei

(5)

i=1

We deﬁne a feasible schedule of tasks to be a schedule in
which all the timing constraints of the tasks are satisﬁed. Note
that the inter-task DVS problem is to ﬁnd a (static) schedule
of each task and a voltage applied to the task to minimize the
amount of energy consumption. On the other hand, the intra-
task DVS problem is to ﬁnd a (dynamic) voltage-adjustment
scheme for each basic block in the task to minimize the amount
of (average) energy consumption while satisﬁng the deadline
constraint of the task. Then, the combined problem of the inter-
and intra-task DVS problems can be described as:

Problem 1 (The Combined DVS Problem) Given an instance
of tasks, ﬁnd an inter-task schedule and an intra-task voltage
scaling scheme that produces a feasible schedule for every
possible execution path of tasks and minimizes the quantity of
Etot in Eq.(5).

III. THE COMBINED DVS TECHNIQUE

The proposed integrated DVS approach is a two-step method:
1) Statically determine energy-optimal starting and ending
times (si and ei) for each task τi considering future use
of an intra-task voltage scaling.

2) Execute τi within [si, ei] while varying the processor
speed according to the voltage scales obtained by an
existing optimal intra-task DVS scheme.

Our key concern is to develop a new inter-task scheduling
algorithm that ﬁnds starting and ending times (si and ei) for
each task, which leads to a minimum value of Etot in Eq.(5)
when an optimal intra-task scheme is applied to the tasks.
(Note that the new inter-task scheduling algorithm is different
from any existing inter-task DVS scheme, which is not aware
of intra-task DVS at all.)

In the following, let us ﬁrst introduce an optimal intra-task
DVS technique [7] since it provides a basis on the development
of our combined enegy-optimal DVS technique.

A. An optimal intra-task DVS technique
A task τ is represented with its CFG (Control Flow Graph)
Gτ = (V, A), where V is the set of basic blocks in the task
and A is the set of directed edges which impose precedence
relations between basic blocks. (For example, see Fig. 1(a).)
The set of immediate successor basic blocks of any bi ∈ V is
denoted by succ(bi). Each basic block bi is annotated with its
non-zero number of execution cycles ni and each arc (bi, bj)
is given a probability pj that the execution follows the arc.

Following theorem states the lower bound energy consump-

tion that any intra-task DVS technique can achieve.
Theorem 1: Given a task’s CFG and its execution proﬁle that
offers the probabilities, the minimum energy consumption of

450

1p  = 0.7

1b
3

0b
6

2p  = 0.3

2b
4

3p  = 0.9

3b
2

4p  = 0.1

4b
7

5b
1

6p  = 0.2

6b
8

7p  = 0.8

7b
14

0b
6

d1 =3+19.1

=22.1

1b
3

3

d0 =6+  0.7*22.1  + 0.3*25.7

3

3

=29.3

2b
4

3

3
d2 =4+  0.9*21.1  + 0.1*26.1

3

=25.7

d3 =2+19.1

=21.1

3b
2

4b
7

d4 =7+19.1

=26.1

5b
1

3

3
d5 =1+  0.2*13 + 0.8*19

3

=19.1

d6 =8+5=13

6b
8

7b
14
d7 =14+5=19

8b
5

(a)

d8 =5

8b
5

(b)

Fig. 1.

(a) CFG of a task τsimple; (b) Calculation of δ values.

any intra-task DVS technique is bounded by:

δ0

(relative) deadline

(cid:5)

2 · δ0

(cid:4)

Eintra = K ·

ni,

ni + 3

(cid:9) (cid:10)
∀bj∈succ(bi)

δi =

where δi is deﬁned as:

if succ(bi) = ∅

3, otherwise

pj · δj

(6)

(7)

and δ0 is the δ value of the top basic block b0. (See [7] for
proof.)

One interesting interpretation of Eq.(6) is that it can be
considered as the energy consumed in the execution of δ0
cycles at the speed of δ0/deadline. We call δ0 energy-optimal
path length of the task.

The above theorem leads to the following optimal voltage
scaling scheme [7]: Before executing the task, we compute
the δi value of each basic block bi and insert the following
instruction code at the beginning of bi

change_f_V(δi/remaining_time());

where the instruction change_f_V(fclk) changes the cur-
rent clock frequency (i.e., speed) to fclk and adjusts the supply
voltage accordingly and remaining_time() returns the
remaining time to deadline.

For example, consider a real-time task τsimple shown in
Fig. 1(a). The number within each node indicates its number
of execution cycles ni. Fig. 1(b) shows the procedure of
calculating each δi value according to Eq.(7). Once we have
ﬁnished the calculation, the operating speed of basic block
bi is simply obtained by dividing δi by the remaining time
to deadline. Suppose that deadline = 10 (unit time) and the
execution follows the path (b0, b2, b3, b5, b6, b8). Then,
the
corresponding speed changes as follows:

Speed : 2.93 > 3.24 > 3.14 > 3.14 > 2.26 > 2.26

In Fig. 1(b), the thick, dotted, and regular arrows indicate
the increase, decrease and no change of the processor speed,
respectively and the basic blocks with changed operating speed

t1
10

t2
5

t4
75

t3
40

t5
20

tsink

d     = 300

sink

(a)

t1
10

t2
5

t3
40

t4
75

t5
20

(b)
initial speed (=    / t  ) 

D i

i

t2

t1

t3

t4

t5

0

20 25

110

260

300

time

(c)

Fig. 2.
dependencies; (c) Scheduled tasks.

(a) A simple task set Tsimple; (b) A task order preserving

are marked with gray-color. More details including practical
issues (e.g. handling loop constructs, voltage transition over-
heads, discretely variable voltages, etc.) can be found in [7].

the

B. An intra-task DVS-aware inter-task DVS technique
One characteristic of the previous intra-task DVS technique is
that after an execution time (or relative deadline) is assigned,
the scheduler ﬁnishes the task exactly at the deadline no matter
which execution path is taken, producing no slack time. Thus,
the next step is to determine the starting and ending times of
each task. We ﬁrst give a characterization of an energy-optimal
schedule for any set of tasks that have no local deadlines.
Case 1 - Tasks with a global deadline only: For
case
where each task has no local deadline and of which ending
time is restricted only by the global deadline dsink (For
example, see Fig. 2(a)), the problem can be solved easily by
properly distributing the permitted execution time (= dsink)
over the tasks and then determining the order of the tasks.
Following lemma states the optimal distribution of dsink which
leads to the minimum energy consumption when the previous
optimal intra-task scheme is applied.
Lemma 1: Given a task set T = {τ1, τ2, . . . , τN} with no
local deadlines and the global deadline dsink the total energy
consumption Etot is minimized when each task is assigned
the execution time proportional to its energy-optimal execution
path length i.e., when ti is given as:
∆1 + ··· + ∆N

· dsink
where ∆i denotes the δ0 of Eq. (7) for τi.
(cid:5)

energy consumption is expressed as:
2 · ∆1 + K ·

(cid:4)∆2

(cid:4)∆1

Proof: For N = 2, if we set t1 = x and t2 = y, the total

(9)
where x + y = dsink. By substituting dsink − x for y and
Eq. (9) has the minimum K·(cid:11)
then differentiating with respect to x, it can be veriﬁed that
2·(∆1+∆2) where x =
∆1+∆2 . For N = m, assume the lemma holds for m−1
dsink· ∆1
tasks. Then the energy equation becomes essentially identical

2 · ∆2

∆1+∆2
dsink

K ·

ti =

∆i

(cid:5)

(8)

(cid:12)

x

y

451

to Eq. (9). (∆2 is replaced by ∆2 + ··· + ∆m.) Therefore the
lemma holds.

Now we know how much time should be given to each
task for execution. Thus the next step is to determine the
starting time (and automatically ending time) for each task,
which is obtained directly from a task order. Since there is no
local deadline for each task, any order that preserves all the
dependencies between the tasks sufﬁces the minimum energy
schedule. Without
loss of generality assume the sequence
(τ1, τ2, . . . , τN ) does not violate the precedence relations.
Then the starting and ending time of each task are determined
simply as:

0,
ei−1,

for i = 0
for i ≥ 1

si =

ei = si + ti

and

(10)
For example, suppose a simple task set Tsimple = {τ1, τ2, τ3,
τ4, τ5} of which DAG is shown in Fig. 2(a). Each task has no
local deadline and there is only one global deadline dsink =
300 unit time speciﬁed by the sink. The number inside each
node indicates the δ0 value of the corresponding task. Fig. 2(b)
shows an order that preserves the dependency relations, from
which we calculate the time stamps for each task as shown in
Fig. 2(c) using Eq. (8) and Eq. (10)

(cid:13)

Case 2 - Tasks with arbitrary deadlines: For the general case
where tasks possibly have local deadlines as well as the
global deadline, we divide the problem into a collection of
subproblems of Case 1 and then apply the result of Case 1.
Firstly, we order the tasks with the Earliest Deadline First
(EDF) policy while preserving the dependency relations1. The
EDF policy is proved to give the best opportunity for energy
savings [11]. Let (τ1, τ2, . . . , τN ) be the obtained sequence
and for the sake of illustration assume only two tasks τi and
τj (i < j) have local deadlines. Then we partition the task
set into three task groups {τ1, . . . , τi}, {τi+1, . . . , τj}, and
{τj+1, . . . , τN} and apply the method in Case 1 to each group
separately, where we set the starting times of τ1, τi+1, and
τj+1 to 0, di, and dj, respectively.

For example, consider a task set shown in Fig. 3(a). We can
order the tasks by the EDF policy as shown in Fig. 3(b). Since
only two tasks τ4 and τ6 have local deadlines, the task set is
divided into three groups {τ1, τ2, τ4}, {τ3, τ6}, and {τ5, τ7} as
shown in Fig. 3(c), and we set the starting time of each group
0, 40(= d4) and 120(= d6), respectively. Then the permitted
execution times 40, 80(= 120− 40), and 80(= 200− 120) are
distributed proportionally to the tasks in each group according
to Lemma 1.

C. An improved inter-task DVS technique
It should be noted that the previous method of assigning a
starting time to each task group does not guarantee to produce
the minimum energy schedule. To overcome this limitation, we
transform the original problem into an inter-task scheduling

1Every task of which deadline is not speciﬁed inherits the deadline of its

earliest successor task.

(a)

t1
5

t2
15

t4
15

d   = 40

4

t7
35

t1
5

t1
5

t3
65

t5
5

t6
20
d   = 120

6

initial speed

t1

t2
15

t2
15
t1

t4
15

t4
15

t3
65

t3
65

t6
 20

t6
 20
t2

t2

t4

t3

t6

t5

t7
35

t7
35

t3

t5
5

t5
5

t7

(b)

(c)

(d)

0

5 20 35

100

120

200

time

tsink

d     = 200

sink

Fig. 3.
groups/supertasks; (d) Scheduled tasks.

(a) Tasks with local deadlines; (b) EDF order; (c) Dividing into task

problem for independent tasks and use the well-known energy-
optimal scheduling algorithm by Yao et al. [10].

In their task model, each task is speciﬁed by the triple of the
arrival time, deadline, and the required number of CPU cycles
to complete. The algorithm identiﬁes a ‘critical’ interval where
a set of tasks should be completed within that interval and no
other tasks should be executed in the interval to minimize
the energy consumption, and then schedules those ‘critical’
tasks at a maximum constant speed by the earliest deadline
policy. The process is repeatedly applied to the subproblem
for the remaining unscheduled tasks until there remains no
unscheduled task. As a result we have a feasible schedule of
which energy consumption:

Einter =

N(cid:1)

(cid:14)
K ·

(cid:5)

(cid:4) ri

ti

i=1

(cid:15)

2 · ri

(11)

is minimized.
Transforming the original problem: Each task group {τ1, . . .
, τi}, {τi+1, . . . , τj}, and {τj+1, . . . , τN} in the previous sec-
tion is now considered as a single task, which we call supertask
and denote ˆτ1, ˆτ2, and ˆτ3. Note that we use the hat symbol(ˆ)
to differentiate supertasks from ordinary tasks in Yao et al.’s
model. The arrival time ˆai of each supertask is set to 0 and
the deadline ˆdi is inherited from the original task group. One
difference is that the required number of CPU cycles ˆri for
each supertask is not set to the sum of consisting tasks’ ri but
to the sum of ∆i, more speciﬁcally they are set as follows:

ˆr1 = ∆1 + ··· + ∆i
ˆr2 = ∆i+1 + ··· + ∆j
ˆr3 = ∆j+1 + ··· + ∆N

The resulting schedule after applying those supertasks to Yao
et al.’s algorithm directly determines the starting and ending
times of each supertask. We then continue the same procedure
of the previous section.

The detailed description of the algorithm is shown in Fig. 4.
The procedure Construct Supertask(1, T ) constructs the set of
supertasks recursively from the given task set T . The function
h(i) returns the index of the ith task that has local deadline in
the EDF order and the pred(τi) returns the set of all immediate
predecessor tasks of the task τi.

452

Fig. 3 shows an example. The original task set T is shown
in Fig. 3(a). Fig. 3(b) represents the sequence obtained by the
EDF ordering. Note that there are two tasks τ4 and τ6 that
have local deadlines. (Therefore h(1) = 4 and h(2) = 6). We
partition the task set into three task groups or equivalently
three supertasks ˆτ1 = {τ1, τ2, τ4}, ˆτ2 = {τ3, τ6}, and ˆτ3 =
{τ5, τ7} as shown in Fig. 3(c). The arrival time, deadline, and
the required number of cycles are set as follows:

ˆa1 = ˆa2 = ˆa3 = 0
ˆd1 = dh(1) = 40
ˆd2 = dh(2) = 120
ˆd3 = dh(3) = 200
ˆr1 = ∆1 + ∆2 + ∆4 = 35
ˆr2 = ∆3 + ∆6 = 85
ˆr3 = ∆5 + ∆7 = 40

Fig. 3(d) shows the resultant schedule of the supertasks using
Yao et al.’s algorithm. Finally, each individual task in the
supertask receives the execution time according to Lemma 1,
which is depicted also in Fig. 3(d).
Theorem 2: DVS-intgr produces a feasible and minimum
energy schedule when followed by the optimal
intra-task
voltage scaling.
Proof Sketch. Since every suppertask has zero arrival time, for
any critical interval (with supertasks), each supertask has all its
preceding (unscheduled) supertasks in the same interval. Thus
the scheduling for the supertask set does not alter the EDF
order, which leads to a feasible schedule. (See [10] for more
details.) The obtained supertask schedule has the minimum
value of the following energy equation:

ˆN(cid:1)

(cid:14)
K ·

i=1

(cid:5)

(cid:4)(cid:10)
∆j
ˆti

2 ·

(cid:15)

∆j

(cid:1)
∀τj∈ˆτi

(12)

Esupertask =

where ˆN is the number of supertasks. Note that this equation is
essentially identical to Eq. (11). It can be proved that the inner
term of the summation in Eq. (12) represents the lower bound
of the energy consumption that the consisting tasks can have
within the interval of ˆti. Since each individual task is assigned
the execution time proportional to its energy-optimal path
length, by Lemma 1 the lower bound is achieved. Therefore
the theorem holds.

IV. EXPERIMENTAL RESULTS

We implemented the proposed integrated DVS technique,
called DVS-intgr,
in C++ and tested it on the task sets
generated by TGFF v3.0 [2]. We used the example input ﬁles
(*.tgffopt) provided with the package to obtain the task sets.
Each individual task was generated by inserting a left/right
child and a grandchild into an arbitrarily chosen basic block
repeatedly until the number of conditional branches reaches
some ﬁxed number in the range of [1, 100]. The probability
at each branch is drawn from a random normal distribution
with standard deviation of 1.0 and mean of 0.5. The number of
execution cycles (i.e., length) of each basic block is restricted

DVS-intgr {
• Input : Set T of tasks
• Output : Tasks with starting/ending time speciﬁed

ˆT = Construct Supertask(1, T );
Yao Inter Scheduler( ˆT );
for ∀ˆτi ∈ ˆT {

current time = ˆsi;
while (ˆτi (cid:3)= ∅) {

τj = Retrieve First Task(ˆτi);
sj = current time;
ej = sj + (∆j /ˆri) · ˆti;
current time = ej ;

// schedule each task
in the supertask
//

}}}
Construct Supertask(i, T ) {

if (T = ∅) return ∅;
ˆτi = {τh(i)};
ˆai = 0;
ˆdi = dh(i);
ˆri = ∆h(i);
T = T − {τh(i)};
T 1 = pred(τh(i));
while (∃τj ∈ T 1) {
ˆτi = ˆτi ∪ {τj};
ˆri = ˆri + ∆j ;
T = T − {τj};
T 1 = T 1 − {τj} ∪ pred(τj );

//
// initialize the ith
//
//

supertask

// add a task to the ith
//

supertask

}
return {ˆτi}∪ Construct Supertask(i + 1, T ); }
// h(i) returns index of ith task that has local deadline.
// pred(τi) returns all immediate predecessors of τi

Fig. 4. A summary of the proposed algorithm.

such that the length of the longest basic block does not exceed
100 times of the length of the shortest one. Then the overall
lengths in each task are multiplied by a factor such that the
worst-case execution of the corresponding supertask produces
20% slack time (within its bound) when the maximum speed
is used.

Table I shows a summary of the experimental results. Our
competitors include the inter-task DVS technique by Zhang
[11] (Zhang-only) and the same technique followed by appli-
cation of the optimal intra-task DVS technique (Zhang+Intra).
Zhang et al. solved the inter-task DVS problem for dependent
tasks using integer programming. Although their technique
produces an optimal solution to the inter-task DVS problem,
they did not consider future application of any intra-task
DVS techniques, which inevitably leads to non-optimal energy
consumption when combined with intra-task DVS techniques.
To give the best chance to Zhang-only,
it was assumed
that when the task ﬁnishes earlier than the scheduled time,
the target system becomes idle with the power-down mode
consuming no energy. We repeated the experiment for each
task set 1000 times and then obtained the average reduction
of energy consumption after discarding higher and lower 100
results, respectively. The sixth and seventh columns in the table
represent the average energy consumptions of Zhang+Intra
and DVS-intgr respectively, normalized to the corresponding
energy consumption of Zhang-only. From the sixth column,
one can easily see that
the follow-up application of the
optimal intra-task DVS technique drastically reduces the total
energy consumption. The eighth column shows the average
energy reduction of our technique over Zhang+Intra for each

453

Task Set
(Script)

[2]
bus0
bus1

kbasic task0
kbasic task1
kbasic task2
kbasic task3
kbasic task4
kbasic task5
kbasic task6
kbasic task7
kbasic task8
kbasic task9
kextended0
kextended1
kextended2

kseries parallel0
kseries parallel1
kseries parallel2
kseries parallel3

kseries parallel xover0
kseries parallel xover1
kseries parallel xover2
kseries parallel xover3

simple0
simple1
simple2
simple3
simple4

Avg.

Number

Number

of

Tasks

of

Edges

No. of

Tasks with
Deadline

Normalized Energy Consumption (Avg.)

Avg. Reduction

Zhang-only

Zhang + Intra

DVS-intgr

over

Zhang + Intra

15
15
42
64
42
36
21
21
18
32
23
35
23
21
22
30
20
62
47
30
21
38
27
12
20
24
11
22

30
30
50
79
46
47
22
23
18
36
27
40
25
28
27
33
19
61
46
37
24
41
30
19
25
28
12
32

5
5
17
23
17
12
10
7
10
15
9
15
8
6
6
4
4
9
6
3
4
4
4
1
6
6
3
4

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

0.3840
0.2564
0.2760
0.3590
0.2617
0.3287
0.3259
0.2595
0.2405
0.3609
0.2856
0.3052
0.3527
0.3496
0.3486
0.3070
0.2472
0.3003
0.3163
0.3057
0.2618
0.3458
0.2494
0.3691
0.2832
0.2897
0.2732
0.2713
0.3041

0.3549
0.2321
0.2516
0.3286
0.2321
0.3007
0.2838
0.2345
0.2127
0.3234
0.2567
0.2888
0.3184
0.3148
0.3141
0.2718
0.2175
0.2770
0.2871
0.2603
0.2422
0.3209
0.2128
0.3181
0.2563
0.2556
0.2337
0.2459
0.2731

7.8%
9.4%
9.3%
9.1%
10.3%
8.7%
13.3%
11.6%
12.8%
8.8%
10.2%
6.3%
10.0%
11.7%
9.9%
12.2%
11.4%
8.0%
10.2%
15.1%
8.2%
7.2%
15.3%
13.9%
9.7%
13.9%
14.2%
9.2%
10.6%

A COMPARISON OF ENERGY CONSUMPTIONS FOR THE TASK SETS IN [2].

TABLE I

task set. (Note that this value does not come directly from
the corresponding energy values in the sixth and seventh
column since it is the average of reduction percentages.) In
comparison, DVS-intgr reduces energy consumption by 72.7%
and 10.6% on average compared to that of Zhang-only and
Zhang + Intra, respectively.

V. CONCLUSIONS

We have presented a novel DVS technique to deal with the
combined inter- and intra-task DVS problem. To solve this
problem, ﬁrst we examined the lower bound of the energy
consumption that any intra-task DVS technique can achieve
and then using this property we devised a method setting
energy-optimal execution time to each task. To obtain the
globally optimal task schedule we divided the task set into
several task groups such that each task in the group can be
scheduled optimally within the group boundary and then trans-
formed the problem of determining the group boundary into an
inter-task DVS problem for independent tasks, which is then
solved using the existing optimal inter-task DVS technique.
The experimental results showed that our proposed integrated
DVS technique was able to reduce the energy consumption by
10.6% over that of the combination of the existing techniques
of [11] followed by [7].

REFERENCES

[1] A. Andrei et al., “Overhead-conscious voltage selection for
dynamic and leakage energy reduction of time-constrained sys-
tems,” Prof. of Design Automation and Test in Europe, 2004.

[2] R. P. Dick, D. L. Rhodes, and W. Wolf, “Tgff: Task graphs
for free,” Proc. of Sixth International Workshop on Hard-
ware/Software Codesign, Mar. 1998.

[3] B. Gorji-Ara et al., “Fast and efﬁcient voltage scheduling by
evolutionary slack distribution,” Proc. of Asia-South Paciﬁc
Design Automation Conference, 2004.

[4] W.-C. Kwon and T. Kim, “Optimal voltage allocation techniques
for dynamically variable voltage processors,” Proc. of Design
Automation Conference, 2003.

[5] C. L. Liu and J. W. Layland, “Scheduling algorithms for
multiprogramming in a hard real-time environment,” Journal of
the ACM, Vol. 20, Jan. 1973.

[6] M. T. Schmitz, B. M. Al-Hashimi, and P. Eles, “Energy-efﬁcient
mapping and scheduling for DVS enabled distributed embedded
systems,” Prof. of Design Automation and Test in Europe , 2002.
[7] J. Seo, T. Kim, and K. Chung, “Proﬁle-based optimal intra-
task voltage scheduling for hard real-time applications,” Proc.
of Design Automation Conference, 2004.

[8] D. Shin, J. Kim, and S. Lee, “Intra-task voltage scheduling for
low-energy hard real-time applications, IEEE Design and Test
of Computers, Vol. 18, 2001.

[9] G. Varatkar and R. Marculescu, “Communication-aware task
scheduling and voltage selection for total systems energy mini-
mization,” Proc. of International Conference on Computer-Aided
Design, 2003.

[10] F. Yao, A. Demers, and S. Shenker, “A scheduling model for
reduced CPU energy,” Proc. of IEEE Symposium on Foundations
of Computer Science, 1995.

[11] Y. Zhang, X. Hu, and D. Z. Chen, “Task scheduling and voltage
selection for energy minimization,” Proc. of Design Automation
Conference, 2002.

[12] http://www.transmeta.com

454

Optimal Integration of Inter-Task and Intra-Task
Dynamic Voltage Scaling Techniques for Hard

Real-Time Applications

Jaewon Seo

KAIST, Daejeon, KOREA
jwseo@jupiter.kaist.ac.kr

Taewhan Kim

Nikil D. Dutt

Seoul National University, Seoul, KOREA

University of California, Irvine, CA

tkim@ssl.snu.ac.kr

dutt@ics.uci.edu

Abstract— It is generally accepted that the dynamic voltage
scaling (DVS) is one of the most effective techniques for energy
minimization. According to the granularity of units to which volt-
age scaling is applied, the DVS problem can be divided into two
subproblems: (i) inter-task DVS problem and (ii) intra-task DVS
problem. A lot of effective DVS techniques have addressed either
one of the two subproblems, but none of them have attempted
to solve both simultaneously, which is mainly due to an excessive
computation complexity to solve it optimally. This work addresses
this core issue, that is, Can the combined problem be solved
effectively and efﬁciently? More specﬁcally, our work shows, for
a set of inter-dependednt tasks, that the combined DVS problem
can be solved optimally in polynomial time. Experimental results
indicate that the proposed integrated DVS technique is able to
reduce energy consumption by 10.6% on average over the results
by [11]+ [7] (i.e., a straightforward combination of two optimal
inter- and intra-task DVS techniques.)

I. INTRODUCTION

Over the past decades there have been enormous efforts to
minimize the energy consumption of CMOS circuit systems.
Dynamic voltage scaling (DVS) – involving dynamic adjust-
ments of the supply voltage and the corresponding operating
clock frequency – has emerged as one of the most effective
energy minimization techniques. A one-to-one correspondence
between the supply voltage and the clock frequency in CMOS
circuits imposes an inherent constraint to DVS techniques
to ensure that voltage adjustments do not violate the target
system’s timing constraints.

Many previous works have focused on hard real-time sys-
tems with multiple tasks. Their primary concern is to assign
a proper operating voltage to each task while satisfying the
task’s timing constraint. In these techniques, determination
of the voltage is carried out on a task-by-task basis and the
voltage assigned to the task is unchanged during the whole ex-
ecution of the task (Inter-Task DVS). Yao et al. [10] proposed
an optimal inter-task voltage scaling algorithm for independent
tasks, which determines the execution speed of each task at any
given time so that the total energy consumption is minimized.
Although their formulation does not impose any constraint on
the number of operating voltages assigned to each task, by
convexity of the power function, every task is given only one
voltage and executed at a constant speed. This result comes
from the underlying assumption that the required number of
cycles to ﬁnish the task is constant, which is unacceptable

in practice. With the same assumption, many works in the
literature have tried to formulate their own inter-task DVS
problems considering other issues such as dependent task sets
[1], [3], [6], [9], [11], discretely variable voltage processors
[3], [4], multi-processor environments [1], [3], [6], [9], [11],
voltage transition overheads [1], etc.

In the past few years, several studies (e.g., [7], [8]) added a
new dimension to the voltage scaling problem, by considering
energy saving opportunities within the task boundary. In their
approach, the operating voltage of the task is dynamically
adjusted according to the execution behavior (Intra-Task
DVS). Shin et al. [8] proposed the remaining worst-case
path-based algorithm which achieves the best granularity by
executing each basic block with possibly different operating
voltage. To obtain tight operating points for minimum energy
consumption, the algorithm updates the remaining path length
as soon as the execution deviates from the previous remain-
ing worst-case path. More recently, a proﬁle-based optimal
intra-task voltage scaling technique was presented in [7]. It
shows the best energy reduction by incorporating the task’s
execution proﬁle into the calculation of the operating voltages.
The algorithm is proved to be optimal in the sense that it
achieves minimum average energy consumption when the task
is executed repeatedly.

To the best of our knowledge, no work has attempted to
solve the inter-task and intra-task DVS problems simultane-
ously so as not to miss the energy saving opportunities at
both granularities. In this paper, we present the ﬁrst work that
solves this combined inter- and intra-task DVS problem to
fully exploit the advantages of both techniques. We propose
an optimal integrated approach based on two energy-optimal
inter-task [10] and intra-task [7] DVS algorithms.

II. DEFINITIONS AND PROBLEM FORMULATION

We consider that an application consists of N communicating
tasks, T = {τ1, τ2, . . . , τN}, of which relations are represented
by a directed acyclic graph (DAG). Each node in the graph
represents task and each arc between two nodes indicates
control/data dependency for two tasks. Fig. 3(a) shows an
example of this graph. If two tasks τi and τj are connected
by arc (τi, τj), the execution of τi must be completed before
the execution of τj. A deadline di might be assigned to task

0-7803-9254-X/05/$20.00 ©2005 IEEE.

449

τi in order to ensure correct functionality (e.g., τ4 and τ6 in
Fig. 3(a)), which we call local deadline. In addition, to restrict
all tasks’ ending time we add a special node called sink to the
graph. Every task of which execution is not constrained by
any of the deadlines is connected to the sink. (e.g., τ5 and
τ7 in Fig. 3(a)) Those tasks should be executed before the
deadline of the sink, which we call global deadline. More
detailed explanation on this task model can be found in the
recent inter-task DVS works e.g., in [3], [6], [11].
Each task τi ∈ T is associated with the following parame-

ters:

• ri : the required number of CPU cycles to complete,
• si : the starting time,
• ei : the ending time (ei ≥ si), and
• ti : the execution time (ti = ei − si)

Note that ri is given for each task τi, while the values of si,
ei and ti are determined after the task is actually scheduled.
The amount of energy consumption during the execution of

the task τi is expressed as:

Ei = Cef f · Vi

2 · ri

(1)

where Cef f and Vi denote the effective charged capacitance
and the voltage supplied to the task, respectively. The rela-
tionship between clock frequency fclk and supply voltage Vdd
in CMOS circuits is [4]:

fclk ∝ 1/Td = µCox(W/L)(Vdd − Vth)2

≈ Vdd

(2)

CLVdd

where Td is the delay, CL is the total node capacitance,
µ is the mobility, Cox is the oxide capacitance, Vth is the
threshold voltage and W/L is the width to length ratio of
transistors. Based on the above one-to-one correspondence,
we use voltage and corresponding speed (i.e., clock frequency)
interchangeably throughout the paper and rewrite Eq.(1) as:

Ei = K · fi

2 · ri

(3)

where K is the system-dependent constant and fi
is the
operational speed of the task τi. Although this equation seems
somewhat oversimpliﬁed, our method is generally applicable
if the energy consumption is represented as a power function
of the speed i.e., Ei = af b
i . For the most of the commercial
DVS processors it gives a good approximation of the actual
energy consumption with an admissible error. (e.g., less than
3.4% error for Transmeta TM5900 1000MHz [12])

Since in most cases a task shows different behaviors depend-
ing on input data, i.e. ri is not given as a constant, we cannot
directly apply the simple energy equation, Eq.(3). One way to
deal with the different execution paths is to consider average
(i.e., expected) energy consumption, which can be expressed
as:

(cid:1)

(cid:2)

(cid:3)
P rob(π) · E(π)

Ei =

(4)

∀exec. path π of τi

where P rob(π) is the probability that the execution of task τi
follows path π and E(π) is the energy consumption on that
path. The fact that many tasks in real-time environments are

N(cid:1)

executed repeatedly provides the rationale for the use of this
model. The overall energy consumption for the entire task set
T is given as:

Etot =

Ei

(5)

i=1

We deﬁne a feasible schedule of tasks to be a schedule in
which all the timing constraints of the tasks are satisﬁed. Note
that the inter-task DVS problem is to ﬁnd a (static) schedule
of each task and a voltage applied to the task to minimize the
amount of energy consumption. On the other hand, the intra-
task DVS problem is to ﬁnd a (dynamic) voltage-adjustment
scheme for each basic block in the task to minimize the amount
of (average) energy consumption while satisﬁng the deadline
constraint of the task. Then, the combined problem of the inter-
and intra-task DVS problems can be described as:

Problem 1 (The Combined DVS Problem) Given an instance
of tasks, ﬁnd an inter-task schedule and an intra-task voltage
scaling scheme that produces a feasible schedule for every
possible execution path of tasks and minimizes the quantity of
Etot in Eq.(5).

III. THE COMBINED DVS TECHNIQUE

The proposed integrated DVS approach is a two-step method:
1) Statically determine energy-optimal starting and ending
times (si and ei) for each task τi considering future use
of an intra-task voltage scaling.

2) Execute τi within [si, ei] while varying the processor
speed according to the voltage scales obtained by an
existing optimal intra-task DVS scheme.

Our key concern is to develop a new inter-task scheduling
algorithm that ﬁnds starting and ending times (si and ei) for
each task, which leads to a minimum value of Etot in Eq.(5)
when an optimal intra-task scheme is applied to the tasks.
(Note that the new inter-task scheduling algorithm is different
from any existing inter-task DVS scheme, which is not aware
of intra-task DVS at all.)

In the following, let us ﬁrst introduce an optimal intra-task
DVS technique [7] since it provides a basis on the development
of our combined enegy-optimal DVS technique.

A. An optimal intra-task DVS technique
A task τ is represented with its CFG (Control Flow Graph)
Gτ = (V, A), where V is the set of basic blocks in the task
and A is the set of directed edges which impose precedence
relations between basic blocks. (For example, see Fig. 1(a).)
The set of immediate successor basic blocks of any bi ∈ V is
denoted by succ(bi). Each basic block bi is annotated with its
non-zero number of execution cycles ni and each arc (bi, bj)
is given a probability pj that the execution follows the arc.

Following theorem states the lower bound energy consump-

tion that any intra-task DVS technique can achieve.
Theorem 1: Given a task’s CFG and its execution proﬁle that
offers the probabilities, the minimum energy consumption of

450

1p  = 0.7

1b
3

0b
6

2p  = 0.3

2b
4

3p  = 0.9

3b
2

4p  = 0.1

4b
7

5b
1

6p  = 0.2

6b
8

7p  = 0.8

7b
14

0b
6

d1 =3+19.1

=22.1

1b
3

3

d0 =6+  0.7*22.1  + 0.3*25.7

3

3

=29.3

2b
4

3

3
d2 =4+  0.9*21.1  + 0.1*26.1

3

=25.7

d3 =2+19.1

=21.1

3b
2

4b
7

d4 =7+19.1

=26.1

5b
1

3

3
d5 =1+  0.2*13 + 0.8*19

3

=19.1

d6 =8+5=13

6b
8

7b
14
d7 =14+5=19

8b
5

(a)

d8 =5

8b
5

(b)

Fig. 1.

(a) CFG of a task τsimple; (b) Calculation of δ values.

any intra-task DVS technique is bounded by:

δ0

(relative) deadline

(cid:5)

2 · δ0

(cid:4)

Eintra = K ·

ni,

ni + 3

(cid:9) (cid:10)
∀bj∈succ(bi)

δi =

where δi is deﬁned as:

if succ(bi) = ∅

3, otherwise

pj · δj

(6)

(7)

and δ0 is the δ value of the top basic block b0. (See [7] for
proof.)

One interesting interpretation of Eq.(6) is that it can be
considered as the energy consumed in the execution of δ0
cycles at the speed of δ0/deadline. We call δ0 energy-optimal
path length of the task.

The above theorem leads to the following optimal voltage
scaling scheme [7]: Before executing the task, we compute
the δi value of each basic block bi and insert the following
instruction code at the beginning of bi

change_f_V(δi/remaining_time());

where the instruction change_f_V(fclk) changes the cur-
rent clock frequency (i.e., speed) to fclk and adjusts the supply
voltage accordingly and remaining_time() returns the
remaining time to deadline.

For example, consider a real-time task τsimple shown in
Fig. 1(a). The number within each node indicates its number
of execution cycles ni. Fig. 1(b) shows the procedure of
calculating each δi value according to Eq.(7). Once we have
ﬁnished the calculation, the operating speed of basic block
bi is simply obtained by dividing δi by the remaining time
to deadline. Suppose that deadline = 10 (unit time) and the
execution follows the path (b0, b2, b3, b5, b6, b8). Then,
the
corresponding speed changes as follows:

Speed : 2.93 > 3.24 > 3.14 > 3.14 > 2.26 > 2.26

In Fig. 1(b), the thick, dotted, and regular arrows indicate
the increase, decrease and no change of the processor speed,
respectively and the basic blocks with changed operating speed

t1
10

t2
5

t4
75

t3
40

t5
20

tsink

d     = 300

sink

(a)

t1
10

t2
5

t3
40

t4
75

t5
20

(b)
initial speed (=    / t  ) 

D i

i

t2

t1

t3

t4

t5

0

20 25

110

260

300

time

(c)

Fig. 2.
dependencies; (c) Scheduled tasks.

(a) A simple task set Tsimple; (b) A task order preserving

are marked with gray-color. More details including practical
issues (e.g. handling loop constructs, voltage transition over-
heads, discretely variable voltages, etc.) can be found in [7].

the

B. An intra-task DVS-aware inter-task DVS technique
One characteristic of the previous intra-task DVS technique is
that after an execution time (or relative deadline) is assigned,
the scheduler ﬁnishes the task exactly at the deadline no matter
which execution path is taken, producing no slack time. Thus,
the next step is to determine the starting and ending times of
each task. We ﬁrst give a characterization of an energy-optimal
schedule for any set of tasks that have no local deadlines.
Case 1 - Tasks with a global deadline only: For
case
where each task has no local deadline and of which ending
time is restricted only by the global deadline dsink (For
example, see Fig. 2(a)), the problem can be solved easily by
properly distributing the permitted execution time (= dsink)
over the tasks and then determining the order of the tasks.
Following lemma states the optimal distribution of dsink which
leads to the minimum energy consumption when the previous
optimal intra-task scheme is applied.
Lemma 1: Given a task set T = {τ1, τ2, . . . , τN} with no
local deadlines and the global deadline dsink the total energy
consumption Etot is minimized when each task is assigned
the execution time proportional to its energy-optimal execution
path length i.e., when ti is given as:
∆1 + ··· + ∆N

· dsink
where ∆i denotes the δ0 of Eq. (7) for τi.
(cid:5)

energy consumption is expressed as:
2 · ∆1 + K ·

(cid:4)∆2

(cid:4)∆1

Proof: For N = 2, if we set t1 = x and t2 = y, the total

(9)
where x + y = dsink. By substituting dsink − x for y and
Eq. (9) has the minimum K·(cid:11)
then differentiating with respect to x, it can be veriﬁed that
2·(∆1+∆2) where x =
∆1+∆2 . For N = m, assume the lemma holds for m−1
dsink· ∆1
tasks. Then the energy equation becomes essentially identical

2 · ∆2

∆1+∆2
dsink

K ·

ti =

∆i

(cid:5)

(8)

(cid:12)

x

y

451

to Eq. (9). (∆2 is replaced by ∆2 + ··· + ∆m.) Therefore the
lemma holds.

Now we know how much time should be given to each
task for execution. Thus the next step is to determine the
starting time (and automatically ending time) for each task,
which is obtained directly from a task order. Since there is no
local deadline for each task, any order that preserves all the
dependencies between the tasks sufﬁces the minimum energy
schedule. Without
loss of generality assume the sequence
(τ1, τ2, . . . , τN ) does not violate the precedence relations.
Then the starting and ending time of each task are determined
simply as:

0,
ei−1,

for i = 0
for i ≥ 1

si =

ei = si + ti

and

(10)
For example, suppose a simple task set Tsimple = {τ1, τ2, τ3,
τ4, τ5} of which DAG is shown in Fig. 2(a). Each task has no
local deadline and there is only one global deadline dsink =
300 unit time speciﬁed by the sink. The number inside each
node indicates the δ0 value of the corresponding task. Fig. 2(b)
shows an order that preserves the dependency relations, from
which we calculate the time stamps for each task as shown in
Fig. 2(c) using Eq. (8) and Eq. (10)

(cid:13)

Case 2 - Tasks with arbitrary deadlines: For the general case
where tasks possibly have local deadlines as well as the
global deadline, we divide the problem into a collection of
subproblems of Case 1 and then apply the result of Case 1.
Firstly, we order the tasks with the Earliest Deadline First
(EDF) policy while preserving the dependency relations1. The
EDF policy is proved to give the best opportunity for energy
savings [11]. Let (τ1, τ2, . . . , τN ) be the obtained sequence
and for the sake of illustration assume only two tasks τi and
τj (i < j) have local deadlines. Then we partition the task
set into three task groups {τ1, . . . , τi}, {τi+1, . . . , τj}, and
{τj+1, . . . , τN} and apply the method in Case 1 to each group
separately, where we set the starting times of τ1, τi+1, and
τj+1 to 0, di, and dj, respectively.

For example, consider a task set shown in Fig. 3(a). We can
order the tasks by the EDF policy as shown in Fig. 3(b). Since
only two tasks τ4 and τ6 have local deadlines, the task set is
divided into three groups {τ1, τ2, τ4}, {τ3, τ6}, and {τ5, τ7} as
shown in Fig. 3(c), and we set the starting time of each group
0, 40(= d4) and 120(= d6), respectively. Then the permitted
execution times 40, 80(= 120− 40), and 80(= 200− 120) are
distributed proportionally to the tasks in each group according
to Lemma 1.

C. An improved inter-task DVS technique
It should be noted that the previous method of assigning a
starting time to each task group does not guarantee to produce
the minimum energy schedule. To overcome this limitation, we
transform the original problem into an inter-task scheduling

1Every task of which deadline is not speciﬁed inherits the deadline of its

earliest successor task.

(a)

t1
5

t2
15

t4
15

d   = 40

4

t7
35

t1
5

t1
5

t3
65

t5
5

t6
20
d   = 120

6

initial speed

t1

t2
15

t2
15
t1

t4
15

t4
15

t3
65

t3
65

t6
 20

t6
 20
t2

t2

t4

t3

t6

t5

t7
35

t7
35

t3

t5
5

t5
5

t7

(b)

(c)

(d)

0

5 20 35

100

120

200

time

tsink

d     = 200

sink

Fig. 3.
groups/supertasks; (d) Scheduled tasks.

(a) Tasks with local deadlines; (b) EDF order; (c) Dividing into task

problem for independent tasks and use the well-known energy-
optimal scheduling algorithm by Yao et al. [10].

In their task model, each task is speciﬁed by the triple of the
arrival time, deadline, and the required number of CPU cycles
to complete. The algorithm identiﬁes a ‘critical’ interval where
a set of tasks should be completed within that interval and no
other tasks should be executed in the interval to minimize
the energy consumption, and then schedules those ‘critical’
tasks at a maximum constant speed by the earliest deadline
policy. The process is repeatedly applied to the subproblem
for the remaining unscheduled tasks until there remains no
unscheduled task. As a result we have a feasible schedule of
which energy consumption:

Einter =

N(cid:1)

(cid:14)
K ·

(cid:5)

(cid:4) ri

ti

i=1

(cid:15)

2 · ri

(11)

is minimized.
Transforming the original problem: Each task group {τ1, . . .
, τi}, {τi+1, . . . , τj}, and {τj+1, . . . , τN} in the previous sec-
tion is now considered as a single task, which we call supertask
and denote ˆτ1, ˆτ2, and ˆτ3. Note that we use the hat symbol(ˆ)
to differentiate supertasks from ordinary tasks in Yao et al.’s
model. The arrival time ˆai of each supertask is set to 0 and
the deadline ˆdi is inherited from the original task group. One
difference is that the required number of CPU cycles ˆri for
each supertask is not set to the sum of consisting tasks’ ri but
to the sum of ∆i, more speciﬁcally they are set as follows:

ˆr1 = ∆1 + ··· + ∆i
ˆr2 = ∆i+1 + ··· + ∆j
ˆr3 = ∆j+1 + ··· + ∆N

The resulting schedule after applying those supertasks to Yao
et al.’s algorithm directly determines the starting and ending
times of each supertask. We then continue the same procedure
of the previous section.

The detailed description of the algorithm is shown in Fig. 4.
The procedure Construct Supertask(1, T ) constructs the set of
supertasks recursively from the given task set T . The function
h(i) returns the index of the ith task that has local deadline in
the EDF order and the pred(τi) returns the set of all immediate
predecessor tasks of the task τi.

452

Fig. 3 shows an example. The original task set T is shown
in Fig. 3(a). Fig. 3(b) represents the sequence obtained by the
EDF ordering. Note that there are two tasks τ4 and τ6 that
have local deadlines. (Therefore h(1) = 4 and h(2) = 6). We
partition the task set into three task groups or equivalently
three supertasks ˆτ1 = {τ1, τ2, τ4}, ˆτ2 = {τ3, τ6}, and ˆτ3 =
{τ5, τ7} as shown in Fig. 3(c). The arrival time, deadline, and
the required number of cycles are set as follows:

ˆa1 = ˆa2 = ˆa3 = 0
ˆd1 = dh(1) = 40
ˆd2 = dh(2) = 120
ˆd3 = dh(3) = 200
ˆr1 = ∆1 + ∆2 + ∆4 = 35
ˆr2 = ∆3 + ∆6 = 85
ˆr3 = ∆5 + ∆7 = 40

Fig. 3(d) shows the resultant schedule of the supertasks using
Yao et al.’s algorithm. Finally, each individual task in the
supertask receives the execution time according to Lemma 1,
which is depicted also in Fig. 3(d).
Theorem 2: DVS-intgr produces a feasible and minimum
energy schedule when followed by the optimal
intra-task
voltage scaling.
Proof Sketch. Since every suppertask has zero arrival time, for
any critical interval (with supertasks), each supertask has all its
preceding (unscheduled) supertasks in the same interval. Thus
the scheduling for the supertask set does not alter the EDF
order, which leads to a feasible schedule. (See [10] for more
details.) The obtained supertask schedule has the minimum
value of the following energy equation:

ˆN(cid:1)

(cid:14)
K ·

i=1

(cid:5)

(cid:4)(cid:10)
∆j
ˆti

2 ·

(cid:15)

∆j

(cid:1)
∀τj∈ˆτi

(12)

Esupertask =

where ˆN is the number of supertasks. Note that this equation is
essentially identical to Eq. (11). It can be proved that the inner
term of the summation in Eq. (12) represents the lower bound
of the energy consumption that the consisting tasks can have
within the interval of ˆti. Since each individual task is assigned
the execution time proportional to its energy-optimal path
length, by Lemma 1 the lower bound is achieved. Therefore
the theorem holds.

IV. EXPERIMENTAL RESULTS

We implemented the proposed integrated DVS technique,
called DVS-intgr,
in C++ and tested it on the task sets
generated by TGFF v3.0 [2]. We used the example input ﬁles
(*.tgffopt) provided with the package to obtain the task sets.
Each individual task was generated by inserting a left/right
child and a grandchild into an arbitrarily chosen basic block
repeatedly until the number of conditional branches reaches
some ﬁxed number in the range of [1, 100]. The probability
at each branch is drawn from a random normal distribution
with standard deviation of 1.0 and mean of 0.5. The number of
execution cycles (i.e., length) of each basic block is restricted

DVS-intgr {
• Input : Set T of tasks
• Output : Tasks with starting/ending time speciﬁed

ˆT = Construct Supertask(1, T );
Yao Inter Scheduler( ˆT );
for ∀ˆτi ∈ ˆT {

current time = ˆsi;
while (ˆτi (cid:3)= ∅) {

τj = Retrieve First Task(ˆτi);
sj = current time;
ej = sj + (∆j /ˆri) · ˆti;
current time = ej ;

// schedule each task
in the supertask
//

}}}
Construct Supertask(i, T ) {

if (T = ∅) return ∅;
ˆτi = {τh(i)};
ˆai = 0;
ˆdi = dh(i);
ˆri = ∆h(i);
T = T − {τh(i)};
T 1 = pred(τh(i));
while (∃τj ∈ T 1) {
ˆτi = ˆτi ∪ {τj};
ˆri = ˆri + ∆j ;
T = T − {τj};
T 1 = T 1 − {τj} ∪ pred(τj );

//
// initialize the ith
//
//

supertask

// add a task to the ith
//

supertask

}
return {ˆτi}∪ Construct Supertask(i + 1, T ); }
// h(i) returns index of ith task that has local deadline.
// pred(τi) returns all immediate predecessors of τi

Fig. 4. A summary of the proposed algorithm.

such that the length of the longest basic block does not exceed
100 times of the length of the shortest one. Then the overall
lengths in each task are multiplied by a factor such that the
worst-case execution of the corresponding supertask produces
20% slack time (within its bound) when the maximum speed
is used.

Table I shows a summary of the experimental results. Our
competitors include the inter-task DVS technique by Zhang
[11] (Zhang-only) and the same technique followed by appli-
cation of the optimal intra-task DVS technique (Zhang+Intra).
Zhang et al. solved the inter-task DVS problem for dependent
tasks using integer programming. Although their technique
produces an optimal solution to the inter-task DVS problem,
they did not consider future application of any intra-task
DVS techniques, which inevitably leads to non-optimal energy
consumption when combined with intra-task DVS techniques.
To give the best chance to Zhang-only,
it was assumed
that when the task ﬁnishes earlier than the scheduled time,
the target system becomes idle with the power-down mode
consuming no energy. We repeated the experiment for each
task set 1000 times and then obtained the average reduction
of energy consumption after discarding higher and lower 100
results, respectively. The sixth and seventh columns in the table
represent the average energy consumptions of Zhang+Intra
and DVS-intgr respectively, normalized to the corresponding
energy consumption of Zhang-only. From the sixth column,
one can easily see that
the follow-up application of the
optimal intra-task DVS technique drastically reduces the total
energy consumption. The eighth column shows the average
energy reduction of our technique over Zhang+Intra for each

453

Task Set
(Script)

[2]
bus0
bus1

kbasic task0
kbasic task1
kbasic task2
kbasic task3
kbasic task4
kbasic task5
kbasic task6
kbasic task7
kbasic task8
kbasic task9
kextended0
kextended1
kextended2

kseries parallel0
kseries parallel1
kseries parallel2
kseries parallel3

kseries parallel xover0
kseries parallel xover1
kseries parallel xover2
kseries parallel xover3

simple0
simple1
simple2
simple3
simple4

Avg.

Number

Number

of

Tasks

of

Edges

No. of

Tasks with
Deadline

Normalized Energy Consumption (Avg.)

Avg. Reduction

Zhang-only

Zhang + Intra

DVS-intgr

over

Zhang + Intra

15
15
42
64
42
36
21
21
18
32
23
35
23
21
22
30
20
62
47
30
21
38
27
12
20
24
11
22

30
30
50
79
46
47
22
23
18
36
27
40
25
28
27
33
19
61
46
37
24
41
30
19
25
28
12
32

5
5
17
23
17
12
10
7
10
15
9
15
8
6
6
4
4
9
6
3
4
4
4
1
6
6
3
4

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

0.3840
0.2564
0.2760
0.3590
0.2617
0.3287
0.3259
0.2595
0.2405
0.3609
0.2856
0.3052
0.3527
0.3496
0.3486
0.3070
0.2472
0.3003
0.3163
0.3057
0.2618
0.3458
0.2494
0.3691
0.2832
0.2897
0.2732
0.2713
0.3041

0.3549
0.2321
0.2516
0.3286
0.2321
0.3007
0.2838
0.2345
0.2127
0.3234
0.2567
0.2888
0.3184
0.3148
0.3141
0.2718
0.2175
0.2770
0.2871
0.2603
0.2422
0.3209
0.2128
0.3181
0.2563
0.2556
0.2337
0.2459
0.2731

7.8%
9.4%
9.3%
9.1%
10.3%
8.7%
13.3%
11.6%
12.8%
8.8%
10.2%
6.3%
10.0%
11.7%
9.9%
12.2%
11.4%
8.0%
10.2%
15.1%
8.2%
7.2%
15.3%
13.9%
9.7%
13.9%
14.2%
9.2%
10.6%

A COMPARISON OF ENERGY CONSUMPTIONS FOR THE TASK SETS IN [2].

TABLE I

task set. (Note that this value does not come directly from
the corresponding energy values in the sixth and seventh
column since it is the average of reduction percentages.) In
comparison, DVS-intgr reduces energy consumption by 72.7%
and 10.6% on average compared to that of Zhang-only and
Zhang + Intra, respectively.

V. CONCLUSIONS

We have presented a novel DVS technique to deal with the
combined inter- and intra-task DVS problem. To solve this
problem, ﬁrst we examined the lower bound of the energy
consumption that any intra-task DVS technique can achieve
and then using this property we devised a method setting
energy-optimal execution time to each task. To obtain the
globally optimal task schedule we divided the task set into
several task groups such that each task in the group can be
scheduled optimally within the group boundary and then trans-
formed the problem of determining the group boundary into an
inter-task DVS problem for independent tasks, which is then
solved using the existing optimal inter-task DVS technique.
The experimental results showed that our proposed integrated
DVS technique was able to reduce the energy consumption by
10.6% over that of the combination of the existing techniques
of [11] followed by [7].

REFERENCES

[1] A. Andrei et al., “Overhead-conscious voltage selection for
dynamic and leakage energy reduction of time-constrained sys-
tems,” Prof. of Design Automation and Test in Europe, 2004.

[2] R. P. Dick, D. L. Rhodes, and W. Wolf, “Tgff: Task graphs
for free,” Proc. of Sixth International Workshop on Hard-
ware/Software Codesign, Mar. 1998.

[3] B. Gorji-Ara et al., “Fast and efﬁcient voltage scheduling by
evolutionary slack distribution,” Proc. of Asia-South Paciﬁc
Design Automation Conference, 2004.

[4] W.-C. Kwon and T. Kim, “Optimal voltage allocation techniques
for dynamically variable voltage processors,” Proc. of Design
Automation Conference, 2003.

[5] C. L. Liu and J. W. Layland, “Scheduling algorithms for
multiprogramming in a hard real-time environment,” Journal of
the ACM, Vol. 20, Jan. 1973.

[6] M. T. Schmitz, B. M. Al-Hashimi, and P. Eles, “Energy-efﬁcient
mapping and scheduling for DVS enabled distributed embedded
systems,” Prof. of Design Automation and Test in Europe , 2002.
[7] J. Seo, T. Kim, and K. Chung, “Proﬁle-based optimal intra-
task voltage scheduling for hard real-time applications,” Proc.
of Design Automation Conference, 2004.

[8] D. Shin, J. Kim, and S. Lee, “Intra-task voltage scheduling for
low-energy hard real-time applications, IEEE Design and Test
of Computers, Vol. 18, 2001.

[9] G. Varatkar and R. Marculescu, “Communication-aware task
scheduling and voltage selection for total systems energy mini-
mization,” Proc. of International Conference on Computer-Aided
Design, 2003.

[10] F. Yao, A. Demers, and S. Shenker, “A scheduling model for
reduced CPU energy,” Proc. of IEEE Symposium on Foundations
of Computer Science, 1995.

[11] Y. Zhang, X. Hu, and D. Z. Chen, “Task scheduling and voltage
selection for energy minimization,” Proc. of Design Automation
Conference, 2002.

[12] http://www.transmeta.com

454

Optimal Integration of Inter-Task and Intra-Task
Dynamic Voltage Scaling Techniques for Hard

Real-Time Applications

Jaewon Seo

KAIST, Daejeon, KOREA
jwseo@jupiter.kaist.ac.kr

Taewhan Kim

Nikil D. Dutt

Seoul National University, Seoul, KOREA

University of California, Irvine, CA

tkim@ssl.snu.ac.kr

dutt@ics.uci.edu

Abstract— It is generally accepted that the dynamic voltage
scaling (DVS) is one of the most effective techniques for energy
minimization. According to the granularity of units to which volt-
age scaling is applied, the DVS problem can be divided into two
subproblems: (i) inter-task DVS problem and (ii) intra-task DVS
problem. A lot of effective DVS techniques have addressed either
one of the two subproblems, but none of them have attempted
to solve both simultaneously, which is mainly due to an excessive
computation complexity to solve it optimally. This work addresses
this core issue, that is, Can the combined problem be solved
effectively and efﬁciently? More specﬁcally, our work shows, for
a set of inter-dependednt tasks, that the combined DVS problem
can be solved optimally in polynomial time. Experimental results
indicate that the proposed integrated DVS technique is able to
reduce energy consumption by 10.6% on average over the results
by [11]+ [7] (i.e., a straightforward combination of two optimal
inter- and intra-task DVS techniques.)

I. INTRODUCTION

Over the past decades there have been enormous efforts to
minimize the energy consumption of CMOS circuit systems.
Dynamic voltage scaling (DVS) – involving dynamic adjust-
ments of the supply voltage and the corresponding operating
clock frequency – has emerged as one of the most effective
energy minimization techniques. A one-to-one correspondence
between the supply voltage and the clock frequency in CMOS
circuits imposes an inherent constraint to DVS techniques
to ensure that voltage adjustments do not violate the target
system’s timing constraints.

Many previous works have focused on hard real-time sys-
tems with multiple tasks. Their primary concern is to assign
a proper operating voltage to each task while satisfying the
task’s timing constraint. In these techniques, determination
of the voltage is carried out on a task-by-task basis and the
voltage assigned to the task is unchanged during the whole ex-
ecution of the task (Inter-Task DVS). Yao et al. [10] proposed
an optimal inter-task voltage scaling algorithm for independent
tasks, which determines the execution speed of each task at any
given time so that the total energy consumption is minimized.
Although their formulation does not impose any constraint on
the number of operating voltages assigned to each task, by
convexity of the power function, every task is given only one
voltage and executed at a constant speed. This result comes
from the underlying assumption that the required number of
cycles to ﬁnish the task is constant, which is unacceptable

in practice. With the same assumption, many works in the
literature have tried to formulate their own inter-task DVS
problems considering other issues such as dependent task sets
[1], [3], [6], [9], [11], discretely variable voltage processors
[3], [4], multi-processor environments [1], [3], [6], [9], [11],
voltage transition overheads [1], etc.

In the past few years, several studies (e.g., [7], [8]) added a
new dimension to the voltage scaling problem, by considering
energy saving opportunities within the task boundary. In their
approach, the operating voltage of the task is dynamically
adjusted according to the execution behavior (Intra-Task
DVS). Shin et al. [8] proposed the remaining worst-case
path-based algorithm which achieves the best granularity by
executing each basic block with possibly different operating
voltage. To obtain tight operating points for minimum energy
consumption, the algorithm updates the remaining path length
as soon as the execution deviates from the previous remain-
ing worst-case path. More recently, a proﬁle-based optimal
intra-task voltage scaling technique was presented in [7]. It
shows the best energy reduction by incorporating the task’s
execution proﬁle into the calculation of the operating voltages.
The algorithm is proved to be optimal in the sense that it
achieves minimum average energy consumption when the task
is executed repeatedly.

To the best of our knowledge, no work has attempted to
solve the inter-task and intra-task DVS problems simultane-
ously so as not to miss the energy saving opportunities at
both granularities. In this paper, we present the ﬁrst work that
solves this combined inter- and intra-task DVS problem to
fully exploit the advantages of both techniques. We propose
an optimal integrated approach based on two energy-optimal
inter-task [10] and intra-task [7] DVS algorithms.

II. DEFINITIONS AND PROBLEM FORMULATION

We consider that an application consists of N communicating
tasks, T = {τ1, τ2, . . . , τN}, of which relations are represented
by a directed acyclic graph (DAG). Each node in the graph
represents task and each arc between two nodes indicates
control/data dependency for two tasks. Fig. 3(a) shows an
example of this graph. If two tasks τi and τj are connected
by arc (τi, τj), the execution of τi must be completed before
the execution of τj. A deadline di might be assigned to task

0-7803-9254-X/05/$20.00 ©2005 IEEE.

449

τi in order to ensure correct functionality (e.g., τ4 and τ6 in
Fig. 3(a)), which we call local deadline. In addition, to restrict
all tasks’ ending time we add a special node called sink to the
graph. Every task of which execution is not constrained by
any of the deadlines is connected to the sink. (e.g., τ5 and
τ7 in Fig. 3(a)) Those tasks should be executed before the
deadline of the sink, which we call global deadline. More
detailed explanation on this task model can be found in the
recent inter-task DVS works e.g., in [3], [6], [11].
Each task τi ∈ T is associated with the following parame-

ters:

• ri : the required number of CPU cycles to complete,
• si : the starting time,
• ei : the ending time (ei ≥ si), and
• ti : the execution time (ti = ei − si)

Note that ri is given for each task τi, while the values of si,
ei and ti are determined after the task is actually scheduled.
The amount of energy consumption during the execution of

the task τi is expressed as:

Ei = Cef f · Vi

2 · ri

(1)

where Cef f and Vi denote the effective charged capacitance
and the voltage supplied to the task, respectively. The rela-
tionship between clock frequency fclk and supply voltage Vdd
in CMOS circuits is [4]:

fclk ∝ 1/Td = µCox(W/L)(Vdd − Vth)2

≈ Vdd

(2)

CLVdd

where Td is the delay, CL is the total node capacitance,
µ is the mobility, Cox is the oxide capacitance, Vth is the
threshold voltage and W/L is the width to length ratio of
transistors. Based on the above one-to-one correspondence,
we use voltage and corresponding speed (i.e., clock frequency)
interchangeably throughout the paper and rewrite Eq.(1) as:

Ei = K · fi

2 · ri

(3)

where K is the system-dependent constant and fi
is the
operational speed of the task τi. Although this equation seems
somewhat oversimpliﬁed, our method is generally applicable
if the energy consumption is represented as a power function
of the speed i.e., Ei = af b
i . For the most of the commercial
DVS processors it gives a good approximation of the actual
energy consumption with an admissible error. (e.g., less than
3.4% error for Transmeta TM5900 1000MHz [12])

Since in most cases a task shows different behaviors depend-
ing on input data, i.e. ri is not given as a constant, we cannot
directly apply the simple energy equation, Eq.(3). One way to
deal with the different execution paths is to consider average
(i.e., expected) energy consumption, which can be expressed
as:

(cid:1)

(cid:2)

(cid:3)
P rob(π) · E(π)

Ei =

(4)

∀exec. path π of τi

where P rob(π) is the probability that the execution of task τi
follows path π and E(π) is the energy consumption on that
path. The fact that many tasks in real-time environments are

N(cid:1)

executed repeatedly provides the rationale for the use of this
model. The overall energy consumption for the entire task set
T is given as:

Etot =

Ei

(5)

i=1

We deﬁne a feasible schedule of tasks to be a schedule in
which all the timing constraints of the tasks are satisﬁed. Note
that the inter-task DVS problem is to ﬁnd a (static) schedule
of each task and a voltage applied to the task to minimize the
amount of energy consumption. On the other hand, the intra-
task DVS problem is to ﬁnd a (dynamic) voltage-adjustment
scheme for each basic block in the task to minimize the amount
of (average) energy consumption while satisﬁng the deadline
constraint of the task. Then, the combined problem of the inter-
and intra-task DVS problems can be described as:

Problem 1 (The Combined DVS Problem) Given an instance
of tasks, ﬁnd an inter-task schedule and an intra-task voltage
scaling scheme that produces a feasible schedule for every
possible execution path of tasks and minimizes the quantity of
Etot in Eq.(5).

III. THE COMBINED DVS TECHNIQUE

The proposed integrated DVS approach is a two-step method:
1) Statically determine energy-optimal starting and ending
times (si and ei) for each task τi considering future use
of an intra-task voltage scaling.

2) Execute τi within [si, ei] while varying the processor
speed according to the voltage scales obtained by an
existing optimal intra-task DVS scheme.

Our key concern is to develop a new inter-task scheduling
algorithm that ﬁnds starting and ending times (si and ei) for
each task, which leads to a minimum value of Etot in Eq.(5)
when an optimal intra-task scheme is applied to the tasks.
(Note that the new inter-task scheduling algorithm is different
from any existing inter-task DVS scheme, which is not aware
of intra-task DVS at all.)

In the following, let us ﬁrst introduce an optimal intra-task
DVS technique [7] since it provides a basis on the development
of our combined enegy-optimal DVS technique.

A. An optimal intra-task DVS technique
A task τ is represented with its CFG (Control Flow Graph)
Gτ = (V, A), where V is the set of basic blocks in the task
and A is the set of directed edges which impose precedence
relations between basic blocks. (For example, see Fig. 1(a).)
The set of immediate successor basic blocks of any bi ∈ V is
denoted by succ(bi). Each basic block bi is annotated with its
non-zero number of execution cycles ni and each arc (bi, bj)
is given a probability pj that the execution follows the arc.

Following theorem states the lower bound energy consump-

tion that any intra-task DVS technique can achieve.
Theorem 1: Given a task’s CFG and its execution proﬁle that
offers the probabilities, the minimum energy consumption of

450

1p  = 0.7

1b
3

0b
6

2p  = 0.3

2b
4

3p  = 0.9

3b
2

4p  = 0.1

4b
7

5b
1

6p  = 0.2

6b
8

7p  = 0.8

7b
14

0b
6

d1 =3+19.1

=22.1

1b
3

3

d0 =6+  0.7*22.1  + 0.3*25.7

3

3

=29.3

2b
4

3

3
d2 =4+  0.9*21.1  + 0.1*26.1

3

=25.7

d3 =2+19.1

=21.1

3b
2

4b
7

d4 =7+19.1

=26.1

5b
1

3

3
d5 =1+  0.2*13 + 0.8*19

3

=19.1

d6 =8+5=13

6b
8

7b
14
d7 =14+5=19

8b
5

(a)

d8 =5

8b
5

(b)

Fig. 1.

(a) CFG of a task τsimple; (b) Calculation of δ values.

any intra-task DVS technique is bounded by:

δ0

(relative) deadline

(cid:5)

2 · δ0

(cid:4)

Eintra = K ·

ni,

ni + 3

(cid:9) (cid:10)
∀bj∈succ(bi)

δi =

where δi is deﬁned as:

if succ(bi) = ∅

3, otherwise

pj · δj

(6)

(7)

and δ0 is the δ value of the top basic block b0. (See [7] for
proof.)

One interesting interpretation of Eq.(6) is that it can be
considered as the energy consumed in the execution of δ0
cycles at the speed of δ0/deadline. We call δ0 energy-optimal
path length of the task.

The above theorem leads to the following optimal voltage
scaling scheme [7]: Before executing the task, we compute
the δi value of each basic block bi and insert the following
instruction code at the beginning of bi

change_f_V(δi/remaining_time());

where the instruction change_f_V(fclk) changes the cur-
rent clock frequency (i.e., speed) to fclk and adjusts the supply
voltage accordingly and remaining_time() returns the
remaining time to deadline.

For example, consider a real-time task τsimple shown in
Fig. 1(a). The number within each node indicates its number
of execution cycles ni. Fig. 1(b) shows the procedure of
calculating each δi value according to Eq.(7). Once we have
ﬁnished the calculation, the operating speed of basic block
bi is simply obtained by dividing δi by the remaining time
to deadline. Suppose that deadline = 10 (unit time) and the
execution follows the path (b0, b2, b3, b5, b6, b8). Then,
the
corresponding speed changes as follows:

Speed : 2.93 > 3.24 > 3.14 > 3.14 > 2.26 > 2.26

In Fig. 1(b), the thick, dotted, and regular arrows indicate
the increase, decrease and no change of the processor speed,
respectively and the basic blocks with changed operating speed

t1
10

t2
5

t4
75

t3
40

t5
20

tsink

d     = 300

sink

(a)

t1
10

t2
5

t3
40

t4
75

t5
20

(b)
initial speed (=    / t  ) 

D i

i

t2

t1

t3

t4

t5

0

20 25

110

260

300

time

(c)

Fig. 2.
dependencies; (c) Scheduled tasks.

(a) A simple task set Tsimple; (b) A task order preserving

are marked with gray-color. More details including practical
issues (e.g. handling loop constructs, voltage transition over-
heads, discretely variable voltages, etc.) can be found in [7].

the

B. An intra-task DVS-aware inter-task DVS technique
One characteristic of the previous intra-task DVS technique is
that after an execution time (or relative deadline) is assigned,
the scheduler ﬁnishes the task exactly at the deadline no matter
which execution path is taken, producing no slack time. Thus,
the next step is to determine the starting and ending times of
each task. We ﬁrst give a characterization of an energy-optimal
schedule for any set of tasks that have no local deadlines.
Case 1 - Tasks with a global deadline only: For
case
where each task has no local deadline and of which ending
time is restricted only by the global deadline dsink (For
example, see Fig. 2(a)), the problem can be solved easily by
properly distributing the permitted execution time (= dsink)
over the tasks and then determining the order of the tasks.
Following lemma states the optimal distribution of dsink which
leads to the minimum energy consumption when the previous
optimal intra-task scheme is applied.
Lemma 1: Given a task set T = {τ1, τ2, . . . , τN} with no
local deadlines and the global deadline dsink the total energy
consumption Etot is minimized when each task is assigned
the execution time proportional to its energy-optimal execution
path length i.e., when ti is given as:
∆1 + ··· + ∆N

· dsink
where ∆i denotes the δ0 of Eq. (7) for τi.
(cid:5)

energy consumption is expressed as:
2 · ∆1 + K ·

(cid:4)∆2

(cid:4)∆1

Proof: For N = 2, if we set t1 = x and t2 = y, the total

(9)
where x + y = dsink. By substituting dsink − x for y and
Eq. (9) has the minimum K·(cid:11)
then differentiating with respect to x, it can be veriﬁed that
2·(∆1+∆2) where x =
∆1+∆2 . For N = m, assume the lemma holds for m−1
dsink· ∆1
tasks. Then the energy equation becomes essentially identical

2 · ∆2

∆1+∆2
dsink

K ·

ti =

∆i

(cid:5)

(8)

(cid:12)

x

y

451

to Eq. (9). (∆2 is replaced by ∆2 + ··· + ∆m.) Therefore the
lemma holds.

Now we know how much time should be given to each
task for execution. Thus the next step is to determine the
starting time (and automatically ending time) for each task,
which is obtained directly from a task order. Since there is no
local deadline for each task, any order that preserves all the
dependencies between the tasks sufﬁces the minimum energy
schedule. Without
loss of generality assume the sequence
(τ1, τ2, . . . , τN ) does not violate the precedence relations.
Then the starting and ending time of each task are determined
simply as:

0,
ei−1,

for i = 0
for i ≥ 1

si =

ei = si + ti

and

(10)
For example, suppose a simple task set Tsimple = {τ1, τ2, τ3,
τ4, τ5} of which DAG is shown in Fig. 2(a). Each task has no
local deadline and there is only one global deadline dsink =
300 unit time speciﬁed by the sink. The number inside each
node indicates the δ0 value of the corresponding task. Fig. 2(b)
shows an order that preserves the dependency relations, from
which we calculate the time stamps for each task as shown in
Fig. 2(c) using Eq. (8) and Eq. (10)

(cid:13)

Case 2 - Tasks with arbitrary deadlines: For the general case
where tasks possibly have local deadlines as well as the
global deadline, we divide the problem into a collection of
subproblems of Case 1 and then apply the result of Case 1.
Firstly, we order the tasks with the Earliest Deadline First
(EDF) policy while preserving the dependency relations1. The
EDF policy is proved to give the best opportunity for energy
savings [11]. Let (τ1, τ2, . . . , τN ) be the obtained sequence
and for the sake of illustration assume only two tasks τi and
τj (i < j) have local deadlines. Then we partition the task
set into three task groups {τ1, . . . , τi}, {τi+1, . . . , τj}, and
{τj+1, . . . , τN} and apply the method in Case 1 to each group
separately, where we set the starting times of τ1, τi+1, and
τj+1 to 0, di, and dj, respectively.

For example, consider a task set shown in Fig. 3(a). We can
order the tasks by the EDF policy as shown in Fig. 3(b). Since
only two tasks τ4 and τ6 have local deadlines, the task set is
divided into three groups {τ1, τ2, τ4}, {τ3, τ6}, and {τ5, τ7} as
shown in Fig. 3(c), and we set the starting time of each group
0, 40(= d4) and 120(= d6), respectively. Then the permitted
execution times 40, 80(= 120− 40), and 80(= 200− 120) are
distributed proportionally to the tasks in each group according
to Lemma 1.

C. An improved inter-task DVS technique
It should be noted that the previous method of assigning a
starting time to each task group does not guarantee to produce
the minimum energy schedule. To overcome this limitation, we
transform the original problem into an inter-task scheduling

1Every task of which deadline is not speciﬁed inherits the deadline of its

earliest successor task.

(a)

t1
5

t2
15

t4
15

d   = 40

4

t7
35

t1
5

t1
5

t3
65

t5
5

t6
20
d   = 120

6

initial speed

t1

t2
15

t2
15
t1

t4
15

t4
15

t3
65

t3
65

t6
 20

t6
 20
t2

t2

t4

t3

t6

t5

t7
35

t7
35

t3

t5
5

t5
5

t7

(b)

(c)

(d)

0

5 20 35

100

120

200

time

tsink

d     = 200

sink

Fig. 3.
groups/supertasks; (d) Scheduled tasks.

(a) Tasks with local deadlines; (b) EDF order; (c) Dividing into task

problem for independent tasks and use the well-known energy-
optimal scheduling algorithm by Yao et al. [10].

In their task model, each task is speciﬁed by the triple of the
arrival time, deadline, and the required number of CPU cycles
to complete. The algorithm identiﬁes a ‘critical’ interval where
a set of tasks should be completed within that interval and no
other tasks should be executed in the interval to minimize
the energy consumption, and then schedules those ‘critical’
tasks at a maximum constant speed by the earliest deadline
policy. The process is repeatedly applied to the subproblem
for the remaining unscheduled tasks until there remains no
unscheduled task. As a result we have a feasible schedule of
which energy consumption:

Einter =

N(cid:1)

(cid:14)
K ·

(cid:5)

(cid:4) ri

ti

i=1

(cid:15)

2 · ri

(11)

is minimized.
Transforming the original problem: Each task group {τ1, . . .
, τi}, {τi+1, . . . , τj}, and {τj+1, . . . , τN} in the previous sec-
tion is now considered as a single task, which we call supertask
and denote ˆτ1, ˆτ2, and ˆτ3. Note that we use the hat symbol(ˆ)
to differentiate supertasks from ordinary tasks in Yao et al.’s
model. The arrival time ˆai of each supertask is set to 0 and
the deadline ˆdi is inherited from the original task group. One
difference is that the required number of CPU cycles ˆri for
each supertask is not set to the sum of consisting tasks’ ri but
to the sum of ∆i, more speciﬁcally they are set as follows:

ˆr1 = ∆1 + ··· + ∆i
ˆr2 = ∆i+1 + ··· + ∆j
ˆr3 = ∆j+1 + ··· + ∆N

The resulting schedule after applying those supertasks to Yao
et al.’s algorithm directly determines the starting and ending
times of each supertask. We then continue the same procedure
of the previous section.

The detailed description of the algorithm is shown in Fig. 4.
The procedure Construct Supertask(1, T ) constructs the set of
supertasks recursively from the given task set T . The function
h(i) returns the index of the ith task that has local deadline in
the EDF order and the pred(τi) returns the set of all immediate
predecessor tasks of the task τi.

452

Fig. 3 shows an example. The original task set T is shown
in Fig. 3(a). Fig. 3(b) represents the sequence obtained by the
EDF ordering. Note that there are two tasks τ4 and τ6 that
have local deadlines. (Therefore h(1) = 4 and h(2) = 6). We
partition the task set into three task groups or equivalently
three supertasks ˆτ1 = {τ1, τ2, τ4}, ˆτ2 = {τ3, τ6}, and ˆτ3 =
{τ5, τ7} as shown in Fig. 3(c). The arrival time, deadline, and
the required number of cycles are set as follows:

ˆa1 = ˆa2 = ˆa3 = 0
ˆd1 = dh(1) = 40
ˆd2 = dh(2) = 120
ˆd3 = dh(3) = 200
ˆr1 = ∆1 + ∆2 + ∆4 = 35
ˆr2 = ∆3 + ∆6 = 85
ˆr3 = ∆5 + ∆7 = 40

Fig. 3(d) shows the resultant schedule of the supertasks using
Yao et al.’s algorithm. Finally, each individual task in the
supertask receives the execution time according to Lemma 1,
which is depicted also in Fig. 3(d).
Theorem 2: DVS-intgr produces a feasible and minimum
energy schedule when followed by the optimal
intra-task
voltage scaling.
Proof Sketch. Since every suppertask has zero arrival time, for
any critical interval (with supertasks), each supertask has all its
preceding (unscheduled) supertasks in the same interval. Thus
the scheduling for the supertask set does not alter the EDF
order, which leads to a feasible schedule. (See [10] for more
details.) The obtained supertask schedule has the minimum
value of the following energy equation:

ˆN(cid:1)

(cid:14)
K ·

i=1

(cid:5)

(cid:4)(cid:10)
∆j
ˆti

2 ·

(cid:15)

∆j

(cid:1)
∀τj∈ˆτi

(12)

Esupertask =

where ˆN is the number of supertasks. Note that this equation is
essentially identical to Eq. (11). It can be proved that the inner
term of the summation in Eq. (12) represents the lower bound
of the energy consumption that the consisting tasks can have
within the interval of ˆti. Since each individual task is assigned
the execution time proportional to its energy-optimal path
length, by Lemma 1 the lower bound is achieved. Therefore
the theorem holds.

IV. EXPERIMENTAL RESULTS

We implemented the proposed integrated DVS technique,
called DVS-intgr,
in C++ and tested it on the task sets
generated by TGFF v3.0 [2]. We used the example input ﬁles
(*.tgffopt) provided with the package to obtain the task sets.
Each individual task was generated by inserting a left/right
child and a grandchild into an arbitrarily chosen basic block
repeatedly until the number of conditional branches reaches
some ﬁxed number in the range of [1, 100]. The probability
at each branch is drawn from a random normal distribution
with standard deviation of 1.0 and mean of 0.5. The number of
execution cycles (i.e., length) of each basic block is restricted

DVS-intgr {
• Input : Set T of tasks
• Output : Tasks with starting/ending time speciﬁed

ˆT = Construct Supertask(1, T );
Yao Inter Scheduler( ˆT );
for ∀ˆτi ∈ ˆT {

current time = ˆsi;
while (ˆτi (cid:3)= ∅) {

τj = Retrieve First Task(ˆτi);
sj = current time;
ej = sj + (∆j /ˆri) · ˆti;
current time = ej ;

// schedule each task
in the supertask
//

}}}
Construct Supertask(i, T ) {

if (T = ∅) return ∅;
ˆτi = {τh(i)};
ˆai = 0;
ˆdi = dh(i);
ˆri = ∆h(i);
T = T − {τh(i)};
T 1 = pred(τh(i));
while (∃τj ∈ T 1) {
ˆτi = ˆτi ∪ {τj};
ˆri = ˆri + ∆j ;
T = T − {τj};
T 1 = T 1 − {τj} ∪ pred(τj );

//
// initialize the ith
//
//

supertask

// add a task to the ith
//

supertask

}
return {ˆτi}∪ Construct Supertask(i + 1, T ); }
// h(i) returns index of ith task that has local deadline.
// pred(τi) returns all immediate predecessors of τi

Fig. 4. A summary of the proposed algorithm.

such that the length of the longest basic block does not exceed
100 times of the length of the shortest one. Then the overall
lengths in each task are multiplied by a factor such that the
worst-case execution of the corresponding supertask produces
20% slack time (within its bound) when the maximum speed
is used.

Table I shows a summary of the experimental results. Our
competitors include the inter-task DVS technique by Zhang
[11] (Zhang-only) and the same technique followed by appli-
cation of the optimal intra-task DVS technique (Zhang+Intra).
Zhang et al. solved the inter-task DVS problem for dependent
tasks using integer programming. Although their technique
produces an optimal solution to the inter-task DVS problem,
they did not consider future application of any intra-task
DVS techniques, which inevitably leads to non-optimal energy
consumption when combined with intra-task DVS techniques.
To give the best chance to Zhang-only,
it was assumed
that when the task ﬁnishes earlier than the scheduled time,
the target system becomes idle with the power-down mode
consuming no energy. We repeated the experiment for each
task set 1000 times and then obtained the average reduction
of energy consumption after discarding higher and lower 100
results, respectively. The sixth and seventh columns in the table
represent the average energy consumptions of Zhang+Intra
and DVS-intgr respectively, normalized to the corresponding
energy consumption of Zhang-only. From the sixth column,
one can easily see that
the follow-up application of the
optimal intra-task DVS technique drastically reduces the total
energy consumption. The eighth column shows the average
energy reduction of our technique over Zhang+Intra for each

453

Task Set
(Script)

[2]
bus0
bus1

kbasic task0
kbasic task1
kbasic task2
kbasic task3
kbasic task4
kbasic task5
kbasic task6
kbasic task7
kbasic task8
kbasic task9
kextended0
kextended1
kextended2

kseries parallel0
kseries parallel1
kseries parallel2
kseries parallel3

kseries parallel xover0
kseries parallel xover1
kseries parallel xover2
kseries parallel xover3

simple0
simple1
simple2
simple3
simple4

Avg.

Number

Number

of

Tasks

of

Edges

No. of

Tasks with
Deadline

Normalized Energy Consumption (Avg.)

Avg. Reduction

Zhang-only

Zhang + Intra

DVS-intgr

over

Zhang + Intra

15
15
42
64
42
36
21
21
18
32
23
35
23
21
22
30
20
62
47
30
21
38
27
12
20
24
11
22

30
30
50
79
46
47
22
23
18
36
27
40
25
28
27
33
19
61
46
37
24
41
30
19
25
28
12
32

5
5
17
23
17
12
10
7
10
15
9
15
8
6
6
4
4
9
6
3
4
4
4
1
6
6
3
4

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

0.3840
0.2564
0.2760
0.3590
0.2617
0.3287
0.3259
0.2595
0.2405
0.3609
0.2856
0.3052
0.3527
0.3496
0.3486
0.3070
0.2472
0.3003
0.3163
0.3057
0.2618
0.3458
0.2494
0.3691
0.2832
0.2897
0.2732
0.2713
0.3041

0.3549
0.2321
0.2516
0.3286
0.2321
0.3007
0.2838
0.2345
0.2127
0.3234
0.2567
0.2888
0.3184
0.3148
0.3141
0.2718
0.2175
0.2770
0.2871
0.2603
0.2422
0.3209
0.2128
0.3181
0.2563
0.2556
0.2337
0.2459
0.2731

7.8%
9.4%
9.3%
9.1%
10.3%
8.7%
13.3%
11.6%
12.8%
8.8%
10.2%
6.3%
10.0%
11.7%
9.9%
12.2%
11.4%
8.0%
10.2%
15.1%
8.2%
7.2%
15.3%
13.9%
9.7%
13.9%
14.2%
9.2%
10.6%

A COMPARISON OF ENERGY CONSUMPTIONS FOR THE TASK SETS IN [2].

TABLE I

task set. (Note that this value does not come directly from
the corresponding energy values in the sixth and seventh
column since it is the average of reduction percentages.) In
comparison, DVS-intgr reduces energy consumption by 72.7%
and 10.6% on average compared to that of Zhang-only and
Zhang + Intra, respectively.

V. CONCLUSIONS

We have presented a novel DVS technique to deal with the
combined inter- and intra-task DVS problem. To solve this
problem, ﬁrst we examined the lower bound of the energy
consumption that any intra-task DVS technique can achieve
and then using this property we devised a method setting
energy-optimal execution time to each task. To obtain the
globally optimal task schedule we divided the task set into
several task groups such that each task in the group can be
scheduled optimally within the group boundary and then trans-
formed the problem of determining the group boundary into an
inter-task DVS problem for independent tasks, which is then
solved using the existing optimal inter-task DVS technique.
The experimental results showed that our proposed integrated
DVS technique was able to reduce the energy consumption by
10.6% over that of the combination of the existing techniques
of [11] followed by [7].

REFERENCES

[1] A. Andrei et al., “Overhead-conscious voltage selection for
dynamic and leakage energy reduction of time-constrained sys-
tems,” Prof. of Design Automation and Test in Europe, 2004.

[2] R. P. Dick, D. L. Rhodes, and W. Wolf, “Tgff: Task graphs
for free,” Proc. of Sixth International Workshop on Hard-
ware/Software Codesign, Mar. 1998.

[3] B. Gorji-Ara et al., “Fast and efﬁcient voltage scheduling by
evolutionary slack distribution,” Proc. of Asia-South Paciﬁc
Design Automation Conference, 2004.

[4] W.-C. Kwon and T. Kim, “Optimal voltage allocation techniques
for dynamically variable voltage processors,” Proc. of Design
Automation Conference, 2003.

[5] C. L. Liu and J. W. Layland, “Scheduling algorithms for
multiprogramming in a hard real-time environment,” Journal of
the ACM, Vol. 20, Jan. 1973.

[6] M. T. Schmitz, B. M. Al-Hashimi, and P. Eles, “Energy-efﬁcient
mapping and scheduling for DVS enabled distributed embedded
systems,” Prof. of Design Automation and Test in Europe , 2002.
[7] J. Seo, T. Kim, and K. Chung, “Proﬁle-based optimal intra-
task voltage scheduling for hard real-time applications,” Proc.
of Design Automation Conference, 2004.

[8] D. Shin, J. Kim, and S. Lee, “Intra-task voltage scheduling for
low-energy hard real-time applications, IEEE Design and Test
of Computers, Vol. 18, 2001.

[9] G. Varatkar and R. Marculescu, “Communication-aware task
scheduling and voltage selection for total systems energy mini-
mization,” Proc. of International Conference on Computer-Aided
Design, 2003.

[10] F. Yao, A. Demers, and S. Shenker, “A scheduling model for
reduced CPU energy,” Proc. of IEEE Symposium on Foundations
of Computer Science, 1995.

[11] Y. Zhang, X. Hu, and D. Z. Chen, “Task scheduling and voltage
selection for energy minimization,” Proc. of Design Automation
Conference, 2002.

[12] http://www.transmeta.com

454

